Overhauled the "Understanding LLMs -- A Cross-Section of the Most Relevant Literature To Get Up to Speed" reading list! 

Now featuring parameter-efficiency methods, and latest studies and insights via Pythia, among others to round it from 13 to 20!
------
If you're looking for an nice read over the long weekend, I highly recommend the recently uploaded survey "Instruction Tuning for Large Language Models" by Zhang et al.: https://arxiv.org/abs/2308.10792

Covers both the creation and usage of instruction-datasets for LLM finetuning.
------
Great post on making random seeds for you, not against you!
------
Embrace the randomness

As promised, here is my first post about potentially useful tips and tricks for training deep learning models. Some of these posts might be quite long, some will be shorter. The first thing we will talk about is randomness when fine-tuning models.

When… Show more
------
The CodeLlama PR just got merged: https://github.com/Lightning-AI/lit-gpt/pull/472…

When I tried it with bnb's 4-bit Normal Float quantization, the 34B Instruct and Python variants used about 20 Gb for inference:
------
And thanks to another kind contribution, Code Llama is making its way into Lit-GPT, too!!
 https://github.com/Lightning-AI/lit-gpt/pull/472…
------
Finally got around adding LIMA to Lit-GPT (upon popular request). 

You can use it now to finetune any of the supported LLMs (Llama 2, Falcon, LongChat, ... you name it). 

More usage info here: https://github.com/Lightning-AI/lit-gpt/blob/main/tutorials/prepare_dataset.md#lima…
------
"LIMA: Less Is More for Alignment" might be a gamechanger for researchers & tinkerers who want to develop capable LLMs. 

The researchers showed that a 65B LLaMA model finetuned on only 1000 examples (in supervised fashion) is not too far behind bigger models like ChatGPT!

1/6
------
Open source AI grants from a16z.
------
[New program] a16z Open Source AI Grants

Hackers & independent devs are massively important to the AI ecosystem.

We're starting a grant funding program so they can continue their work without pressure to generate financial returns.

https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/…
------
How do we define and categorize LLM "finetuning" these days?

There's 

1) Supervised finetuning: to
  1a) train an encoder-LLM for classification or 
  1b) train a decoder-LLM to generate a specific response text.

[1/2]
------
[2/2]

2) Instruction finetuning: usually RLHF to generate a specific response. (But can also refer to 1b depending on the technique)

3) Long-context finetuning: like pretraining but w longer contexts (as per Code Llama paper)?
------
And thanks to another kind contribution, Code Llama is making its way into Lit-GPT, too!!
 https://github.com/Lightning-AI/lit-gpt/pull/472…
------
Thanks to kind contributions from the open-source community, we added 3 new LLMs to Lit-GPT this week: Nous-Hermes, StableCode, and Platypus!

Try them here  https://github.com/Lightning-AI/lit-gpt…

#LLM #GPT #MachineLearning
------
Oh, and there was also Lemur last week. A code LLM not based on Code Llama but based on LLama 2 directly! 
https://xlang.ai/blog/openlemur
------
Code Llama was just released 4 days ago. Since then, we already got 

1) WizardCoder-34B (https://github.com/nlpxucan/WizardLM/tree/main/WizardCoder…)
2) Phind's finetuned CodeLLama-34B (https://phind.com/blog/code-llama-beats-gpt4…)

*Both reported to be surpassing GPT-4 on HumanEval.

The open source community is amazing!
------
*they have a 70B model!
------
Code Llama was just released 4 days ago. Since then, we already got 

1) WizardCoder-34B (https://github.com/nlpxucan/WizardLM/tree/main/WizardCoder…)
2) Phind's finetuned CodeLLama-34B (https://phind.com/blog/code-llama-beats-gpt4…)

*Both reported to be surpassing GPT-4 on HumanEval.

The open source community is amazing!
------
Llama 2, CodeLlama, and leaked GPT-4 details. 

Here's my new write-up on the noteworthy developments around LLMs of this summer so far:
------
It blows my mind that fine-tuning open models works so well.  I've pointed it at all kinds of small data and it works brilliantly.  On the order of 1k examples.

If you haven't tried fine tuning yet drop everything and do it.  It's a magical experience.
------
Llama 2 is awesome, however, coding tasks were not its strong suite. (See HumanEval, a coding-related evaluation task from the paper Evaluating Large Language Models Trained on Code.)

The new 34B CodeLlama model is twice as good as the original 70B Llama 2 model and closes the… Show more
------
CodeLlama -- a version of Llama2 that was fine-tuned for code tasks is live now. Available in 7B, 13B and 34B.
https://ai.meta.com/blog/code-llama-large-language-model-coding/…
------
CodeLlama -- a version of Llama2 that was fine-tuned for code tasks is live now. Available in 7B, 13B and 34B.
https://ai.meta.com/blog/code-llama-large-language-model-coding/…
------
Will be interesting how this LoRA-on-demand service will compare to open-source LoRA on prem.

Here's a little reminder that open-source Llama 2 compares very favorably to ChatGPT / GPT 3.5
------
fine-tuning for GPT-3.5 turbo!

(and coming this fall for GPT-4) twitter.com/OpenAI/status/…
------
Overhauled the "Understanding LLMs -- A Cross-Section of the Most Relevant Literature To Get Up to Speed" reading list! 

Now featuring parameter-efficiency methods, and latest studies and insights via Pythia, among others to round it from 13 to 20!
------
If you're looking for an nice read over the long weekend, I highly recommend the recently uploaded survey "Instruction Tuning for Large Language Models" by Zhang et al.: https://arxiv.org/abs/2308.10792

Covers both the creation and usage of instruction-datasets for LLM finetuning.
------
Great post on making random seeds for you, not against you!
------
Embrace the randomness

As promised, here is my first post about potentially useful tips and tricks for training deep learning models. Some of these posts might be quite long, some will be shorter. The first thing we will talk about is randomness when fine-tuning models.

When… Show more
------
The CodeLlama PR just got merged: https://github.com/Lightning-AI/lit-gpt/pull/472…

When I tried it with bnb's 4-bit Normal Float quantization, the 34B Instruct and Python variants used about 20 Gb for inference:
------
And thanks to another kind contribution, Code Llama is making its way into Lit-GPT, too!!
 https://github.com/Lightning-AI/lit-gpt/pull/472…
------
Finally got around adding LIMA to Lit-GPT (upon popular request). 

You can use it now to finetune any of the supported LLMs (Llama 2, Falcon, LongChat, ... you name it). 

More usage info here: https://github.com/Lightning-AI/lit-gpt/blob/main/tutorials/prepare_dataset.md#lima…
------
"LIMA: Less Is More for Alignment" might be a gamechanger for researchers & tinkerers who want to develop capable LLMs. 

The researchers showed that a 65B LLaMA model finetuned on only 1000 examples (in supervised fashion) is not too far behind bigger models like ChatGPT!

1/6
------
Open source AI grants from a16z.
------
[New program] a16z Open Source AI Grants

Hackers & independent devs are massively important to the AI ecosystem.

We're starting a grant funding program so they can continue their work without pressure to generate financial returns.

https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/…
------
How do we define and categorize LLM "finetuning" these days?

There's 

1) Supervised finetuning: to
  1a) train an encoder-LLM for classification or 
  1b) train a decoder-LLM to generate a specific response text.

[1/2]
------
[2/2]

2) Instruction finetuning: usually RLHF to generate a specific response. (But can also refer to 1b depending on the technique)

3) Long-context finetuning: like pretraining but w longer contexts (as per Code Llama paper)?
------
And thanks to another kind contribution, Code Llama is making its way into Lit-GPT, too!!
 https://github.com/Lightning-AI/lit-gpt/pull/472…
------
Thanks to kind contributions from the open-source community, we added 3 new LLMs to Lit-GPT this week: Nous-Hermes, StableCode, and Platypus!

Try them here  https://github.com/Lightning-AI/lit-gpt…

#LLM #GPT #MachineLearning
------
Oh, and there was also Lemur last week. A code LLM not based on Code Llama but based on LLama 2 directly! 
https://xlang.ai/blog/openlemur
------
Code Llama was just released 4 days ago. Since then, we already got 

1) WizardCoder-34B (https://github.com/nlpxucan/WizardLM/tree/main/WizardCoder…)
2) Phind's finetuned CodeLLama-34B (https://phind.com/blog/code-llama-beats-gpt4…)

*Both reported to be surpassing GPT-4 on HumanEval.

The open source community is amazing!
------
*they have a 70B model!
------
Code Llama was just released 4 days ago. Since then, we already got 

1) WizardCoder-34B (https://github.com/nlpxucan/WizardLM/tree/main/WizardCoder…)
2) Phind's finetuned CodeLLama-34B (https://phind.com/blog/code-llama-beats-gpt4…)

*Both reported to be surpassing GPT-4 on HumanEval.

The open source community is amazing!
------
Llama 2, CodeLlama, and leaked GPT-4 details. 

Here's my new write-up on the noteworthy developments around LLMs of this summer so far:
------
It blows my mind that fine-tuning open models works so well.  I've pointed it at all kinds of small data and it works brilliantly.  On the order of 1k examples.

If you haven't tried fine tuning yet drop everything and do it.  It's a magical experience.
------
Llama 2 is awesome, however, coding tasks were not its strong suite. (See HumanEval, a coding-related evaluation task from the paper Evaluating Large Language Models Trained on Code.)

The new 34B CodeLlama model is twice as good as the original 70B Llama 2 model and closes the… Show more
------
CodeLlama -- a version of Llama2 that was fine-tuned for code tasks is live now. Available in 7B, 13B and 34B.
https://ai.meta.com/blog/code-llama-large-language-model-coding/…
------
CodeLlama -- a version of Llama2 that was fine-tuned for code tasks is live now. Available in 7B, 13B and 34B.
https://ai.meta.com/blog/code-llama-large-language-model-coding/…
------
Will be interesting how this LoRA-on-demand service will compare to open-source LoRA on prem.

Here's a little reminder that open-source Llama 2 compares very favorably to ChatGPT / GPT 3.5
------
fine-tuning for GPT-3.5 turbo!

(and coming this fall for GPT-4) twitter.com/OpenAI/status/…
------
Just added QLoRA support for all LLMs in Lit-GPT: Llama 2, Falcon, Pythia, StableLM, and all others!

You can use it via 
`python finetune/lora.py --quantize "bnb.nf4"` 
to save significant GPU memory.

I've ran a few more benchmarks on the Gh PR here: https://github.com/Lightning-AI/lit-gpt/pull/275…
------
QLoRA-style LLM finetuning is now available for all models in Lit-GPT: https://github.com/Lightning-AI/lit-gpt…

Finetune larger models on cheaper hardware via the `--quantize bnb.nf4` flag.

It significantly reduces memory usage during finetuning (the larger the model the larger the benefit).
------
I forgot to mention the probably most important thing:   Finetuning Llama 2 7B this way 
- only requires 13 GB RAM 
- and can thus comfortably run on a single GPU 
------
What motivated self-attention mechanisms in transformer-based LLMs in the first place?

A made a short video covering
- the limitations of RNNs
- the original (Bahdanau) attention mechanism for RNNs, 
- how it all led to the original Transformer architecture used in LLMs
------
Ever wondered where Transformers came from?

In this video, @rasbt explains the original attention mechanism for RNNs and how it all led to the original Transformer architecture — the foundation of state-of-the-art NLP.

#LLM #GPT #MachineLearning
------
Overhauled the "Understanding LLMs -- A Cross-Section of the Most Relevant Literature To Get Up to Speed" reading list! 

Now featuring parameter-efficiency methods, and latest studies and insights via Pythia, among others to round it from 13 to 20!
------
If you're looking for an nice read over the long weekend, I highly recommend the recently uploaded survey "Instruction Tuning for Large Language Models" by Zhang et al.: https://arxiv.org/abs/2308.10792

Covers both the creation and usage of instruction-datasets for LLM finetuning.
------
Great post on making random seeds for you, not against you!
------
Embrace the randomness

As promised, here is my first post about potentially useful tips and tricks for training deep learning models. Some of these posts might be quite long, some will be shorter. The first thing we will talk about is randomness when fine-tuning models.

When… Show more
------
The CodeLlama PR just got merged: https://github.com/Lightning-AI/lit-gpt/pull/472…

When I tried it with bnb's 4-bit Normal Float quantization, the 34B Instruct and Python variants used about 20 Gb for inference:
------
And thanks to another kind contribution, Code Llama is making its way into Lit-GPT, too!!
 https://github.com/Lightning-AI/lit-gpt/pull/472…
------
Finally got around adding LIMA to Lit-GPT (upon popular request). 

You can use it now to finetune any of the supported LLMs (Llama 2, Falcon, LongChat, ... you name it). 

More usage info here: https://github.com/Lightning-AI/lit-gpt/blob/main/tutorials/prepare_dataset.md#lima…
------
"LIMA: Less Is More for Alignment" might be a gamechanger for researchers & tinkerers who want to develop capable LLMs. 

The researchers showed that a 65B LLaMA model finetuned on only 1000 examples (in supervised fashion) is not too far behind bigger models like ChatGPT!

1/6
------
Open source AI grants from a16z.
------
[New program] a16z Open Source AI Grants

Hackers & independent devs are massively important to the AI ecosystem.

We're starting a grant funding program so they can continue their work without pressure to generate financial returns.

https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/…
------
How do we define and categorize LLM "finetuning" these days?

There's 

1) Supervised finetuning: to
  1a) train an encoder-LLM for classification or 
  1b) train a decoder-LLM to generate a specific response text.

[1/2]
------
[2/2]

2) Instruction finetuning: usually RLHF to generate a specific response. (But can also refer to 1b depending on the technique)

3) Long-context finetuning: like pretraining but w longer contexts (as per Code Llama paper)?
------
And thanks to another kind contribution, Code Llama is making its way into Lit-GPT, too!!
 https://github.com/Lightning-AI/lit-gpt/pull/472…
------
Thanks to kind contributions from the open-source community, we added 3 new LLMs to Lit-GPT this week: Nous-Hermes, StableCode, and Platypus!

Try them here  https://github.com/Lightning-AI/lit-gpt…

#LLM #GPT #MachineLearning
------
Oh, and there was also Lemur last week. A code LLM not based on Code Llama but based on LLama 2 directly! 
https://xlang.ai/blog/openlemur
------
Code Llama was just released 4 days ago. Since then, we already got 

1) WizardCoder-34B (https://github.com/nlpxucan/WizardLM/tree/main/WizardCoder…)
2) Phind's finetuned CodeLLama-34B (https://phind.com/blog/code-llama-beats-gpt4…)

*Both reported to be surpassing GPT-4 on HumanEval.

The open source community is amazing!
------
*they have a 70B model!
------
Code Llama was just released 4 days ago. Since then, we already got 

1) WizardCoder-34B (https://github.com/nlpxucan/WizardLM/tree/main/WizardCoder…)
2) Phind's finetuned CodeLLama-34B (https://phind.com/blog/code-llama-beats-gpt4…)

*Both reported to be surpassing GPT-4 on HumanEval.

The open source community is amazing!
------
Llama 2, CodeLlama, and leaked GPT-4 details. 

Here's my new write-up on the noteworthy developments around LLMs of this summer so far:
------
It blows my mind that fine-tuning open models works so well.  I've pointed it at all kinds of small data and it works brilliantly.  On the order of 1k examples.

If you haven't tried fine tuning yet drop everything and do it.  It's a magical experience.
------
Llama 2 is awesome, however, coding tasks were not its strong suite. (See HumanEval, a coding-related evaluation task from the paper Evaluating Large Language Models Trained on Code.)

The new 34B CodeLlama model is twice as good as the original 70B Llama 2 model and closes the… Show more
------
CodeLlama -- a version of Llama2 that was fine-tuned for code tasks is live now. Available in 7B, 13B and 34B.
https://ai.meta.com/blog/code-llama-large-language-model-coding/…
------
CodeLlama -- a version of Llama2 that was fine-tuned for code tasks is live now. Available in 7B, 13B and 34B.
https://ai.meta.com/blog/code-llama-large-language-model-coding/…
------
Will be interesting how this LoRA-on-demand service will compare to open-source LoRA on prem.

Here's a little reminder that open-source Llama 2 compares very favorably to ChatGPT / GPT 3.5
------
fine-tuning for GPT-3.5 turbo!

(and coming this fall for GPT-4) twitter.com/OpenAI/status/…
------
Just added QLoRA support for all LLMs in Lit-GPT: Llama 2, Falcon, Pythia, StableLM, and all others!

You can use it via 
`python finetune/lora.py --quantize "bnb.nf4"` 
to save significant GPU memory.

I've ran a few more benchmarks on the Gh PR here: https://github.com/Lightning-AI/lit-gpt/pull/275…
------
QLoRA-style LLM finetuning is now available for all models in Lit-GPT: https://github.com/Lightning-AI/lit-gpt…

Finetune larger models on cheaper hardware via the `--quantize bnb.nf4` flag.

It significantly reduces memory usage during finetuning (the larger the model the larger the benefit).
------
I forgot to mention the probably most important thing:   Finetuning Llama 2 7B this way 
- only requires 13 GB RAM 
- and can thus comfortably run on a single GPU 
------
What motivated self-attention mechanisms in transformer-based LLMs in the first place?

A made a short video covering
- the limitations of RNNs
- the original (Bahdanau) attention mechanism for RNNs, 
- how it all led to the original Transformer architecture used in LLMs
------
Ever wondered where Transformers came from?

In this video, @rasbt explains the original attention mechanism for RNNs and how it all led to the original Transformer architecture — the foundation of state-of-the-art NLP.

#LLM #GPT #MachineLearning
------
"What do you use as your go-to tool for note-taking and writing?" -- you'd be surprised, but people ask me at least once per week.

Boring answer: most of the time, I'm just using TextEdit in plaintext mode.
------
And for longer articles, I usually switch to one of the following at some point during the writing process:

- Typora markdown editor (go-to when it's just me)
- Google Docs (less technical, collaborating)
- Overleaf (technical, collaboration)
------
Finally got around adding LIMA to Lit-GPT (upon popular request). 

You can use it now to finetune any of the supported LLMs (Llama 2, Falcon, LongChat, ... you name it). 

More usage info here: https://github.com/Lightning-AI/lit-gpt/blob/main/tutorials/prepare_dataset.md#lima…
------
"LIMA: Less Is More for Alignment" might be a gamechanger for researchers & tinkerers who want to develop capable LLMs. 

The researchers showed that a 65B LLaMA model finetuned on only 1000 examples (in supervised fashion) is not too far behind bigger models like ChatGPT!

1/6
------
Open source AI grants from a16z.
------
[New program] a16z Open Source AI Grants

Hackers & independent devs are massively important to the AI ecosystem.

We're starting a grant funding program so they can continue their work without pressure to generate financial returns.

https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/…
------
How do we define and categorize LLM "finetuning" these days?

There's 

1) Supervised finetuning: to
  1a) train an encoder-LLM for classification or 
  1b) train a decoder-LLM to generate a specific response text.

[1/2]
------
[2/2]

2) Instruction finetuning: usually RLHF to generate a specific response. (But can also refer to 1b depending on the technique)

3) Long-context finetuning: like pretraining but w longer contexts (as per Code Llama paper)?
------
And thanks to another kind contribution, Code Llama is making its way into Lit-GPT, too!!
 https://github.com/Lightning-AI/lit-gpt/pull/472…
------
Thanks to kind contributions from the open-source community, we added 3 new LLMs to Lit-GPT this week: Nous-Hermes, StableCode, and Platypus!

Try them here  https://github.com/Lightning-AI/lit-gpt…

#LLM #GPT #MachineLearning
------
Oh, and there was also Lemur last week. A code LLM not based on Code Llama but based on LLama 2 directly! 
https://xlang.ai/blog/openlemur
------
Code Llama was just released 4 days ago. Since then, we already got 

1) WizardCoder-34B (https://github.com/nlpxucan/WizardLM/tree/main/WizardCoder…)
2) Phind's finetuned CodeLLama-34B (https://phind.com/blog/code-llama-beats-gpt4…)

*Both reported to be surpassing GPT-4 on HumanEval.

The open source community is amazing!
------
*they have a 70B model!
------
Code Llama was just released 4 days ago. Since then, we already got 

1) WizardCoder-34B (https://github.com/nlpxucan/WizardLM/tree/main/WizardCoder…)
2) Phind's finetuned CodeLLama-34B (https://phind.com/blog/code-llama-beats-gpt4…)

*Both reported to be surpassing GPT-4 on HumanEval.

The open source community is amazing!
------
Llama 2, CodeLlama, and leaked GPT-4 details. 

Here's my new write-up on the noteworthy developments around LLMs of this summer so far:
------
It blows my mind that fine-tuning open models works so well.  I've pointed it at all kinds of small data and it works brilliantly.  On the order of 1k examples.

If you haven't tried fine tuning yet drop everything and do it.  It's a magical experience.
------
Llama 2 is awesome, however, coding tasks were not its strong suite. (See HumanEval, a coding-related evaluation task from the paper Evaluating Large Language Models Trained on Code.)

The new 34B CodeLlama model is twice as good as the original 70B Llama 2 model and closes the… Show more
------
CodeLlama -- a version of Llama2 that was fine-tuned for code tasks is live now. Available in 7B, 13B and 34B.
https://ai.meta.com/blog/code-llama-large-language-model-coding/…
------
CodeLlama -- a version of Llama2 that was fine-tuned for code tasks is live now. Available in 7B, 13B and 34B.
https://ai.meta.com/blog/code-llama-large-language-model-coding/…
------
Will be interesting how this LoRA-on-demand service will compare to open-source LoRA on prem.

Here's a little reminder that open-source Llama 2 compares very favorably to ChatGPT / GPT 3.5
------
fine-tuning for GPT-3.5 turbo!

(and coming this fall for GPT-4) twitter.com/OpenAI/status/…
------
Just added QLoRA support for all LLMs in Lit-GPT: Llama 2, Falcon, Pythia, StableLM, and all others!

You can use it via 
`python finetune/lora.py --quantize "bnb.nf4"` 
to save significant GPU memory.

I've ran a few more benchmarks on the Gh PR here: https://github.com/Lightning-AI/lit-gpt/pull/275…
------
QLoRA-style LLM finetuning is now available for all models in Lit-GPT: https://github.com/Lightning-AI/lit-gpt…

Finetune larger models on cheaper hardware via the `--quantize bnb.nf4` flag.

It significantly reduces memory usage during finetuning (the larger the model the larger the benefit).
------
I forgot to mention the probably most important thing:   Finetuning Llama 2 7B this way 
- only requires 13 GB RAM 
- and can thus comfortably run on a single GPU 
------
What motivated self-attention mechanisms in transformer-based LLMs in the first place?

A made a short video covering
- the limitations of RNNs
- the original (Bahdanau) attention mechanism for RNNs, 
- how it all led to the original Transformer architecture used in LLMs
------
Ever wondered where Transformers came from?

In this video, @rasbt explains the original attention mechanism for RNNs and how it all led to the original Transformer architecture — the foundation of state-of-the-art NLP.

#LLM #GPT #MachineLearning
------
"What do you use as your go-to tool for note-taking and writing?" -- you'd be surprised, but people ask me at least once per week.

Boring answer: most of the time, I'm just using TextEdit in plaintext mode.
------
And for longer articles, I usually switch to one of the following at some point during the writing process:

- Typora markdown editor (go-to when it's just me)
- Google Docs (less technical, collaborating)
- Overleaf (technical, collaboration)
------
Interested in the NeurIPS 2023 LLM Efficiency Challenge or just training your custom LLM on 1 GPU?

Wrote a new quickstart guide to get started in ~5 min: https://github.com/Lightning-AI/lit-gpt/blob/main/tutorials/neurips_challenge_quickstart.md…

If there's interest, also happy to write an article w. research directions to explore. Let me know!
------
 Join the #NeurIPS2023 LLM Efficiency Challenge! Get up and running with @rasbt’s quick start guide: https://github.com/Lightning-AI/lit-gpt/blob/main/tutorials/neurips_challenge_quickstart.md…

#LLMs #GenAI #GPT
------
If you are working on the NeurIPS LLM Efficiency Challenge, Dolly 15k support is now merged: https://github.com/Lightning-AI/lit-gpt/pull/430…

python scripts/prepare_dolly.py
python finetune/lora.py --data_dir data/dolly/
------
Platypus, a new open-source LLM at the top of the leaderboard: https://arxiv.org/abs/2308.07317

Key points are
1) a curated dataset: removing similar & duplicate questions
2) finetuning and merging Low Rank Approximation (LoRA) modules: focusing on the non-attention modules
------
Never thought I'd see the day I'd have a publication in JMLR  
So happy that the BLOOM carbon footprint paper has finally found a home at such an incredible venue!
Thank you 
@shakir_za
 for being such a great editor, it warms my heart to see your name on this paper 
------
Just saw the new Amazon website UI and at a first glance I thought book reviews were down to 1 Star :D 

And they say Amazon can't innovate anymore 
------
If you are a NumPy & PyTorch user and occassionally use pandas for data processing ...

 Here's a reminder that .loc includes the index stop point
------
Another classic:
------
And thanks to another kind contribution, Code Llama is making its way into Lit-GPT, too!!
 https://github.com/Lightning-AI/lit-gpt/pull/472…
------
Thanks to kind contributions from the open-source community, we added 3 new LLMs to Lit-GPT this week: Nous-Hermes, StableCode, and Platypus!

Try them here  https://github.com/Lightning-AI/lit-gpt…

#LLM #GPT #MachineLearning
------
Oh, and there was also Lemur last week. A code LLM not based on Code Llama but based on LLama 2 directly! 
https://xlang.ai/blog/openlemur
------
Code Llama was just released 4 days ago. Since then, we already got 

1) WizardCoder-34B (https://github.com/nlpxucan/WizardLM/tree/main/WizardCoder…)
2) Phind's finetuned CodeLLama-34B (https://phind.com/blog/code-llama-beats-gpt4…)

*Both reported to be surpassing GPT-4 on HumanEval.

The open source community is amazing!
------
*they have a 70B model!
------
Code Llama was just released 4 days ago. Since then, we already got 

1) WizardCoder-34B (https://github.com/nlpxucan/WizardLM/tree/main/WizardCoder…)
2) Phind's finetuned CodeLLama-34B (https://phind.com/blog/code-llama-beats-gpt4…)

*Both reported to be surpassing GPT-4 on HumanEval.

The open source community is amazing!
------
Llama 2, CodeLlama, and leaked GPT-4 details. 

Here's my new write-up on the noteworthy developments around LLMs of this summer so far:
------
It blows my mind that fine-tuning open models works so well.  I've pointed it at all kinds of small data and it works brilliantly.  On the order of 1k examples.

If you haven't tried fine tuning yet drop everything and do it.  It's a magical experience.
------
Llama 2 is awesome, however, coding tasks were not its strong suite. (See HumanEval, a coding-related evaluation task from the paper Evaluating Large Language Models Trained on Code.)

The new 34B CodeLlama model is twice as good as the original 70B Llama 2 model and closes the… Show more
------
CodeLlama -- a version of Llama2 that was fine-tuned for code tasks is live now. Available in 7B, 13B and 34B.
https://ai.meta.com/blog/code-llama-large-language-model-coding/…
------
CodeLlama -- a version of Llama2 that was fine-tuned for code tasks is live now. Available in 7B, 13B and 34B.
https://ai.meta.com/blog/code-llama-large-language-model-coding/…
------
Will be interesting how this LoRA-on-demand service will compare to open-source LoRA on prem.

Here's a little reminder that open-source Llama 2 compares very favorably to ChatGPT / GPT 3.5
------
fine-tuning for GPT-3.5 turbo!

(and coming this fall for GPT-4) twitter.com/OpenAI/status/…
------
Just added QLoRA support for all LLMs in Lit-GPT: Llama 2, Falcon, Pythia, StableLM, and all others!

You can use it via 
`python finetune/lora.py --quantize "bnb.nf4"` 
to save significant GPU memory.

I've ran a few more benchmarks on the Gh PR here: https://github.com/Lightning-AI/lit-gpt/pull/275…
------
QLoRA-style LLM finetuning is now available for all models in Lit-GPT: https://github.com/Lightning-AI/lit-gpt…

Finetune larger models on cheaper hardware via the `--quantize bnb.nf4` flag.

It significantly reduces memory usage during finetuning (the larger the model the larger the benefit).
------
I forgot to mention the probably most important thing:   Finetuning Llama 2 7B this way 
- only requires 13 GB RAM 
- and can thus comfortably run on a single GPU 
------
What motivated self-attention mechanisms in transformer-based LLMs in the first place?

A made a short video covering
- the limitations of RNNs
- the original (Bahdanau) attention mechanism for RNNs, 
- how it all led to the original Transformer architecture used in LLMs
------
Ever wondered where Transformers came from?

In this video, @rasbt explains the original attention mechanism for RNNs and how it all led to the original Transformer architecture — the foundation of state-of-the-art NLP.

#LLM #GPT #MachineLearning
------
"What do you use as your go-to tool for note-taking and writing?" -- you'd be surprised, but people ask me at least once per week.

Boring answer: most of the time, I'm just using TextEdit in plaintext mode.
------
And for longer articles, I usually switch to one of the following at some point during the writing process:

- Typora markdown editor (go-to when it's just me)
- Google Docs (less technical, collaborating)
- Overleaf (technical, collaboration)
------
Interested in the NeurIPS 2023 LLM Efficiency Challenge or just training your custom LLM on 1 GPU?

Wrote a new quickstart guide to get started in ~5 min: https://github.com/Lightning-AI/lit-gpt/blob/main/tutorials/neurips_challenge_quickstart.md…

If there's interest, also happy to write an article w. research directions to explore. Let me know!
------
 Join the #NeurIPS2023 LLM Efficiency Challenge! Get up and running with @rasbt’s quick start guide: https://github.com/Lightning-AI/lit-gpt/blob/main/tutorials/neurips_challenge_quickstart.md…

#LLMs #GenAI #GPT
------
If you are working on the NeurIPS LLM Efficiency Challenge, Dolly 15k support is now merged: https://github.com/Lightning-AI/lit-gpt/pull/430…

python scripts/prepare_dolly.py
python finetune/lora.py --data_dir data/dolly/
------
Platypus, a new open-source LLM at the top of the leaderboard: https://arxiv.org/abs/2308.07317

Key points are
1) a curated dataset: removing similar & duplicate questions
2) finetuning and merging Low Rank Approximation (LoRA) modules: focusing on the non-attention modules
------
Never thought I'd see the day I'd have a publication in JMLR  
So happy that the BLOOM carbon footprint paper has finally found a home at such an incredible venue!
Thank you 
@shakir_za
 for being such a great editor, it warms my heart to see your name on this paper 
------
Just saw the new Amazon website UI and at a first glance I thought book reviews were down to 1 Star :D 

And they say Amazon can't innovate anymore 
------
If you are a NumPy & PyTorch user and occassionally use pandas for data processing ...

 Here's a reminder that .loc includes the index stop point
------
Another classic:
------
New base models with Llama 2, pretraining LLMs with ReLoRA, training with less data via AlpaGasus, and many more ... 

There's been lots of interesting research in the last month! I summarized the research highlights in three sentences each:
------
The NeurIPS 2023 LLM Efficiency Challenge is a super exciting opportunity for developing & benchmarking new research directions for param-efficient LLMs. 

If you are looking for something to tinker with this weekend, I just put together a Starter Guide: https://lightning.ai/pages/community/tutorial/neurips2023-llm-efficiency-guide/…
------
 Master LLM finetuning on a single GPU to compete in the NeurIPS LLM Efficiency Challenge!

 Get started with our guide: https://lightning.ai/pages/community/tutorial/neurips2023-llm-efficiency-guide/…
 Set up Lit-GPT
 Save GPU memory
 Experiment & evaluate LLMs

 Your essential manual for success. Conquer the challenge and… Show more
------
I am really excited for the research community to develop (more) efficient methods for finetuning LLMs.
And I hope you find this competition as useful and exciting as I do!

Link to the competition here:
------
I love readings papers & books. And I can put these down any time & move on if I notice major quality problems.

With peer reviews, you have to go past that point, which often makes it very unenjoyable (even though it’s necessary).
------
And with articles for your own blog, you can choose to only incorporate the positive feedback.

If you submit a research article, you have to incorporate all (sorts of) feedback 
------
It blows my mind that fine-tuning open models works so well.  I've pointed it at all kinds of small data and it works brilliantly.  On the order of 1k examples.

If you haven't tried fine tuning yet drop everything and do it.  It's a magical experience.
------
Llama 2 is awesome, however, coding tasks were not its strong suite. (See HumanEval, a coding-related evaluation task from the paper Evaluating Large Language Models Trained on Code.)

The new 34B CodeLlama model is twice as good as the original 70B Llama 2 model and closes the… Show more
------
CodeLlama -- a version of Llama2 that was fine-tuned for code tasks is live now. Available in 7B, 13B and 34B.
https://ai.meta.com/blog/code-llama-large-language-model-coding/…
------
CodeLlama -- a version of Llama2 that was fine-tuned for code tasks is live now. Available in 7B, 13B and 34B.
https://ai.meta.com/blog/code-llama-large-language-model-coding/…
------
Will be interesting how this LoRA-on-demand service will compare to open-source LoRA on prem.

Here's a little reminder that open-source Llama 2 compares very favorably to ChatGPT / GPT 3.5
------
fine-tuning for GPT-3.5 turbo!

(and coming this fall for GPT-4) twitter.com/OpenAI/status/…
------
Just added QLoRA support for all LLMs in Lit-GPT: Llama 2, Falcon, Pythia, StableLM, and all others!

You can use it via 
`python finetune/lora.py --quantize "bnb.nf4"` 
to save significant GPU memory.

I've ran a few more benchmarks on the Gh PR here: https://github.com/Lightning-AI/lit-gpt/pull/275…
------
QLoRA-style LLM finetuning is now available for all models in Lit-GPT: https://github.com/Lightning-AI/lit-gpt…

Finetune larger models on cheaper hardware via the `--quantize bnb.nf4` flag.

It significantly reduces memory usage during finetuning (the larger the model the larger the benefit).
------
I forgot to mention the probably most important thing:   Finetuning Llama 2 7B this way 
- only requires 13 GB RAM 
- and can thus comfortably run on a single GPU 
------
What motivated self-attention mechanisms in transformer-based LLMs in the first place?

A made a short video covering
- the limitations of RNNs
- the original (Bahdanau) attention mechanism for RNNs, 
- how it all led to the original Transformer architecture used in LLMs
------
Ever wondered where Transformers came from?

In this video, @rasbt explains the original attention mechanism for RNNs and how it all led to the original Transformer architecture — the foundation of state-of-the-art NLP.

#LLM #GPT #MachineLearning
------
"What do you use as your go-to tool for note-taking and writing?" -- you'd be surprised, but people ask me at least once per week.

Boring answer: most of the time, I'm just using TextEdit in plaintext mode.
------
And for longer articles, I usually switch to one of the following at some point during the writing process:

- Typora markdown editor (go-to when it's just me)
- Google Docs (less technical, collaborating)
- Overleaf (technical, collaboration)
------
Interested in the NeurIPS 2023 LLM Efficiency Challenge or just training your custom LLM on 1 GPU?

Wrote a new quickstart guide to get started in ~5 min: https://github.com/Lightning-AI/lit-gpt/blob/main/tutorials/neurips_challenge_quickstart.md…

If there's interest, also happy to write an article w. research directions to explore. Let me know!
------
 Join the #NeurIPS2023 LLM Efficiency Challenge! Get up and running with @rasbt’s quick start guide: https://github.com/Lightning-AI/lit-gpt/blob/main/tutorials/neurips_challenge_quickstart.md…

#LLMs #GenAI #GPT
------
If you are working on the NeurIPS LLM Efficiency Challenge, Dolly 15k support is now merged: https://github.com/Lightning-AI/lit-gpt/pull/430…

python scripts/prepare_dolly.py
python finetune/lora.py --data_dir data/dolly/
------
Platypus, a new open-source LLM at the top of the leaderboard: https://arxiv.org/abs/2308.07317

Key points are
1) a curated dataset: removing similar & duplicate questions
2) finetuning and merging Low Rank Approximation (LoRA) modules: focusing on the non-attention modules
------
Never thought I'd see the day I'd have a publication in JMLR  
So happy that the BLOOM carbon footprint paper has finally found a home at such an incredible venue!
Thank you 
@shakir_za
 for being such a great editor, it warms my heart to see your name on this paper 
------
Just saw the new Amazon website UI and at a first glance I thought book reviews were down to 1 Star :D 

And they say Amazon can't innovate anymore 
------
If you are a NumPy & PyTorch user and occassionally use pandas for data processing ...

 Here's a reminder that .loc includes the index stop point
------
Another classic:
------
New base models with Llama 2, pretraining LLMs with ReLoRA, training with less data via AlpaGasus, and many more ... 

There's been lots of interesting research in the last month! I summarized the research highlights in three sentences each:
------
The NeurIPS 2023 LLM Efficiency Challenge is a super exciting opportunity for developing & benchmarking new research directions for param-efficient LLMs. 

If you are looking for something to tinker with this weekend, I just put together a Starter Guide: https://lightning.ai/pages/community/tutorial/neurips2023-llm-efficiency-guide/…
------
 Master LLM finetuning on a single GPU to compete in the NeurIPS LLM Efficiency Challenge!

 Get started with our guide: https://lightning.ai/pages/community/tutorial/neurips2023-llm-efficiency-guide/…
 Set up Lit-GPT
 Save GPU memory
 Experiment & evaluate LLMs

 Your essential manual for success. Conquer the challenge and… Show more
------
I am really excited for the research community to develop (more) efficient methods for finetuning LLMs.
And I hope you find this competition as useful and exciting as I do!

Link to the competition here:
------
I love readings papers & books. And I can put these down any time & move on if I notice major quality problems.

With peer reviews, you have to go past that point, which often makes it very unenjoyable (even though it’s necessary).
------
And with articles for your own blog, you can choose to only incorporate the positive feedback.

If you submit a research article, you have to incorporate all (sorts of) feedback 
------
Someone just made my day 
------
I can attest that this is one of the best courses I have taken. Even someone with some experience can learn new things here.
------
Just catching up with the ReLoRA paper (https://arxiv.org/abs/2307.05695) that explores whether LoRA can be used for pretraining LLMs (vs  finetuning).

Looks promising!

Caveat: they pretrained models up to 350 M parameters (the smallest Llama model is 7 B parameters, for comparison)
------
It's refreshing to see how much fun we (still) get out of the Gzip + NN paper.

It was provocative, intuitive, and easily hackable: the perfect storm for a rainy weekend.
------
GZIP vs Bag of Words for text classification, more experiments. 

To summarize: A Bag-of-Words distance in KNN is more effective than GZIP KNN. Moreover, Bag-of-Words in a linear classifier can achieve almost the same accuracy as BERT. #nlproc #machinelearning 1/4
------
Hah, 
@francoisfleuret
 knows that I have a knack for (text)books! I was quite excited to find this little gem in my mailbox. 

Stay tuned: my plan is to include it in my yearly book reviews!! 

(The 7 inch kobo for scale; a nice delight: the paperback is in color!).
------
Will be interesting how this LoRA-on-demand service will compare to open-source LoRA on prem.

Here's a little reminder that open-source Llama 2 compares very favorably to ChatGPT / GPT 3.5
------
fine-tuning for GPT-3.5 turbo!

(and coming this fall for GPT-4) twitter.com/OpenAI/status/…
------
Just added QLoRA support for all LLMs in Lit-GPT: Llama 2, Falcon, Pythia, StableLM, and all others!

You can use it via 
`python finetune/lora.py --quantize "bnb.nf4"` 
to save significant GPU memory.

I've ran a few more benchmarks on the Gh PR here: https://github.com/Lightning-AI/lit-gpt/pull/275…
------
QLoRA-style LLM finetuning is now available for all models in Lit-GPT: https://github.com/Lightning-AI/lit-gpt…

Finetune larger models on cheaper hardware via the `--quantize bnb.nf4` flag.

It significantly reduces memory usage during finetuning (the larger the model the larger the benefit).
------
I forgot to mention the probably most important thing:   Finetuning Llama 2 7B this way 
- only requires 13 GB RAM 
- and can thus comfortably run on a single GPU 
------
What motivated self-attention mechanisms in transformer-based LLMs in the first place?

A made a short video covering
- the limitations of RNNs
- the original (Bahdanau) attention mechanism for RNNs, 
- how it all led to the original Transformer architecture used in LLMs
------
Ever wondered where Transformers came from?

In this video, @rasbt explains the original attention mechanism for RNNs and how it all led to the original Transformer architecture — the foundation of state-of-the-art NLP.

#LLM #GPT #MachineLearning
------
"What do you use as your go-to tool for note-taking and writing?" -- you'd be surprised, but people ask me at least once per week.

Boring answer: most of the time, I'm just using TextEdit in plaintext mode.
------
And for longer articles, I usually switch to one of the following at some point during the writing process:

- Typora markdown editor (go-to when it's just me)
- Google Docs (less technical, collaborating)
- Overleaf (technical, collaboration)
------
Interested in the NeurIPS 2023 LLM Efficiency Challenge or just training your custom LLM on 1 GPU?

Wrote a new quickstart guide to get started in ~5 min: https://github.com/Lightning-AI/lit-gpt/blob/main/tutorials/neurips_challenge_quickstart.md…

If there's interest, also happy to write an article w. research directions to explore. Let me know!
------
 Join the #NeurIPS2023 LLM Efficiency Challenge! Get up and running with @rasbt’s quick start guide: https://github.com/Lightning-AI/lit-gpt/blob/main/tutorials/neurips_challenge_quickstart.md…

#LLMs #GenAI #GPT
------
If you are working on the NeurIPS LLM Efficiency Challenge, Dolly 15k support is now merged: https://github.com/Lightning-AI/lit-gpt/pull/430…

python scripts/prepare_dolly.py
python finetune/lora.py --data_dir data/dolly/
------
Platypus, a new open-source LLM at the top of the leaderboard: https://arxiv.org/abs/2308.07317

Key points are
1) a curated dataset: removing similar & duplicate questions
2) finetuning and merging Low Rank Approximation (LoRA) modules: focusing on the non-attention modules
------
Never thought I'd see the day I'd have a publication in JMLR  
So happy that the BLOOM carbon footprint paper has finally found a home at such an incredible venue!
Thank you 
@shakir_za
 for being such a great editor, it warms my heart to see your name on this paper 
------
Just saw the new Amazon website UI and at a first glance I thought book reviews were down to 1 Star :D 

And they say Amazon can't innovate anymore 
------
If you are a NumPy & PyTorch user and occassionally use pandas for data processing ...

 Here's a reminder that .loc includes the index stop point
------
Another classic:
------
New base models with Llama 2, pretraining LLMs with ReLoRA, training with less data via AlpaGasus, and many more ... 

There's been lots of interesting research in the last month! I summarized the research highlights in three sentences each:
------
The NeurIPS 2023 LLM Efficiency Challenge is a super exciting opportunity for developing & benchmarking new research directions for param-efficient LLMs. 

If you are looking for something to tinker with this weekend, I just put together a Starter Guide: https://lightning.ai/pages/community/tutorial/neurips2023-llm-efficiency-guide/…
------
 Master LLM finetuning on a single GPU to compete in the NeurIPS LLM Efficiency Challenge!

 Get started with our guide: https://lightning.ai/pages/community/tutorial/neurips2023-llm-efficiency-guide/…
 Set up Lit-GPT
 Save GPU memory
 Experiment & evaluate LLMs

 Your essential manual for success. Conquer the challenge and… Show more
------
I am really excited for the research community to develop (more) efficient methods for finetuning LLMs.
And I hope you find this competition as useful and exciting as I do!

Link to the competition here:
------
I love readings papers & books. And I can put these down any time & move on if I notice major quality problems.

With peer reviews, you have to go past that point, which often makes it very unenjoyable (even though it’s necessary).
------
And with articles for your own blog, you can choose to only incorporate the positive feedback.

If you submit a research article, you have to incorporate all (sorts of) feedback 
------
Someone just made my day 
------
I can attest that this is one of the best courses I have taken. Even someone with some experience can learn new things here.
------
Just catching up with the ReLoRA paper (https://arxiv.org/abs/2307.05695) that explores whether LoRA can be used for pretraining LLMs (vs  finetuning).

Looks promising!

Caveat: they pretrained models up to 350 M parameters (the smallest Llama model is 7 B parameters, for comparison)
------
It's refreshing to see how much fun we (still) get out of the Gzip + NN paper.

It was provocative, intuitive, and easily hackable: the perfect storm for a rainy weekend.
------
GZIP vs Bag of Words for text classification, more experiments. 

To summarize: A Bag-of-Words distance in KNN is more effective than GZIP KNN. Moreover, Bag-of-Words in a linear classifier can achieve almost the same accuracy as BERT. #nlproc #machinelearning 1/4
------
Hah, 
@francoisfleuret
 knows that I have a knack for (text)books! I was quite excited to find this little gem in my mailbox. 

Stay tuned: my plan is to include it in my yearly book reviews!! 

(The 7 inch kobo for scale; a nice delight: the paperback is in color!).
------
The secret sauce for getting into modern deep learning & AI.

For me "building things" has been a powerful motivator with respect to learning how things work under the hood (later).
------
Sometimes the wise decision
------
If you liked my NN+Gzip article last week (https://magazine.sebastianraschka.com/p/large-language-models-and-nearest…), you'll probably love 
@abhi9u
 & 
@alepiad
 's follow-up.

This one looks at the NN+Gzip method from a compression algo perspective comparing Gzip, Huffman coding, LZ77/LZ4, and Bzip2: https://codeconfessions.substack.com/p/lz77-is-all-you-need/…
------
Do Machine Learning Models Memorize or Generalize?

https://pair.withgoogle.com/explorables/grokking/…

An interactive introduction to grokking and mechanistic interpretability w/ 
@ghandeharioun
, 
@nadamused_
, 
@Nithum
, 
@wattenberg
 and 
@iislucas
------
And for longer articles, I usually switch to one of the following at some point during the writing process:

- Typora markdown editor (go-to when it's just me)
- Google Docs (less technical, collaborating)
- Overleaf (technical, collaboration)
------
Interested in the NeurIPS 2023 LLM Efficiency Challenge or just training your custom LLM on 1 GPU?

Wrote a new quickstart guide to get started in ~5 min: https://github.com/Lightning-AI/lit-gpt/blob/main/tutorials/neurips_challenge_quickstart.md…

If there's interest, also happy to write an article w. research directions to explore. Let me know!
------
 Join the #NeurIPS2023 LLM Efficiency Challenge! Get up and running with @rasbt’s quick start guide: https://github.com/Lightning-AI/lit-gpt/blob/main/tutorials/neurips_challenge_quickstart.md…

#LLMs #GenAI #GPT
------
If you are working on the NeurIPS LLM Efficiency Challenge, Dolly 15k support is now merged: https://github.com/Lightning-AI/lit-gpt/pull/430…

python scripts/prepare_dolly.py
python finetune/lora.py --data_dir data/dolly/
------
Platypus, a new open-source LLM at the top of the leaderboard: https://arxiv.org/abs/2308.07317

Key points are
1) a curated dataset: removing similar & duplicate questions
2) finetuning and merging Low Rank Approximation (LoRA) modules: focusing on the non-attention modules
------
Never thought I'd see the day I'd have a publication in JMLR  
So happy that the BLOOM carbon footprint paper has finally found a home at such an incredible venue!
Thank you 
@shakir_za
 for being such a great editor, it warms my heart to see your name on this paper 
------
Just saw the new Amazon website UI and at a first glance I thought book reviews were down to 1 Star :D 

And they say Amazon can't innovate anymore 
------
If you are a NumPy & PyTorch user and occassionally use pandas for data processing ...

 Here's a reminder that .loc includes the index stop point
------
Another classic:
------
New base models with Llama 2, pretraining LLMs with ReLoRA, training with less data via AlpaGasus, and many more ... 

There's been lots of interesting research in the last month! I summarized the research highlights in three sentences each:
------
The NeurIPS 2023 LLM Efficiency Challenge is a super exciting opportunity for developing & benchmarking new research directions for param-efficient LLMs. 

If you are looking for something to tinker with this weekend, I just put together a Starter Guide: https://lightning.ai/pages/community/tutorial/neurips2023-llm-efficiency-guide/…
------
 Master LLM finetuning on a single GPU to compete in the NeurIPS LLM Efficiency Challenge!

 Get started with our guide: https://lightning.ai/pages/community/tutorial/neurips2023-llm-efficiency-guide/…
 Set up Lit-GPT
 Save GPU memory
 Experiment & evaluate LLMs

 Your essential manual for success. Conquer the challenge and… Show more
------
I am really excited for the research community to develop (more) efficient methods for finetuning LLMs.
And I hope you find this competition as useful and exciting as I do!

Link to the competition here:
------
I love readings papers & books. And I can put these down any time & move on if I notice major quality problems.

With peer reviews, you have to go past that point, which often makes it very unenjoyable (even though it’s necessary).
------
And with articles for your own blog, you can choose to only incorporate the positive feedback.

If you submit a research article, you have to incorporate all (sorts of) feedback 
------
Someone just made my day 
------
I can attest that this is one of the best courses I have taken. Even someone with some experience can learn new things here.
------
Just catching up with the ReLoRA paper (https://arxiv.org/abs/2307.05695) that explores whether LoRA can be used for pretraining LLMs (vs  finetuning).

Looks promising!

Caveat: they pretrained models up to 350 M parameters (the smallest Llama model is 7 B parameters, for comparison)
------
It's refreshing to see how much fun we (still) get out of the Gzip + NN paper.

It was provocative, intuitive, and easily hackable: the perfect storm for a rainy weekend.
------
GZIP vs Bag of Words for text classification, more experiments. 

To summarize: A Bag-of-Words distance in KNN is more effective than GZIP KNN. Moreover, Bag-of-Words in a linear classifier can achieve almost the same accuracy as BERT. #nlproc #machinelearning 1/4
------
Hah, 
@francoisfleuret
 knows that I have a knack for (text)books! I was quite excited to find this little gem in my mailbox. 

Stay tuned: my plan is to include it in my yearly book reviews!! 

(The 7 inch kobo for scale; a nice delight: the paperback is in color!).
------
The secret sauce for getting into modern deep learning & AI.

For me "building things" has been a powerful motivator with respect to learning how things work under the hood (later).
------
Sometimes the wise decision
------
If you liked my NN+Gzip article last week (https://magazine.sebastianraschka.com/p/large-language-models-and-nearest…), you'll probably love 
@abhi9u
 & 
@alepiad
 's follow-up.

This one looks at the NN+Gzip method from a compression algo perspective comparing Gzip, Huffman coding, LZ77/LZ4, and Bzip2: https://codeconfessions.substack.com/p/lz77-is-all-you-need/…
------
Do Machine Learning Models Memorize or Generalize?

https://pair.withgoogle.com/explorables/grokking/…

An interactive introduction to grokking and mechanistic interpretability w/ 
@ghandeharioun
, 
@nadamused_
, 
@Nithum
, 
@wattenberg
 and 
@iislucas
------
Interestingly, DeBERTa-1.5B (and encoder-only model) beats Llama 2 on BoolQ, which is a nice example that encoders still outperform large decoders on classification task. 

For fairness: The DeBERTa-1.5B model was likely finetuned on the training data 

1/3
------
2/3 
For fairness: The DeBERTa-1.5B model was likely finetuned on the training data whereas Llama 2 was used via few-shot prompting. In that case, it highlights once more that finetuning custom LLMs remains worthwhile.
------
3/3
Here are the papers for reference:

Llama 2: Open Foundation and Fine-Tuned Chat Models: https://arxiv.org/abs/2307.09288

DeBERTa: Decoding-enhanced BERT with Disentangled Attention:
------
Read a textbook for at least 1 hour per day to invest in your future self.
------
What is your single best investment idea right now?
------
Use cases for encoder LLMs (classification) & decoder LLMs (chatbots) are obvious.

Seq2seq tasks like translation, where it makes sense to have access to the whole input, is where it gets interesting.
Re encoder-decoder LLMs (eg T5): Is there still a benefit of using an encoder?
------
A major bug in 8-bit optimizers that could cause some instabilities later in training has been fixed. Please update bitsandbytes to 0.41.1 via `pip install -U bitsandbytes`. Now 8-bit optimizer should again reproduce 32-bit optimizer performance.
------
Not all transformers are LLMs, since transformers can also be used for computer vision. 

And not all LLMs are transformers as there are large language models based on 
- recurrent (RWKV, https://arxiv.org/abs/2305.13048) 
- or convolutional (Hyena, https://arxiv.org/abs/2302.10866) 
approaches.
------
Next to LIMA, this is another interesting paper highlighting that more data is not always better when finetuning LLMs: https://arxiv.org/abs/2307.08701

Trimming the orig 52k Alpaca dataset to 9k can improve the performance when finetuning 7B and 13B parameter  LLMs.
------
Never thought I'd see the day I'd have a publication in JMLR  
So happy that the BLOOM carbon footprint paper has finally found a home at such an incredible venue!
Thank you 
@shakir_za
 for being such a great editor, it warms my heart to see your name on this paper 
------
Just saw the new Amazon website UI and at a first glance I thought book reviews were down to 1 Star :D 

And they say Amazon can't innovate anymore 
------
If you are a NumPy & PyTorch user and occassionally use pandas for data processing ...

 Here's a reminder that .loc includes the index stop point
------
Another classic:
------
New base models with Llama 2, pretraining LLMs with ReLoRA, training with less data via AlpaGasus, and many more ... 

There's been lots of interesting research in the last month! I summarized the research highlights in three sentences each:
------
The NeurIPS 2023 LLM Efficiency Challenge is a super exciting opportunity for developing & benchmarking new research directions for param-efficient LLMs. 

If you are looking for something to tinker with this weekend, I just put together a Starter Guide: https://lightning.ai/pages/community/tutorial/neurips2023-llm-efficiency-guide/…
------
 Master LLM finetuning on a single GPU to compete in the NeurIPS LLM Efficiency Challenge!

 Get started with our guide: https://lightning.ai/pages/community/tutorial/neurips2023-llm-efficiency-guide/…
 Set up Lit-GPT
 Save GPU memory
 Experiment & evaluate LLMs

 Your essential manual for success. Conquer the challenge and… Show more
------
I am really excited for the research community to develop (more) efficient methods for finetuning LLMs.
And I hope you find this competition as useful and exciting as I do!

Link to the competition here:
------
I love readings papers & books. And I can put these down any time & move on if I notice major quality problems.

With peer reviews, you have to go past that point, which often makes it very unenjoyable (even though it’s necessary).
------
And with articles for your own blog, you can choose to only incorporate the positive feedback.

If you submit a research article, you have to incorporate all (sorts of) feedback 
------
Someone just made my day 
------
I can attest that this is one of the best courses I have taken. Even someone with some experience can learn new things here.
------
Just catching up with the ReLoRA paper (https://arxiv.org/abs/2307.05695) that explores whether LoRA can be used for pretraining LLMs (vs  finetuning).

Looks promising!

Caveat: they pretrained models up to 350 M parameters (the smallest Llama model is 7 B parameters, for comparison)
------
It's refreshing to see how much fun we (still) get out of the Gzip + NN paper.

It was provocative, intuitive, and easily hackable: the perfect storm for a rainy weekend.
------
GZIP vs Bag of Words for text classification, more experiments. 

To summarize: A Bag-of-Words distance in KNN is more effective than GZIP KNN. Moreover, Bag-of-Words in a linear classifier can achieve almost the same accuracy as BERT. #nlproc #machinelearning 1/4
------
Hah, 
@francoisfleuret
 knows that I have a knack for (text)books! I was quite excited to find this little gem in my mailbox. 

Stay tuned: my plan is to include it in my yearly book reviews!! 

(The 7 inch kobo for scale; a nice delight: the paperback is in color!).
------
The secret sauce for getting into modern deep learning & AI.

For me "building things" has been a powerful motivator with respect to learning how things work under the hood (later).
------
Sometimes the wise decision
------
If you liked my NN+Gzip article last week (https://magazine.sebastianraschka.com/p/large-language-models-and-nearest…), you'll probably love 
@abhi9u
 & 
@alepiad
 's follow-up.

This one looks at the NN+Gzip method from a compression algo perspective comparing Gzip, Huffman coding, LZ77/LZ4, and Bzip2: https://codeconfessions.substack.com/p/lz77-is-all-you-need/…
------
Do Machine Learning Models Memorize or Generalize?

https://pair.withgoogle.com/explorables/grokking/…

An interactive introduction to grokking and mechanistic interpretability w/ 
@ghandeharioun
, 
@nadamused_
, 
@Nithum
, 
@wattenberg
 and 
@iislucas
------
Interestingly, DeBERTa-1.5B (and encoder-only model) beats Llama 2 on BoolQ, which is a nice example that encoders still outperform large decoders on classification task. 

For fairness: The DeBERTa-1.5B model was likely finetuned on the training data 

1/3
------
2/3 
For fairness: The DeBERTa-1.5B model was likely finetuned on the training data whereas Llama 2 was used via few-shot prompting. In that case, it highlights once more that finetuning custom LLMs remains worthwhile.
------
3/3
Here are the papers for reference:

Llama 2: Open Foundation and Fine-Tuned Chat Models: https://arxiv.org/abs/2307.09288

DeBERTa: Decoding-enhanced BERT with Disentangled Attention:
------
Read a textbook for at least 1 hour per day to invest in your future self.
------
What is your single best investment idea right now?
------
Use cases for encoder LLMs (classification) & decoder LLMs (chatbots) are obvious.

Seq2seq tasks like translation, where it makes sense to have access to the whole input, is where it gets interesting.
Re encoder-decoder LLMs (eg T5): Is there still a benefit of using an encoder?
------
A major bug in 8-bit optimizers that could cause some instabilities later in training has been fixed. Please update bitsandbytes to 0.41.1 via `pip install -U bitsandbytes`. Now 8-bit optimizer should again reproduce 32-bit optimizer performance.
------
Not all transformers are LLMs, since transformers can also be used for computer vision. 

And not all LLMs are transformers as there are large language models based on 
- recurrent (RWKV, https://arxiv.org/abs/2305.13048) 
- or convolutional (Hyena, https://arxiv.org/abs/2302.10866) 
approaches.
------
Next to LIMA, this is another interesting paper highlighting that more data is not always better when finetuning LLMs: https://arxiv.org/abs/2307.08701

Trimming the orig 52k Alpaca dataset to 9k can improve the performance when finetuning 7B and 13B parameter  LLMs.
------
Given that the "Attention Is All You Need" authors have left Google, the paper was recently updated to cross out their emails. A new notice was also added to the paper's header. I've never seen something like this before, but obviously this paper is in a category of its own.
------
Update on the  NN+Gzip front.

Added
1) a caching mechanism, which speeds it up 25%.
2) Parallel processing to speed it up 400% (on a MB Air).

Love this method. It's a nice test case for teaching multiprocessing!

Code: https://github.com/rasbt/nn_plus_gzip/blob/main/1_2_caching-multiprocessing.py…
------
Following up on some discussions of the "NN + Gzip versus LLMs" methods a few weeks back here's my little deep dive with a reimplementation and additional experiments: https://magazine.sebastianraschka.com/p/large-language-models-and-nearest/…

This includes fixing the tie-breaking and more!
------
Really enjoyed chatting with 
@sophiamyang
 and the book club! 

It's awesome to get some many interesting questions from so many interested readers !

PS: A link to the recording here https://youtube.com/watch?v=h70elw0mCpg…
------
 Loved our book club's discussion with @rasbt on his latest book 'Machine Learning Q and AI', which covers 30 important ML/AI questions.  We discussed parallelism in Deep Learning, fine-tuning LLMs, the relevance of XGBoost, transformers, quantization Techniques, and more.… Show more
------
New base models with Llama 2, pretraining LLMs with ReLoRA, training with less data via AlpaGasus, and many more ... 

There's been lots of interesting research in the last month! I summarized the research highlights in three sentences each:
------
The NeurIPS 2023 LLM Efficiency Challenge is a super exciting opportunity for developing & benchmarking new research directions for param-efficient LLMs. 

If you are looking for something to tinker with this weekend, I just put together a Starter Guide: https://lightning.ai/pages/community/tutorial/neurips2023-llm-efficiency-guide/…
------
 Master LLM finetuning on a single GPU to compete in the NeurIPS LLM Efficiency Challenge!

 Get started with our guide: https://lightning.ai/pages/community/tutorial/neurips2023-llm-efficiency-guide/…
 Set up Lit-GPT
 Save GPU memory
 Experiment & evaluate LLMs

 Your essential manual for success. Conquer the challenge and… Show more
------
I am really excited for the research community to develop (more) efficient methods for finetuning LLMs.
And I hope you find this competition as useful and exciting as I do!

Link to the competition here:
------
I love readings papers & books. And I can put these down any time & move on if I notice major quality problems.

With peer reviews, you have to go past that point, which often makes it very unenjoyable (even though it’s necessary).
------
And with articles for your own blog, you can choose to only incorporate the positive feedback.

If you submit a research article, you have to incorporate all (sorts of) feedback 
------
Someone just made my day 
------
I can attest that this is one of the best courses I have taken. Even someone with some experience can learn new things here.
------
Just catching up with the ReLoRA paper (https://arxiv.org/abs/2307.05695) that explores whether LoRA can be used for pretraining LLMs (vs  finetuning).

Looks promising!

Caveat: they pretrained models up to 350 M parameters (the smallest Llama model is 7 B parameters, for comparison)
------
It's refreshing to see how much fun we (still) get out of the Gzip + NN paper.

It was provocative, intuitive, and easily hackable: the perfect storm for a rainy weekend.
------
GZIP vs Bag of Words for text classification, more experiments. 

To summarize: A Bag-of-Words distance in KNN is more effective than GZIP KNN. Moreover, Bag-of-Words in a linear classifier can achieve almost the same accuracy as BERT. #nlproc #machinelearning 1/4
------
Hah, 
@francoisfleuret
 knows that I have a knack for (text)books! I was quite excited to find this little gem in my mailbox. 

Stay tuned: my plan is to include it in my yearly book reviews!! 

(The 7 inch kobo for scale; a nice delight: the paperback is in color!).
------
The secret sauce for getting into modern deep learning & AI.

For me "building things" has been a powerful motivator with respect to learning how things work under the hood (later).
------
Sometimes the wise decision
------
If you liked my NN+Gzip article last week (https://magazine.sebastianraschka.com/p/large-language-models-and-nearest…), you'll probably love 
@abhi9u
 & 
@alepiad
 's follow-up.

This one looks at the NN+Gzip method from a compression algo perspective comparing Gzip, Huffman coding, LZ77/LZ4, and Bzip2: https://codeconfessions.substack.com/p/lz77-is-all-you-need/…
------
Do Machine Learning Models Memorize or Generalize?

https://pair.withgoogle.com/explorables/grokking/…

An interactive introduction to grokking and mechanistic interpretability w/ 
@ghandeharioun
, 
@nadamused_
, 
@Nithum
, 
@wattenberg
 and 
@iislucas
------
Interestingly, DeBERTa-1.5B (and encoder-only model) beats Llama 2 on BoolQ, which is a nice example that encoders still outperform large decoders on classification task. 

For fairness: The DeBERTa-1.5B model was likely finetuned on the training data 

1/3
------
2/3 
For fairness: The DeBERTa-1.5B model was likely finetuned on the training data whereas Llama 2 was used via few-shot prompting. In that case, it highlights once more that finetuning custom LLMs remains worthwhile.
------
3/3
Here are the papers for reference:

Llama 2: Open Foundation and Fine-Tuned Chat Models: https://arxiv.org/abs/2307.09288

DeBERTa: Decoding-enhanced BERT with Disentangled Attention:
------
Read a textbook for at least 1 hour per day to invest in your future self.
------
What is your single best investment idea right now?
------
Use cases for encoder LLMs (classification) & decoder LLMs (chatbots) are obvious.

Seq2seq tasks like translation, where it makes sense to have access to the whole input, is where it gets interesting.
Re encoder-decoder LLMs (eg T5): Is there still a benefit of using an encoder?
------
A major bug in 8-bit optimizers that could cause some instabilities later in training has been fixed. Please update bitsandbytes to 0.41.1 via `pip install -U bitsandbytes`. Now 8-bit optimizer should again reproduce 32-bit optimizer performance.
------
Not all transformers are LLMs, since transformers can also be used for computer vision. 

And not all LLMs are transformers as there are large language models based on 
- recurrent (RWKV, https://arxiv.org/abs/2305.13048) 
- or convolutional (Hyena, https://arxiv.org/abs/2302.10866) 
approaches.
------
Next to LIMA, this is another interesting paper highlighting that more data is not always better when finetuning LLMs: https://arxiv.org/abs/2307.08701

Trimming the orig 52k Alpaca dataset to 9k can improve the performance when finetuning 7B and 13B parameter  LLMs.
------
Given that the "Attention Is All You Need" authors have left Google, the paper was recently updated to cross out their emails. A new notice was also added to the paper's header. I've never seen something like this before, but obviously this paper is in a category of its own.
------
Update on the  NN+Gzip front.

Added
1) a caching mechanism, which speeds it up 25%.
2) Parallel processing to speed it up 400% (on a MB Air).

Love this method. It's a nice test case for teaching multiprocessing!

Code: https://github.com/rasbt/nn_plus_gzip/blob/main/1_2_caching-multiprocessing.py…
------
Following up on some discussions of the "NN + Gzip versus LLMs" methods a few weeks back here's my little deep dive with a reimplementation and additional experiments: https://magazine.sebastianraschka.com/p/large-language-models-and-nearest/…

This includes fixing the tie-breaking and more!
------
Really enjoyed chatting with 
@sophiamyang
 and the book club! 

It's awesome to get some many interesting questions from so many interested readers !

PS: A link to the recording here https://youtube.com/watch?v=h70elw0mCpg…
------
 Loved our book club's discussion with @rasbt on his latest book 'Machine Learning Q and AI', which covers 30 important ML/AI questions.  We discussed parallelism in Deep Learning, fine-tuning LLMs, the relevance of XGBoost, transformers, quantization Techniques, and more.… Show more
------
Finally got a chance to spend some time with the 77-page Llama 2 paper (https://arxiv.org/abs/2307.09288). Here are some takeaways at a glance.

Appreciate that someone finally did a comprehensive supervised finetuning vs RLHF evaluation! (Lower right)
------
Following up on some discussions of the "NN + Gzip versus LLMs" methods a few weeks back here's my little deep dive with a reimplementation and additional experiments: https://magazine.sebastianraschka.com/p/large-language-models-and-nearest/…

This includes fixing the tie-breaking and more!
------
Trying to develop the next huge LLM is fun, but how about a side project with a more reasonable scope like the 1 LLM + 1 GPU + 1 Day NeurIPS efficiency challenge: https://llm-efficiency-challenge.github.io

Below a list of the approved models

(PS: Lit-GPT was chosen as the official starter kit )
------
Looking forward to joining 
@sophiamyang
 's
DS/ML book club tomorrow over lunch (1 pm CT) to talk & discuss Machine Learning Q and AI (https://leanpub.com/machine-learning-q-and-ai/…)! 

This book is a bit more advanced than my previous ones, covering concepts such as   
- Multi-GPU training paradigms.… Show more
------
 Join us for our monthly book club author meetup this Friday! We'll be having a live discussion with the talented author @rasbt  about his brand-new book "Machine Learning Q and AI". 
 Mark your calendars: https://discord.gg/6BremEf9db?event=1127679698638471330…
------
Someone just made my day 
------
I can attest that this is one of the best courses I have taken. Even someone with some experience can learn new things here.
------
Just catching up with the ReLoRA paper (https://arxiv.org/abs/2307.05695) that explores whether LoRA can be used for pretraining LLMs (vs  finetuning).

Looks promising!

Caveat: they pretrained models up to 350 M parameters (the smallest Llama model is 7 B parameters, for comparison)
------
It's refreshing to see how much fun we (still) get out of the Gzip + NN paper.

It was provocative, intuitive, and easily hackable: the perfect storm for a rainy weekend.
------
GZIP vs Bag of Words for text classification, more experiments. 

To summarize: A Bag-of-Words distance in KNN is more effective than GZIP KNN. Moreover, Bag-of-Words in a linear classifier can achieve almost the same accuracy as BERT. #nlproc #machinelearning 1/4
------
Hah, 
@francoisfleuret
 knows that I have a knack for (text)books! I was quite excited to find this little gem in my mailbox. 

Stay tuned: my plan is to include it in my yearly book reviews!! 

(The 7 inch kobo for scale; a nice delight: the paperback is in color!).
------
The secret sauce for getting into modern deep learning & AI.

For me "building things" has been a powerful motivator with respect to learning how things work under the hood (later).
------
Sometimes the wise decision
------
If you liked my NN+Gzip article last week (https://magazine.sebastianraschka.com/p/large-language-models-and-nearest…), you'll probably love 
@abhi9u
 & 
@alepiad
 's follow-up.

This one looks at the NN+Gzip method from a compression algo perspective comparing Gzip, Huffman coding, LZ77/LZ4, and Bzip2: https://codeconfessions.substack.com/p/lz77-is-all-you-need/…
------
Do Machine Learning Models Memorize or Generalize?

https://pair.withgoogle.com/explorables/grokking/…

An interactive introduction to grokking and mechanistic interpretability w/ 
@ghandeharioun
, 
@nadamused_
, 
@Nithum
, 
@wattenberg
 and 
@iislucas
------
Interestingly, DeBERTa-1.5B (and encoder-only model) beats Llama 2 on BoolQ, which is a nice example that encoders still outperform large decoders on classification task. 

For fairness: The DeBERTa-1.5B model was likely finetuned on the training data 

1/3
------
2/3 
For fairness: The DeBERTa-1.5B model was likely finetuned on the training data whereas Llama 2 was used via few-shot prompting. In that case, it highlights once more that finetuning custom LLMs remains worthwhile.
------
3/3
Here are the papers for reference:

Llama 2: Open Foundation and Fine-Tuned Chat Models: https://arxiv.org/abs/2307.09288

DeBERTa: Decoding-enhanced BERT with Disentangled Attention:
------
Read a textbook for at least 1 hour per day to invest in your future self.
------
What is your single best investment idea right now?
------
Use cases for encoder LLMs (classification) & decoder LLMs (chatbots) are obvious.

Seq2seq tasks like translation, where it makes sense to have access to the whole input, is where it gets interesting.
Re encoder-decoder LLMs (eg T5): Is there still a benefit of using an encoder?
------
A major bug in 8-bit optimizers that could cause some instabilities later in training has been fixed. Please update bitsandbytes to 0.41.1 via `pip install -U bitsandbytes`. Now 8-bit optimizer should again reproduce 32-bit optimizer performance.
------
Not all transformers are LLMs, since transformers can also be used for computer vision. 

And not all LLMs are transformers as there are large language models based on 
- recurrent (RWKV, https://arxiv.org/abs/2305.13048) 
- or convolutional (Hyena, https://arxiv.org/abs/2302.10866) 
approaches.
------
Next to LIMA, this is another interesting paper highlighting that more data is not always better when finetuning LLMs: https://arxiv.org/abs/2307.08701

Trimming the orig 52k Alpaca dataset to 9k can improve the performance when finetuning 7B and 13B parameter  LLMs.
------
Given that the "Attention Is All You Need" authors have left Google, the paper was recently updated to cross out their emails. A new notice was also added to the paper's header. I've never seen something like this before, but obviously this paper is in a category of its own.
------
Update on the  NN+Gzip front.

Added
1) a caching mechanism, which speeds it up 25%.
2) Parallel processing to speed it up 400% (on a MB Air).

Love this method. It's a nice test case for teaching multiprocessing!

Code: https://github.com/rasbt/nn_plus_gzip/blob/main/1_2_caching-multiprocessing.py…
------
Following up on some discussions of the "NN + Gzip versus LLMs" methods a few weeks back here's my little deep dive with a reimplementation and additional experiments: https://magazine.sebastianraschka.com/p/large-language-models-and-nearest/…

This includes fixing the tie-breaking and more!
------
Really enjoyed chatting with 
@sophiamyang
 and the book club! 

It's awesome to get some many interesting questions from so many interested readers !

PS: A link to the recording here https://youtube.com/watch?v=h70elw0mCpg…
------
 Loved our book club's discussion with @rasbt on his latest book 'Machine Learning Q and AI', which covers 30 important ML/AI questions.  We discussed parallelism in Deep Learning, fine-tuning LLMs, the relevance of XGBoost, transformers, quantization Techniques, and more.… Show more
------
Finally got a chance to spend some time with the 77-page Llama 2 paper (https://arxiv.org/abs/2307.09288). Here are some takeaways at a glance.

Appreciate that someone finally did a comprehensive supervised finetuning vs RLHF evaluation! (Lower right)
------
Following up on some discussions of the "NN + Gzip versus LLMs" methods a few weeks back here's my little deep dive with a reimplementation and additional experiments: https://magazine.sebastianraschka.com/p/large-language-models-and-nearest/…

This includes fixing the tie-breaking and more!
------
Trying to develop the next huge LLM is fun, but how about a side project with a more reasonable scope like the 1 LLM + 1 GPU + 1 Day NeurIPS efficiency challenge: https://llm-efficiency-challenge.github.io

Below a list of the approved models

(PS: Lit-GPT was chosen as the official starter kit )
------
Looking forward to joining 
@sophiamyang
 's
DS/ML book club tomorrow over lunch (1 pm CT) to talk & discuss Machine Learning Q and AI (https://leanpub.com/machine-learning-q-and-ai/…)! 

This book is a bit more advanced than my previous ones, covering concepts such as   
- Multi-GPU training paradigms.… Show more
------
 Join us for our monthly book club author meetup this Friday! We'll be having a live discussion with the talented author @rasbt  about his brand-new book "Machine Learning Q and AI". 
 Mark your calendars: https://discord.gg/6BremEf9db?event=1127679698638471330…
------
A quick experiment that took a bit due to kNN scaling on large datasets: the kNN + Gzip method is a bit  better than cosine similarity on count vectors.

On the IMDb Movie review dataset:
- 70% test acc for gzip
- 65% test acc for cosine distance

My (re)implementation code:… Show more
------
In short, the idea behind this gzip + kNN approach is that a given text A is close to the size of two concatenated texts A + B when compressed using Gzip. 

Or, the compressed size of A+A is similar to the compressed size of just A (plus a constant value for the back reference)  twitter.com/rasbt/status/1…
------
Small update:

In the orig code example in the paper, tie breaking was based on the lowest class label index, i.e., it was biased towards class 0.

When I fixed the tie-breaking to select the closest neighbor, I could get a 70.05% -> 71.91% improvement (https://github.com/rasbt/nn_plus_gzip…).
------
The secret sauce for getting into modern deep learning & AI.

For me "building things" has been a powerful motivator with respect to learning how things work under the hood (later).
------
Sometimes the wise decision
------
If you liked my NN+Gzip article last week (https://magazine.sebastianraschka.com/p/large-language-models-and-nearest…), you'll probably love 
@abhi9u
 & 
@alepiad
 's follow-up.

This one looks at the NN+Gzip method from a compression algo perspective comparing Gzip, Huffman coding, LZ77/LZ4, and Bzip2: https://codeconfessions.substack.com/p/lz77-is-all-you-need/…
------
Do Machine Learning Models Memorize or Generalize?

https://pair.withgoogle.com/explorables/grokking/…

An interactive introduction to grokking and mechanistic interpretability w/ 
@ghandeharioun
, 
@nadamused_
, 
@Nithum
, 
@wattenberg
 and 
@iislucas
------
Interestingly, DeBERTa-1.5B (and encoder-only model) beats Llama 2 on BoolQ, which is a nice example that encoders still outperform large decoders on classification task. 

For fairness: The DeBERTa-1.5B model was likely finetuned on the training data 

1/3
------
2/3 
For fairness: The DeBERTa-1.5B model was likely finetuned on the training data whereas Llama 2 was used via few-shot prompting. In that case, it highlights once more that finetuning custom LLMs remains worthwhile.
------
3/3
Here are the papers for reference:

Llama 2: Open Foundation and Fine-Tuned Chat Models: https://arxiv.org/abs/2307.09288

DeBERTa: Decoding-enhanced BERT with Disentangled Attention:
------
Read a textbook for at least 1 hour per day to invest in your future self.
------
What is your single best investment idea right now?
------
Use cases for encoder LLMs (classification) & decoder LLMs (chatbots) are obvious.

Seq2seq tasks like translation, where it makes sense to have access to the whole input, is where it gets interesting.
Re encoder-decoder LLMs (eg T5): Is there still a benefit of using an encoder?
------
A major bug in 8-bit optimizers that could cause some instabilities later in training has been fixed. Please update bitsandbytes to 0.41.1 via `pip install -U bitsandbytes`. Now 8-bit optimizer should again reproduce 32-bit optimizer performance.
------
Not all transformers are LLMs, since transformers can also be used for computer vision. 

And not all LLMs are transformers as there are large language models based on 
- recurrent (RWKV, https://arxiv.org/abs/2305.13048) 
- or convolutional (Hyena, https://arxiv.org/abs/2302.10866) 
approaches.
------
Next to LIMA, this is another interesting paper highlighting that more data is not always better when finetuning LLMs: https://arxiv.org/abs/2307.08701

Trimming the orig 52k Alpaca dataset to 9k can improve the performance when finetuning 7B and 13B parameter  LLMs.
------
Given that the "Attention Is All You Need" authors have left Google, the paper was recently updated to cross out their emails. A new notice was also added to the paper's header. I've never seen something like this before, but obviously this paper is in a category of its own.
------
Update on the  NN+Gzip front.

Added
1) a caching mechanism, which speeds it up 25%.
2) Parallel processing to speed it up 400% (on a MB Air).

Love this method. It's a nice test case for teaching multiprocessing!

Code: https://github.com/rasbt/nn_plus_gzip/blob/main/1_2_caching-multiprocessing.py…
------
Following up on some discussions of the "NN + Gzip versus LLMs" methods a few weeks back here's my little deep dive with a reimplementation and additional experiments: https://magazine.sebastianraschka.com/p/large-language-models-and-nearest/…

This includes fixing the tie-breaking and more!
------
Really enjoyed chatting with 
@sophiamyang
 and the book club! 

It's awesome to get some many interesting questions from so many interested readers !

PS: A link to the recording here https://youtube.com/watch?v=h70elw0mCpg…
------
 Loved our book club's discussion with @rasbt on his latest book 'Machine Learning Q and AI', which covers 30 important ML/AI questions.  We discussed parallelism in Deep Learning, fine-tuning LLMs, the relevance of XGBoost, transformers, quantization Techniques, and more.… Show more
------
Finally got a chance to spend some time with the 77-page Llama 2 paper (https://arxiv.org/abs/2307.09288). Here are some takeaways at a glance.

Appreciate that someone finally did a comprehensive supervised finetuning vs RLHF evaluation! (Lower right)
------
Following up on some discussions of the "NN + Gzip versus LLMs" methods a few weeks back here's my little deep dive with a reimplementation and additional experiments: https://magazine.sebastianraschka.com/p/large-language-models-and-nearest/…

This includes fixing the tie-breaking and more!
------
Trying to develop the next huge LLM is fun, but how about a side project with a more reasonable scope like the 1 LLM + 1 GPU + 1 Day NeurIPS efficiency challenge: https://llm-efficiency-challenge.github.io

Below a list of the approved models

(PS: Lit-GPT was chosen as the official starter kit )
------
Looking forward to joining 
@sophiamyang
 's
DS/ML book club tomorrow over lunch (1 pm CT) to talk & discuss Machine Learning Q and AI (https://leanpub.com/machine-learning-q-and-ai/…)! 

This book is a bit more advanced than my previous ones, covering concepts such as   
- Multi-GPU training paradigms.… Show more
------
 Join us for our monthly book club author meetup this Friday! We'll be having a live discussion with the talented author @rasbt  about his brand-new book "Machine Learning Q and AI". 
 Mark your calendars: https://discord.gg/6BremEf9db?event=1127679698638471330…
------
A quick experiment that took a bit due to kNN scaling on large datasets: the kNN + Gzip method is a bit  better than cosine similarity on count vectors.

On the IMDb Movie review dataset:
- 70% test acc for gzip
- 65% test acc for cosine distance

My (re)implementation code:… Show more
------
In short, the idea behind this gzip + kNN approach is that a given text A is close to the size of two concatenated texts A + B when compressed using Gzip. 

Or, the compressed size of A+A is similar to the compressed size of just A (plus a constant value for the back reference)  twitter.com/rasbt/status/1…
------
Small update:

In the orig code example in the paper, tie breaking was based on the lowest class label index, i.e., it was biased towards class 0.

When I fixed the tie-breaking to select the closest neighbor, I could get a 70.05% -> 71.91% improvement (https://github.com/rasbt/nn_plus_gzip…).
------
Will do a more thorough write-up on the weekend!
------
My colleagues already added Flash Attention 2 to our Lit-GPT repo!
So if you are working on the NeurIPS LLM Efficiency Challenge (for which Lit-GPT is the official starter kit, https://llm-efficiency-challenge.github.io), you can shave ~11% off your total runtime 
------
Lit-GPT supports Flash Attention 2 (2x faster than v1).  Enjoy faster training, finetuning and inference for LLMs like Llama 2, Falcon and Pythia. Get started now  https://github.com/Lightning-AI/lit-gpt#setup…

#MachineLearning #LLM #Llama2
------
2/3 
For fairness: The DeBERTa-1.5B model was likely finetuned on the training data whereas Llama 2 was used via few-shot prompting. In that case, it highlights once more that finetuning custom LLMs remains worthwhile.
------
3/3
Here are the papers for reference:

Llama 2: Open Foundation and Fine-Tuned Chat Models: https://arxiv.org/abs/2307.09288

DeBERTa: Decoding-enhanced BERT with Disentangled Attention:
------
Read a textbook for at least 1 hour per day to invest in your future self.
------
What is your single best investment idea right now?
------
Use cases for encoder LLMs (classification) & decoder LLMs (chatbots) are obvious.

Seq2seq tasks like translation, where it makes sense to have access to the whole input, is where it gets interesting.
Re encoder-decoder LLMs (eg T5): Is there still a benefit of using an encoder?
------
A major bug in 8-bit optimizers that could cause some instabilities later in training has been fixed. Please update bitsandbytes to 0.41.1 via `pip install -U bitsandbytes`. Now 8-bit optimizer should again reproduce 32-bit optimizer performance.
------
Not all transformers are LLMs, since transformers can also be used for computer vision. 

And not all LLMs are transformers as there are large language models based on 
- recurrent (RWKV, https://arxiv.org/abs/2305.13048) 
- or convolutional (Hyena, https://arxiv.org/abs/2302.10866) 
approaches.
------
Next to LIMA, this is another interesting paper highlighting that more data is not always better when finetuning LLMs: https://arxiv.org/abs/2307.08701

Trimming the orig 52k Alpaca dataset to 9k can improve the performance when finetuning 7B and 13B parameter  LLMs.
------
Given that the "Attention Is All You Need" authors have left Google, the paper was recently updated to cross out their emails. A new notice was also added to the paper's header. I've never seen something like this before, but obviously this paper is in a category of its own.
------
Update on the  NN+Gzip front.

Added
1) a caching mechanism, which speeds it up 25%.
2) Parallel processing to speed it up 400% (on a MB Air).

Love this method. It's a nice test case for teaching multiprocessing!

Code: https://github.com/rasbt/nn_plus_gzip/blob/main/1_2_caching-multiprocessing.py…
------
Following up on some discussions of the "NN + Gzip versus LLMs" methods a few weeks back here's my little deep dive with a reimplementation and additional experiments: https://magazine.sebastianraschka.com/p/large-language-models-and-nearest/…

This includes fixing the tie-breaking and more!
------
Really enjoyed chatting with 
@sophiamyang
 and the book club! 

It's awesome to get some many interesting questions from so many interested readers !

PS: A link to the recording here https://youtube.com/watch?v=h70elw0mCpg…
------
 Loved our book club's discussion with @rasbt on his latest book 'Machine Learning Q and AI', which covers 30 important ML/AI questions.  We discussed parallelism in Deep Learning, fine-tuning LLMs, the relevance of XGBoost, transformers, quantization Techniques, and more.… Show more
------
Finally got a chance to spend some time with the 77-page Llama 2 paper (https://arxiv.org/abs/2307.09288). Here are some takeaways at a glance.

Appreciate that someone finally did a comprehensive supervised finetuning vs RLHF evaluation! (Lower right)
------
Following up on some discussions of the "NN + Gzip versus LLMs" methods a few weeks back here's my little deep dive with a reimplementation and additional experiments: https://magazine.sebastianraschka.com/p/large-language-models-and-nearest/…

This includes fixing the tie-breaking and more!
------
Trying to develop the next huge LLM is fun, but how about a side project with a more reasonable scope like the 1 LLM + 1 GPU + 1 Day NeurIPS efficiency challenge: https://llm-efficiency-challenge.github.io

Below a list of the approved models

(PS: Lit-GPT was chosen as the official starter kit )
------
Looking forward to joining 
@sophiamyang
 's
DS/ML book club tomorrow over lunch (1 pm CT) to talk & discuss Machine Learning Q and AI (https://leanpub.com/machine-learning-q-and-ai/…)! 

This book is a bit more advanced than my previous ones, covering concepts such as   
- Multi-GPU training paradigms.… Show more
------
 Join us for our monthly book club author meetup this Friday! We'll be having a live discussion with the talented author @rasbt  about his brand-new book "Machine Learning Q and AI". 
 Mark your calendars: https://discord.gg/6BremEf9db?event=1127679698638471330…
------
A quick experiment that took a bit due to kNN scaling on large datasets: the kNN + Gzip method is a bit  better than cosine similarity on count vectors.

On the IMDb Movie review dataset:
- 70% test acc for gzip
- 65% test acc for cosine distance

My (re)implementation code:… Show more
------
In short, the idea behind this gzip + kNN approach is that a given text A is close to the size of two concatenated texts A + B when compressed using Gzip. 

Or, the compressed size of A+A is similar to the compressed size of just A (plus a constant value for the back reference)  twitter.com/rasbt/status/1…
------
Small update:

In the orig code example in the paper, tie breaking was based on the lowest class label index, i.e., it was biased towards class 0.

When I fixed the tie-breaking to select the closest neighbor, I could get a 70.05% -> 71.91% improvement (https://github.com/rasbt/nn_plus_gzip…).
------
Will do a more thorough write-up on the weekend!
------
My colleagues already added Flash Attention 2 to our Lit-GPT repo!
So if you are working on the NeurIPS LLM Efficiency Challenge (for which Lit-GPT is the official starter kit, https://llm-efficiency-challenge.github.io), you can shave ~11% off your total runtime 
------
Lit-GPT supports Flash Attention 2 (2x faster than v1).  Enjoy faster training, finetuning and inference for LLMs like Llama 2, Falcon and Pythia. Get started now  https://github.com/Lightning-AI/lit-gpt#setup…

#MachineLearning #LLM #Llama2
------
"A self-attention model is a linear MLP with dynamic weights" sounds about right 
------
A self-attention model is a linear MLP with dynamic weights?
------
Just got back from a week in nature, and wow, it seems like everyone and everything went full steam ahead:
- LLaMA 2
- FlashAttention 2
- ...
What else did I miss last week? 
------
PS: I'll be heading onto a little summer cabin-in-the-woods retreat. 

Will be bringing pen & paper but not the internet!

I'll be back ... with a lot of fun and useful content! 
------
If you think academic research conferences are fun, you'll love 
@SciPyConf
!

Just got back and had a great time and met so many amazing people!

Planning to put all my learnings into writing in the upcoming weeks (from new array to new multithreading standards, and more!) 
------
At which conference can you casually chat with both scientists at both NASA's Hubble and Webb telescopes -- spoiler: it's much more interesting than Python vs R!
------
From long context lengths in LLMs to more efficient transformers for diffusion models. 
It's been another month with lots of interesting research. 

I put together an annotated list of 24 highlights from June to July:
------
*PS: I didn't include the kNN + Gzip paper in this one since the paper was originally uploaded to arxiv in December 2022 https://twitter.com/rasbt/status/1679680423671001090?s=20…

But I have another "special coverage" for that one in mind in a few weeks once I am back from vacation :)
------
In short, the idea behind this gzip + kNN approach is that a given text A is close to the size of two concatenated texts A + B when compressed using Gzip. 

Or, the compressed size of A+A is similar to the compressed size of just A (plus a constant value for the back reference)  twitter.com/rasbt/status/1…
------
Next to LIMA, this is another interesting paper highlighting that more data is not always better when finetuning LLMs: https://arxiv.org/abs/2307.08701

Trimming the orig 52k Alpaca dataset to 9k can improve the performance when finetuning 7B and 13B parameter  LLMs.
------
Given that the "Attention Is All You Need" authors have left Google, the paper was recently updated to cross out their emails. A new notice was also added to the paper's header. I've never seen something like this before, but obviously this paper is in a category of its own.
------
Update on the  NN+Gzip front.

Added
1) a caching mechanism, which speeds it up 25%.
2) Parallel processing to speed it up 400% (on a MB Air).

Love this method. It's a nice test case for teaching multiprocessing!

Code: https://github.com/rasbt/nn_plus_gzip/blob/main/1_2_caching-multiprocessing.py…
------
Following up on some discussions of the "NN + Gzip versus LLMs" methods a few weeks back here's my little deep dive with a reimplementation and additional experiments: https://magazine.sebastianraschka.com/p/large-language-models-and-nearest/…

This includes fixing the tie-breaking and more!
------
Really enjoyed chatting with 
@sophiamyang
 and the book club! 

It's awesome to get some many interesting questions from so many interested readers !

PS: A link to the recording here https://youtube.com/watch?v=h70elw0mCpg…
------
 Loved our book club's discussion with @rasbt on his latest book 'Machine Learning Q and AI', which covers 30 important ML/AI questions.  We discussed parallelism in Deep Learning, fine-tuning LLMs, the relevance of XGBoost, transformers, quantization Techniques, and more.… Show more
------
Finally got a chance to spend some time with the 77-page Llama 2 paper (https://arxiv.org/abs/2307.09288). Here are some takeaways at a glance.

Appreciate that someone finally did a comprehensive supervised finetuning vs RLHF evaluation! (Lower right)
------
Following up on some discussions of the "NN + Gzip versus LLMs" methods a few weeks back here's my little deep dive with a reimplementation and additional experiments: https://magazine.sebastianraschka.com/p/large-language-models-and-nearest/…

This includes fixing the tie-breaking and more!
------
Trying to develop the next huge LLM is fun, but how about a side project with a more reasonable scope like the 1 LLM + 1 GPU + 1 Day NeurIPS efficiency challenge: https://llm-efficiency-challenge.github.io

Below a list of the approved models

(PS: Lit-GPT was chosen as the official starter kit )
------
Looking forward to joining 
@sophiamyang
 's
DS/ML book club tomorrow over lunch (1 pm CT) to talk & discuss Machine Learning Q and AI (https://leanpub.com/machine-learning-q-and-ai/…)! 

This book is a bit more advanced than my previous ones, covering concepts such as   
- Multi-GPU training paradigms.… Show more
------
 Join us for our monthly book club author meetup this Friday! We'll be having a live discussion with the talented author @rasbt  about his brand-new book "Machine Learning Q and AI". 
 Mark your calendars: https://discord.gg/6BremEf9db?event=1127679698638471330…
------
A quick experiment that took a bit due to kNN scaling on large datasets: the kNN + Gzip method is a bit  better than cosine similarity on count vectors.

On the IMDb Movie review dataset:
- 70% test acc for gzip
- 65% test acc for cosine distance

My (re)implementation code:… Show more
------
In short, the idea behind this gzip + kNN approach is that a given text A is close to the size of two concatenated texts A + B when compressed using Gzip. 

Or, the compressed size of A+A is similar to the compressed size of just A (plus a constant value for the back reference)  twitter.com/rasbt/status/1…
------
Small update:

In the orig code example in the paper, tie breaking was based on the lowest class label index, i.e., it was biased towards class 0.

When I fixed the tie-breaking to select the closest neighbor, I could get a 70.05% -> 71.91% improvement (https://github.com/rasbt/nn_plus_gzip…).
------
Will do a more thorough write-up on the weekend!
------
My colleagues already added Flash Attention 2 to our Lit-GPT repo!
So if you are working on the NeurIPS LLM Efficiency Challenge (for which Lit-GPT is the official starter kit, https://llm-efficiency-challenge.github.io), you can shave ~11% off your total runtime 
------
Lit-GPT supports Flash Attention 2 (2x faster than v1).  Enjoy faster training, finetuning and inference for LLMs like Llama 2, Falcon and Pythia. Get started now  https://github.com/Lightning-AI/lit-gpt#setup…

#MachineLearning #LLM #Llama2
------
"A self-attention model is a linear MLP with dynamic weights" sounds about right 
------
A self-attention model is a linear MLP with dynamic weights?
------
Just got back from a week in nature, and wow, it seems like everyone and everything went full steam ahead:
- LLaMA 2
- FlashAttention 2
- ...
What else did I miss last week? 
------
PS: I'll be heading onto a little summer cabin-in-the-woods retreat. 

Will be bringing pen & paper but not the internet!

I'll be back ... with a lot of fun and useful content! 
------
If you think academic research conferences are fun, you'll love 
@SciPyConf
!

Just got back and had a great time and met so many amazing people!

Planning to put all my learnings into writing in the upcoming weeks (from new array to new multithreading standards, and more!) 
------
At which conference can you casually chat with both scientists at both NASA's Hubble and Webb telescopes -- spoiler: it's much more interesting than Python vs R!
------
From long context lengths in LLMs to more efficient transformers for diffusion models. 
It's been another month with lots of interesting research. 

I put together an annotated list of 24 highlights from June to July:
------
*PS: I didn't include the kNN + Gzip paper in this one since the paper was originally uploaded to arxiv in December 2022 https://twitter.com/rasbt/status/1679680423671001090?s=20…

But I have another "special coverage" for that one in mind in a few weeks once I am back from vacation :)
------
In short, the idea behind this gzip + kNN approach is that a given text A is close to the size of two concatenated texts A + B when compressed using Gzip. 

Or, the compressed size of A+A is similar to the compressed size of just A (plus a constant value for the back reference)  twitter.com/rasbt/status/1…
------
If all this gzip == LLMs talk continues, we might actually start teaching data structures in computer science again
------
New in #SciPy:

- sobol indices
- lazy loading
- alpine wheels
- overhaul of scipy.sparse 

#SciPy2023
------
It’s a strength that many Python libraries are mostly not written in Python: it brings devs & users with different preferences together in a single ecosystem. 

The alternative is scientific computing being fragmented across C, Fortran, Rust & CUDA and users have to learn it all?
------
There was a tweet yesterday about how major python libraries mostly aren't written in python, and it implied that this was something that should be fixed. I take it as revealed preference: experts don't like working in python. It's also a terrible language for beginners.
------
In short, the idea behind this gzip + kNN approach is that a given text A is close to the size of two concatenated texts A + B when compressed using Gzip. 

Or, the compressed size of A+A is similar to the compressed size of just A (plus a constant value for the back reference)
------
Gzip + kNN beats transformers on text classification.

(Gzip as in good old zip file compression) twitter.com/LukeGessler/st…
------
Finally got a chance to spend some time with the 77-page Llama 2 paper (https://arxiv.org/abs/2307.09288). Here are some takeaways at a glance.

Appreciate that someone finally did a comprehensive supervised finetuning vs RLHF evaluation! (Lower right)
------
Following up on some discussions of the "NN + Gzip versus LLMs" methods a few weeks back here's my little deep dive with a reimplementation and additional experiments: https://magazine.sebastianraschka.com/p/large-language-models-and-nearest/…

This includes fixing the tie-breaking and more!
------
Trying to develop the next huge LLM is fun, but how about a side project with a more reasonable scope like the 1 LLM + 1 GPU + 1 Day NeurIPS efficiency challenge: https://llm-efficiency-challenge.github.io

Below a list of the approved models

(PS: Lit-GPT was chosen as the official starter kit )
------
Looking forward to joining 
@sophiamyang
 's
DS/ML book club tomorrow over lunch (1 pm CT) to talk & discuss Machine Learning Q and AI (https://leanpub.com/machine-learning-q-and-ai/…)! 

This book is a bit more advanced than my previous ones, covering concepts such as   
- Multi-GPU training paradigms.… Show more
------
 Join us for our monthly book club author meetup this Friday! We'll be having a live discussion with the talented author @rasbt  about his brand-new book "Machine Learning Q and AI". 
 Mark your calendars: https://discord.gg/6BremEf9db?event=1127679698638471330…
------
A quick experiment that took a bit due to kNN scaling on large datasets: the kNN + Gzip method is a bit  better than cosine similarity on count vectors.

On the IMDb Movie review dataset:
- 70% test acc for gzip
- 65% test acc for cosine distance

My (re)implementation code:… Show more
------
In short, the idea behind this gzip + kNN approach is that a given text A is close to the size of two concatenated texts A + B when compressed using Gzip. 

Or, the compressed size of A+A is similar to the compressed size of just A (plus a constant value for the back reference)  twitter.com/rasbt/status/1…
------
Small update:

In the orig code example in the paper, tie breaking was based on the lowest class label index, i.e., it was biased towards class 0.

When I fixed the tie-breaking to select the closest neighbor, I could get a 70.05% -> 71.91% improvement (https://github.com/rasbt/nn_plus_gzip…).
------
Will do a more thorough write-up on the weekend!
------
My colleagues already added Flash Attention 2 to our Lit-GPT repo!
So if you are working on the NeurIPS LLM Efficiency Challenge (for which Lit-GPT is the official starter kit, https://llm-efficiency-challenge.github.io), you can shave ~11% off your total runtime 
------
Lit-GPT supports Flash Attention 2 (2x faster than v1).  Enjoy faster training, finetuning and inference for LLMs like Llama 2, Falcon and Pythia. Get started now  https://github.com/Lightning-AI/lit-gpt#setup…

#MachineLearning #LLM #Llama2
------
"A self-attention model is a linear MLP with dynamic weights" sounds about right 
------
A self-attention model is a linear MLP with dynamic weights?
------
Just got back from a week in nature, and wow, it seems like everyone and everything went full steam ahead:
- LLaMA 2
- FlashAttention 2
- ...
What else did I miss last week? 
------
PS: I'll be heading onto a little summer cabin-in-the-woods retreat. 

Will be bringing pen & paper but not the internet!

I'll be back ... with a lot of fun and useful content! 
------
If you think academic research conferences are fun, you'll love 
@SciPyConf
!

Just got back and had a great time and met so many amazing people!

Planning to put all my learnings into writing in the upcoming weeks (from new array to new multithreading standards, and more!) 
------
At which conference can you casually chat with both scientists at both NASA's Hubble and Webb telescopes -- spoiler: it's much more interesting than Python vs R!
------
From long context lengths in LLMs to more efficient transformers for diffusion models. 
It's been another month with lots of interesting research. 

I put together an annotated list of 24 highlights from June to July:
------
*PS: I didn't include the kNN + Gzip paper in this one since the paper was originally uploaded to arxiv in December 2022 https://twitter.com/rasbt/status/1679680423671001090?s=20…

But I have another "special coverage" for that one in mind in a few weeks once I am back from vacation :)
------
In short, the idea behind this gzip + kNN approach is that a given text A is close to the size of two concatenated texts A + B when compressed using Gzip. 

Or, the compressed size of A+A is similar to the compressed size of just A (plus a constant value for the back reference)  twitter.com/rasbt/status/1…
------
If all this gzip == LLMs talk continues, we might actually start teaching data structures in computer science again
------
New in #SciPy:

- sobol indices
- lazy loading
- alpine wheels
- overhaul of scipy.sparse 

#SciPy2023
------
It’s a strength that many Python libraries are mostly not written in Python: it brings devs & users with different preferences together in a single ecosystem. 

The alternative is scientific computing being fragmented across C, Fortran, Rust & CUDA and users have to learn it all?
------
There was a tweet yesterday about how major python libraries mostly aren't written in python, and it implied that this was something that should be fixed. I take it as revealed preference: experts don't like working in python. It's also a terrible language for beginners.
------
In short, the idea behind this gzip + kNN approach is that a given text A is close to the size of two concatenated texts A + B when compressed using Gzip. 

Or, the compressed size of A+A is similar to the compressed size of just A (plus a constant value for the back reference)
------
Gzip + kNN beats transformers on text classification.

(Gzip as in good old zip file compression) twitter.com/LukeGessler/st…
------
Gzip + kNN beats transformers on text classification.

(Gzip as in good old zip file compression)
------
this paper's nuts. for sentence classification on out-of-domain datasets, all neural (Transformer or not) approaches lose to good old kNN on representations generated by.... gzip https://aclanthology.org/2023.findings-acl.426/…
------
Part of my vision for 
@datascience_uw
 is to foster an #OpenSource ethos on campus and to develop scalable, distributed approaches to contribute to the Open Source communities that we rely on. I’m thrilled to launch the OSPO 
@UWMadison
. Thank 
@SloanFoundation
 for supporting it!
------
.@UWMadison is one of six universities recently selected by @SloanFoundation to establish #OpenSource Program Offices. Partners include the Data Science Institute, @MadisonCollege, the Data Science Hub, the @UWMadLibraries and @UWMadisonExt. https://datascience.wisc.edu/2023/07/11/data-science-institute-to-establish-campus-open-source-program-office/…
------
One of the weird things about working in the #Python community is that our core technologies are mostly not written in Python.

If someone wants to contribute code, now they have two problems.

- Michael Droettboom at #SciPy2023
------
Looking forward to joining 
@sophiamyang
 's
DS/ML book club tomorrow over lunch (1 pm CT) to talk & discuss Machine Learning Q and AI (https://leanpub.com/machine-learning-q-and-ai/…)! 

This book is a bit more advanced than my previous ones, covering concepts such as   
- Multi-GPU training paradigms.… Show more
------
 Join us for our monthly book club author meetup this Friday! We'll be having a live discussion with the talented author @rasbt  about his brand-new book "Machine Learning Q and AI". 
 Mark your calendars: https://discord.gg/6BremEf9db?event=1127679698638471330…
------
A quick experiment that took a bit due to kNN scaling on large datasets: the kNN + Gzip method is a bit  better than cosine similarity on count vectors.

On the IMDb Movie review dataset:
- 70% test acc for gzip
- 65% test acc for cosine distance

My (re)implementation code:… Show more
------
In short, the idea behind this gzip + kNN approach is that a given text A is close to the size of two concatenated texts A + B when compressed using Gzip. 

Or, the compressed size of A+A is similar to the compressed size of just A (plus a constant value for the back reference)  twitter.com/rasbt/status/1…
------
Small update:

In the orig code example in the paper, tie breaking was based on the lowest class label index, i.e., it was biased towards class 0.

When I fixed the tie-breaking to select the closest neighbor, I could get a 70.05% -> 71.91% improvement (https://github.com/rasbt/nn_plus_gzip…).
------
Will do a more thorough write-up on the weekend!
------
My colleagues already added Flash Attention 2 to our Lit-GPT repo!
So if you are working on the NeurIPS LLM Efficiency Challenge (for which Lit-GPT is the official starter kit, https://llm-efficiency-challenge.github.io), you can shave ~11% off your total runtime 
------
Lit-GPT supports Flash Attention 2 (2x faster than v1).  Enjoy faster training, finetuning and inference for LLMs like Llama 2, Falcon and Pythia. Get started now  https://github.com/Lightning-AI/lit-gpt#setup…

#MachineLearning #LLM #Llama2
------
"A self-attention model is a linear MLP with dynamic weights" sounds about right 
------
A self-attention model is a linear MLP with dynamic weights?
------
Just got back from a week in nature, and wow, it seems like everyone and everything went full steam ahead:
- LLaMA 2
- FlashAttention 2
- ...
What else did I miss last week? 
------
PS: I'll be heading onto a little summer cabin-in-the-woods retreat. 

Will be bringing pen & paper but not the internet!

I'll be back ... with a lot of fun and useful content! 
------
If you think academic research conferences are fun, you'll love 
@SciPyConf
!

Just got back and had a great time and met so many amazing people!

Planning to put all my learnings into writing in the upcoming weeks (from new array to new multithreading standards, and more!) 
------
At which conference can you casually chat with both scientists at both NASA's Hubble and Webb telescopes -- spoiler: it's much more interesting than Python vs R!
------
From long context lengths in LLMs to more efficient transformers for diffusion models. 
It's been another month with lots of interesting research. 

I put together an annotated list of 24 highlights from June to July:
------
*PS: I didn't include the kNN + Gzip paper in this one since the paper was originally uploaded to arxiv in December 2022 https://twitter.com/rasbt/status/1679680423671001090?s=20…

But I have another "special coverage" for that one in mind in a few weeks once I am back from vacation :)
------
In short, the idea behind this gzip + kNN approach is that a given text A is close to the size of two concatenated texts A + B when compressed using Gzip. 

Or, the compressed size of A+A is similar to the compressed size of just A (plus a constant value for the back reference)  twitter.com/rasbt/status/1…
------
If all this gzip == LLMs talk continues, we might actually start teaching data structures in computer science again
------
New in #SciPy:

- sobol indices
- lazy loading
- alpine wheels
- overhaul of scipy.sparse 

#SciPy2023
------
It’s a strength that many Python libraries are mostly not written in Python: it brings devs & users with different preferences together in a single ecosystem. 

The alternative is scientific computing being fragmented across C, Fortran, Rust & CUDA and users have to learn it all?
------
There was a tweet yesterday about how major python libraries mostly aren't written in python, and it implied that this was something that should be fixed. I take it as revealed preference: experts don't like working in python. It's also a terrible language for beginners.
------
In short, the idea behind this gzip + kNN approach is that a given text A is close to the size of two concatenated texts A + B when compressed using Gzip. 

Or, the compressed size of A+A is similar to the compressed size of just A (plus a constant value for the back reference)
------
Gzip + kNN beats transformers on text classification.

(Gzip as in good old zip file compression) twitter.com/LukeGessler/st…
------
Gzip + kNN beats transformers on text classification.

(Gzip as in good old zip file compression)
------
this paper's nuts. for sentence classification on out-of-domain datasets, all neural (Transformer or not) approaches lose to good old kNN on representations generated by.... gzip https://aclanthology.org/2023.findings-acl.426/…
------
Part of my vision for 
@datascience_uw
 is to foster an #OpenSource ethos on campus and to develop scalable, distributed approaches to contribute to the Open Source communities that we rely on. I’m thrilled to launch the OSPO 
@UWMadison
. Thank 
@SloanFoundation
 for supporting it!
------
.@UWMadison is one of six universities recently selected by @SloanFoundation to establish #OpenSource Program Offices. Partners include the Data Science Institute, @MadisonCollege, the Data Science Hub, the @UWMadLibraries and @UWMadisonExt. https://datascience.wisc.edu/2023/07/11/data-science-institute-to-establish-campus-open-source-program-office/…
------
One of the weird things about working in the #Python community is that our core technologies are mostly not written in Python.

If someone wants to contribute code, now they have two problems.

- Michael Droettboom at #SciPy2023
------
We use self-supervised learning to pretrain LLMs (e.g., next-word prediction). 

Here's an interesting take using self-supervised learning for evaluating LLMs: https://arxiv.org/abs//2306.13651
Turns out, there's correlation between self-supervised evaluations & human evaluations.
------
Excited for LLM Summer at #SDSC23! Don't miss 
@rasbt
 keynote on "LLMs for Everything & Everyone". Dive into top-tier #machinelearning insights. Check out his renowned Ahead of AI newsletter: http://magazine.sebastianraschka.com
------
"Python vs R" is now finally solved  
------
Hi #EconTwitter!

Exciting news: 

@robtibshirani-@daniela_witten-James-Hastie-Taylor have released the #python version of the influential 𝐄𝐥𝐞𝐦𝐞𝐧𝐭𝐬 𝐨𝐟 𝐒𝐭𝐚𝐭𝐢𝐬𝐭𝐢𝐜𝐚𝐥 𝐋𝐞𝐚𝐫𝐧𝐢𝐧𝐠!

Course, datasets, notebook (and more) included. 

Don't miss out!
------
Small update:

In the orig code example in the paper, tie breaking was based on the lowest class label index, i.e., it was biased towards class 0.

When I fixed the tie-breaking to select the closest neighbor, I could get a 70.05% -> 71.91% improvement (https://github.com/rasbt/nn_plus_gzip…).
------
Will do a more thorough write-up on the weekend!
------
My colleagues already added Flash Attention 2 to our Lit-GPT repo!
So if you are working on the NeurIPS LLM Efficiency Challenge (for which Lit-GPT is the official starter kit, https://llm-efficiency-challenge.github.io), you can shave ~11% off your total runtime 
------
Lit-GPT supports Flash Attention 2 (2x faster than v1).  Enjoy faster training, finetuning and inference for LLMs like Llama 2, Falcon and Pythia. Get started now  https://github.com/Lightning-AI/lit-gpt#setup…

#MachineLearning #LLM #Llama2
------
"A self-attention model is a linear MLP with dynamic weights" sounds about right 
------
A self-attention model is a linear MLP with dynamic weights?
------
Just got back from a week in nature, and wow, it seems like everyone and everything went full steam ahead:
- LLaMA 2
- FlashAttention 2
- ...
What else did I miss last week? 
------
PS: I'll be heading onto a little summer cabin-in-the-woods retreat. 

Will be bringing pen & paper but not the internet!

I'll be back ... with a lot of fun and useful content! 
------
If you think academic research conferences are fun, you'll love 
@SciPyConf
!

Just got back and had a great time and met so many amazing people!

Planning to put all my learnings into writing in the upcoming weeks (from new array to new multithreading standards, and more!) 
------
At which conference can you casually chat with both scientists at both NASA's Hubble and Webb telescopes -- spoiler: it's much more interesting than Python vs R!
------
From long context lengths in LLMs to more efficient transformers for diffusion models. 
It's been another month with lots of interesting research. 

I put together an annotated list of 24 highlights from June to July:
------
*PS: I didn't include the kNN + Gzip paper in this one since the paper was originally uploaded to arxiv in December 2022 https://twitter.com/rasbt/status/1679680423671001090?s=20…

But I have another "special coverage" for that one in mind in a few weeks once I am back from vacation :)
------
In short, the idea behind this gzip + kNN approach is that a given text A is close to the size of two concatenated texts A + B when compressed using Gzip. 

Or, the compressed size of A+A is similar to the compressed size of just A (plus a constant value for the back reference)  twitter.com/rasbt/status/1…
------
If all this gzip == LLMs talk continues, we might actually start teaching data structures in computer science again
------
New in #SciPy:

- sobol indices
- lazy loading
- alpine wheels
- overhaul of scipy.sparse 

#SciPy2023
------
It’s a strength that many Python libraries are mostly not written in Python: it brings devs & users with different preferences together in a single ecosystem. 

The alternative is scientific computing being fragmented across C, Fortran, Rust & CUDA and users have to learn it all?
------
There was a tweet yesterday about how major python libraries mostly aren't written in python, and it implied that this was something that should be fixed. I take it as revealed preference: experts don't like working in python. It's also a terrible language for beginners.
------
In short, the idea behind this gzip + kNN approach is that a given text A is close to the size of two concatenated texts A + B when compressed using Gzip. 

Or, the compressed size of A+A is similar to the compressed size of just A (plus a constant value for the back reference)
------
Gzip + kNN beats transformers on text classification.

(Gzip as in good old zip file compression) twitter.com/LukeGessler/st…
------
Gzip + kNN beats transformers on text classification.

(Gzip as in good old zip file compression)
------
this paper's nuts. for sentence classification on out-of-domain datasets, all neural (Transformer or not) approaches lose to good old kNN on representations generated by.... gzip https://aclanthology.org/2023.findings-acl.426/…
------
Part of my vision for 
@datascience_uw
 is to foster an #OpenSource ethos on campus and to develop scalable, distributed approaches to contribute to the Open Source communities that we rely on. I’m thrilled to launch the OSPO 
@UWMadison
. Thank 
@SloanFoundation
 for supporting it!
------
.@UWMadison is one of six universities recently selected by @SloanFoundation to establish #OpenSource Program Offices. Partners include the Data Science Institute, @MadisonCollege, the Data Science Hub, the @UWMadLibraries and @UWMadisonExt. https://datascience.wisc.edu/2023/07/11/data-science-institute-to-establish-campus-open-source-program-office/…
------
One of the weird things about working in the #Python community is that our core technologies are mostly not written in Python.

If someone wants to contribute code, now they have two problems.

- Michael Droettboom at #SciPy2023
------
We use self-supervised learning to pretrain LLMs (e.g., next-word prediction). 

Here's an interesting take using self-supervised learning for evaluating LLMs: https://arxiv.org/abs//2306.13651
Turns out, there's correlation between self-supervised evaluations & human evaluations.
------
Excited for LLM Summer at #SDSC23! Don't miss 
@rasbt
 keynote on "LLMs for Everything & Everyone". Dive into top-tier #machinelearning insights. Check out his renowned Ahead of AI newsletter: http://magazine.sebastianraschka.com
------
"Python vs R" is now finally solved  
------
Hi #EconTwitter!

Exciting news: 

@robtibshirani-@daniela_witten-James-Hastie-Taylor have released the #python version of the influential 𝐄𝐥𝐞𝐦𝐞𝐧𝐭𝐬 𝐨𝐟 𝐒𝐭𝐚𝐭𝐢𝐬𝐭𝐢𝐜𝐚𝐥 𝐋𝐞𝐚𝐫𝐧𝐢𝐧𝐠!

Course, datasets, notebook (and more) included. 

Don't miss out!
------
 Loved 
@rasbt
's PyTorch tutorial at 
@SciPyConf
 today. I learned how to accelerate model training, organize PyTorch code, Lightning tips, and fine-tune models.

Check out the tutorial repository here  https://github.com/rasbt/scipy2023-deeplearning/…
------
Just arrived in Austin and looking forward to attending SciPy once more! 

Ready and excited for my workshop on “Modern Deep Learning with PyTorch” tomorrow! 

https://github.com/rasbt/scipy2023-deeplearning…
------
For those who complain about Google Search ads: Ads in 2024 will be become a lot more subtle ...
------
New paper: 
LLMs can be fine-tuned with tiny amounts of data. This makes LLMs useful, but also exploitable; tweaked training data can cause bad behavior.

This LLM is tweaked to inject product placement for McDonalds into stuff. Not sure if I'm entertained or disturbed.  [1/3]
------
Just got back from a week in nature, and wow, it seems like everyone and everything went full steam ahead:
- LLaMA 2
- FlashAttention 2
- ...
What else did I miss last week? 
------
PS: I'll be heading onto a little summer cabin-in-the-woods retreat. 

Will be bringing pen & paper but not the internet!

I'll be back ... with a lot of fun and useful content! 
------
If you think academic research conferences are fun, you'll love 
@SciPyConf
!

Just got back and had a great time and met so many amazing people!

Planning to put all my learnings into writing in the upcoming weeks (from new array to new multithreading standards, and more!) 
------
At which conference can you casually chat with both scientists at both NASA's Hubble and Webb telescopes -- spoiler: it's much more interesting than Python vs R!
------
From long context lengths in LLMs to more efficient transformers for diffusion models. 
It's been another month with lots of interesting research. 

I put together an annotated list of 24 highlights from June to July:
------
*PS: I didn't include the kNN + Gzip paper in this one since the paper was originally uploaded to arxiv in December 2022 https://twitter.com/rasbt/status/1679680423671001090?s=20…

But I have another "special coverage" for that one in mind in a few weeks once I am back from vacation :)
------
In short, the idea behind this gzip + kNN approach is that a given text A is close to the size of two concatenated texts A + B when compressed using Gzip. 

Or, the compressed size of A+A is similar to the compressed size of just A (plus a constant value for the back reference)  twitter.com/rasbt/status/1…
------
If all this gzip == LLMs talk continues, we might actually start teaching data structures in computer science again
------
New in #SciPy:

- sobol indices
- lazy loading
- alpine wheels
- overhaul of scipy.sparse 

#SciPy2023
------
It’s a strength that many Python libraries are mostly not written in Python: it brings devs & users with different preferences together in a single ecosystem. 

The alternative is scientific computing being fragmented across C, Fortran, Rust & CUDA and users have to learn it all?
------
There was a tweet yesterday about how major python libraries mostly aren't written in python, and it implied that this was something that should be fixed. I take it as revealed preference: experts don't like working in python. It's also a terrible language for beginners.
------
In short, the idea behind this gzip + kNN approach is that a given text A is close to the size of two concatenated texts A + B when compressed using Gzip. 

Or, the compressed size of A+A is similar to the compressed size of just A (plus a constant value for the back reference)
------
Gzip + kNN beats transformers on text classification.

(Gzip as in good old zip file compression) twitter.com/LukeGessler/st…
------
Gzip + kNN beats transformers on text classification.

(Gzip as in good old zip file compression)
------
this paper's nuts. for sentence classification on out-of-domain datasets, all neural (Transformer or not) approaches lose to good old kNN on representations generated by.... gzip https://aclanthology.org/2023.findings-acl.426/…
------
Part of my vision for 
@datascience_uw
 is to foster an #OpenSource ethos on campus and to develop scalable, distributed approaches to contribute to the Open Source communities that we rely on. I’m thrilled to launch the OSPO 
@UWMadison
. Thank 
@SloanFoundation
 for supporting it!
------
.@UWMadison is one of six universities recently selected by @SloanFoundation to establish #OpenSource Program Offices. Partners include the Data Science Institute, @MadisonCollege, the Data Science Hub, the @UWMadLibraries and @UWMadisonExt. https://datascience.wisc.edu/2023/07/11/data-science-institute-to-establish-campus-open-source-program-office/…
------
One of the weird things about working in the #Python community is that our core technologies are mostly not written in Python.

If someone wants to contribute code, now they have two problems.

- Michael Droettboom at #SciPy2023
------
We use self-supervised learning to pretrain LLMs (e.g., next-word prediction). 

Here's an interesting take using self-supervised learning for evaluating LLMs: https://arxiv.org/abs//2306.13651
Turns out, there's correlation between self-supervised evaluations & human evaluations.
------
Excited for LLM Summer at #SDSC23! Don't miss 
@rasbt
 keynote on "LLMs for Everything & Everyone". Dive into top-tier #machinelearning insights. Check out his renowned Ahead of AI newsletter: http://magazine.sebastianraschka.com
------
"Python vs R" is now finally solved  
------
Hi #EconTwitter!

Exciting news: 

@robtibshirani-@daniela_witten-James-Hastie-Taylor have released the #python version of the influential 𝐄𝐥𝐞𝐦𝐞𝐧𝐭𝐬 𝐨𝐟 𝐒𝐭𝐚𝐭𝐢𝐬𝐭𝐢𝐜𝐚𝐥 𝐋𝐞𝐚𝐫𝐧𝐢𝐧𝐠!

Course, datasets, notebook (and more) included. 

Don't miss out!
------
 Loved 
@rasbt
's PyTorch tutorial at 
@SciPyConf
 today. I learned how to accelerate model training, organize PyTorch code, Lightning tips, and fine-tune models.

Check out the tutorial repository here  https://github.com/rasbt/scipy2023-deeplearning/…
------
Just arrived in Austin and looking forward to attending SciPy once more! 

Ready and excited for my workshop on “Modern Deep Learning with PyTorch” tomorrow! 

https://github.com/rasbt/scipy2023-deeplearning…
------
For those who complain about Google Search ads: Ads in 2024 will be become a lot more subtle ...
------
New paper: 
LLMs can be fine-tuned with tiny amounts of data. This makes LLMs useful, but also exploitable; tweaked training data can cause bad behavior.

This LLM is tweaked to inject product placement for McDonalds into stuff. Not sure if I'm entertained or disturbed.  [1/3]
------
In the last couple of days, we talked a lot about extending the context window of transformer LLMs. Here's one more: "Extending Context Window of Large Language Models via Positional Interpolation" 

1/3
------
*the focus here is on improving modeling performance, not computational performance.
------
Rotary positional embeddings (aka RoPE) have been a recent cornerstone of modern LLM implementations since it supports flexible sequence lengths. In this paper, researchers propose Position Interpolation to increase RoPE-based context window sizes to 32,768 tokens 

2/3
------
This sounds a bit more humble than the recent 1M or 1B context lengths claims.

But this requires only minimal (1000 steps) finetuning. And it  allows long document summarization using LLaMA 7B and 65B models, for example.

Link to the paper here: https://arxiv.org/abs/2306.15595

3/3
------
What books are you reading this week? I'm currently reading Machine Learning Q and AI by 
@rasbt
 and I'm thoroughly enjoying it! 

Here is the book link: https://leanpub.com/machine-learning-q-and-ai…

Join our book club if you are interested in reading this book together: http://dsbookclub.github.io
------
We have seen a new wave of LLMs for longer contexts: 1) RMT, 2) Hyena LLM & 3) LongNet

There are several use-cases for such long LLMs but the elephant in the room is: How well do LLMs use these longer contexts? 
Turns out not so well if info is in the middle of the input.
1/5
------
*PS: I didn't include the kNN + Gzip paper in this one since the paper was originally uploaded to arxiv in December 2022 https://twitter.com/rasbt/status/1679680423671001090?s=20…

But I have another "special coverage" for that one in mind in a few weeks once I am back from vacation :)
------
In short, the idea behind this gzip + kNN approach is that a given text A is close to the size of two concatenated texts A + B when compressed using Gzip. 

Or, the compressed size of A+A is similar to the compressed size of just A (plus a constant value for the back reference)  twitter.com/rasbt/status/1…
------
If all this gzip == LLMs talk continues, we might actually start teaching data structures in computer science again
------
New in #SciPy:

- sobol indices
- lazy loading
- alpine wheels
- overhaul of scipy.sparse 

#SciPy2023
------
It’s a strength that many Python libraries are mostly not written in Python: it brings devs & users with different preferences together in a single ecosystem. 

The alternative is scientific computing being fragmented across C, Fortran, Rust & CUDA and users have to learn it all?
------
There was a tweet yesterday about how major python libraries mostly aren't written in python, and it implied that this was something that should be fixed. I take it as revealed preference: experts don't like working in python. It's also a terrible language for beginners.
------
In short, the idea behind this gzip + kNN approach is that a given text A is close to the size of two concatenated texts A + B when compressed using Gzip. 

Or, the compressed size of A+A is similar to the compressed size of just A (plus a constant value for the back reference)
------
Gzip + kNN beats transformers on text classification.

(Gzip as in good old zip file compression) twitter.com/LukeGessler/st…
------
Gzip + kNN beats transformers on text classification.

(Gzip as in good old zip file compression)
------
this paper's nuts. for sentence classification on out-of-domain datasets, all neural (Transformer or not) approaches lose to good old kNN on representations generated by.... gzip https://aclanthology.org/2023.findings-acl.426/…
------
Part of my vision for 
@datascience_uw
 is to foster an #OpenSource ethos on campus and to develop scalable, distributed approaches to contribute to the Open Source communities that we rely on. I’m thrilled to launch the OSPO 
@UWMadison
. Thank 
@SloanFoundation
 for supporting it!
------
.@UWMadison is one of six universities recently selected by @SloanFoundation to establish #OpenSource Program Offices. Partners include the Data Science Institute, @MadisonCollege, the Data Science Hub, the @UWMadLibraries and @UWMadisonExt. https://datascience.wisc.edu/2023/07/11/data-science-institute-to-establish-campus-open-source-program-office/…
------
One of the weird things about working in the #Python community is that our core technologies are mostly not written in Python.

If someone wants to contribute code, now they have two problems.

- Michael Droettboom at #SciPy2023
------
We use self-supervised learning to pretrain LLMs (e.g., next-word prediction). 

Here's an interesting take using self-supervised learning for evaluating LLMs: https://arxiv.org/abs//2306.13651
Turns out, there's correlation between self-supervised evaluations & human evaluations.
------
Excited for LLM Summer at #SDSC23! Don't miss 
@rasbt
 keynote on "LLMs for Everything & Everyone". Dive into top-tier #machinelearning insights. Check out his renowned Ahead of AI newsletter: http://magazine.sebastianraschka.com
------
"Python vs R" is now finally solved  
------
Hi #EconTwitter!

Exciting news: 

@robtibshirani-@daniela_witten-James-Hastie-Taylor have released the #python version of the influential 𝐄𝐥𝐞𝐦𝐞𝐧𝐭𝐬 𝐨𝐟 𝐒𝐭𝐚𝐭𝐢𝐬𝐭𝐢𝐜𝐚𝐥 𝐋𝐞𝐚𝐫𝐧𝐢𝐧𝐠!

Course, datasets, notebook (and more) included. 

Don't miss out!
------
 Loved 
@rasbt
's PyTorch tutorial at 
@SciPyConf
 today. I learned how to accelerate model training, organize PyTorch code, Lightning tips, and fine-tune models.

Check out the tutorial repository here  https://github.com/rasbt/scipy2023-deeplearning/…
------
Just arrived in Austin and looking forward to attending SciPy once more! 

Ready and excited for my workshop on “Modern Deep Learning with PyTorch” tomorrow! 

https://github.com/rasbt/scipy2023-deeplearning…
------
For those who complain about Google Search ads: Ads in 2024 will be become a lot more subtle ...
------
New paper: 
LLMs can be fine-tuned with tiny amounts of data. This makes LLMs useful, but also exploitable; tweaked training data can cause bad behavior.

This LLM is tweaked to inject product placement for McDonalds into stuff. Not sure if I'm entertained or disturbed.  [1/3]
------
In the last couple of days, we talked a lot about extending the context window of transformer LLMs. Here's one more: "Extending Context Window of Large Language Models via Positional Interpolation" 

1/3
------
*the focus here is on improving modeling performance, not computational performance.
------
Rotary positional embeddings (aka RoPE) have been a recent cornerstone of modern LLM implementations since it supports flexible sequence lengths. In this paper, researchers propose Position Interpolation to increase RoPE-based context window sizes to 32,768 tokens 

2/3
------
This sounds a bit more humble than the recent 1M or 1B context lengths claims.

But this requires only minimal (1000 steps) finetuning. And it  allows long document summarization using LLaMA 7B and 65B models, for example.

Link to the paper here: https://arxiv.org/abs/2306.15595

3/3
------
What books are you reading this week? I'm currently reading Machine Learning Q and AI by 
@rasbt
 and I'm thoroughly enjoying it! 

Here is the book link: https://leanpub.com/machine-learning-q-and-ai…

Join our book club if you are interested in reading this book together: http://dsbookclub.github.io
------
We have seen a new wave of LLMs for longer contexts: 1) RMT, 2) Hyena LLM & 3) LongNet

There are several use-cases for such long LLMs but the elephant in the room is: How well do LLMs use these longer contexts? 
Turns out not so well if info is in the middle of the input.
1/5
------
2) To my knowledge, there is no specific inductive bias in transformer-based LLM architectures that explains why the retrieval performance should be worse for text in the middle of the document. 

4/5
------
I suspect it is all because of the training data and how humans write: the most important information is usually in the beginning or the end (think paper Abstracts and Conclusion sections), and it's then how LLMs parameterize the attention weights during training.

5/5
------
We had 1) the RMT paper on scaling Transformers to 1M tokens, and 2) the convolutional Hyena LLM for 1M tokens. 

How about 3) LONGNET: Scaling Transformers to 1 Billion Tokens (https://arxiv.org/abs/2307.02486)!?

It achieves linear (vs quadratic) scaling via dilated (vs self) attention.
------
Of course I couldn’t resist giving the other app (threads) a try. 

I am ‘sebastianraschka’ over there.
------
Felt the urge to write about computer vision for a change! 

Summarizing the main themes from CVPR:
1) Vision transformers
2) Diffusion models
3) NeRFs
4) Object detection & segmentation

Recapping it in more detail along with some paper highlights here:
------
In short, the idea behind this gzip + kNN approach is that a given text A is close to the size of two concatenated texts A + B when compressed using Gzip. 

Or, the compressed size of A+A is similar to the compressed size of just A (plus a constant value for the back reference)
------
Gzip + kNN beats transformers on text classification.

(Gzip as in good old zip file compression) twitter.com/LukeGessler/st…
------
Gzip + kNN beats transformers on text classification.

(Gzip as in good old zip file compression)
------
this paper's nuts. for sentence classification on out-of-domain datasets, all neural (Transformer or not) approaches lose to good old kNN on representations generated by.... gzip https://aclanthology.org/2023.findings-acl.426/…
------
Part of my vision for 
@datascience_uw
 is to foster an #OpenSource ethos on campus and to develop scalable, distributed approaches to contribute to the Open Source communities that we rely on. I’m thrilled to launch the OSPO 
@UWMadison
. Thank 
@SloanFoundation
 for supporting it!
------
.@UWMadison is one of six universities recently selected by @SloanFoundation to establish #OpenSource Program Offices. Partners include the Data Science Institute, @MadisonCollege, the Data Science Hub, the @UWMadLibraries and @UWMadisonExt. https://datascience.wisc.edu/2023/07/11/data-science-institute-to-establish-campus-open-source-program-office/…
------
One of the weird things about working in the #Python community is that our core technologies are mostly not written in Python.

If someone wants to contribute code, now they have two problems.

- Michael Droettboom at #SciPy2023
------
We use self-supervised learning to pretrain LLMs (e.g., next-word prediction). 

Here's an interesting take using self-supervised learning for evaluating LLMs: https://arxiv.org/abs//2306.13651
Turns out, there's correlation between self-supervised evaluations & human evaluations.
------
Excited for LLM Summer at #SDSC23! Don't miss 
@rasbt
 keynote on "LLMs for Everything & Everyone". Dive into top-tier #machinelearning insights. Check out his renowned Ahead of AI newsletter: http://magazine.sebastianraschka.com
------
"Python vs R" is now finally solved  
------
Hi #EconTwitter!

Exciting news: 

@robtibshirani-@daniela_witten-James-Hastie-Taylor have released the #python version of the influential 𝐄𝐥𝐞𝐦𝐞𝐧𝐭𝐬 𝐨𝐟 𝐒𝐭𝐚𝐭𝐢𝐬𝐭𝐢𝐜𝐚𝐥 𝐋𝐞𝐚𝐫𝐧𝐢𝐧𝐠!

Course, datasets, notebook (and more) included. 

Don't miss out!
------
 Loved 
@rasbt
's PyTorch tutorial at 
@SciPyConf
 today. I learned how to accelerate model training, organize PyTorch code, Lightning tips, and fine-tune models.

Check out the tutorial repository here  https://github.com/rasbt/scipy2023-deeplearning/…
------
Just arrived in Austin and looking forward to attending SciPy once more! 

Ready and excited for my workshop on “Modern Deep Learning with PyTorch” tomorrow! 

https://github.com/rasbt/scipy2023-deeplearning…
------
For those who complain about Google Search ads: Ads in 2024 will be become a lot more subtle ...
------
New paper: 
LLMs can be fine-tuned with tiny amounts of data. This makes LLMs useful, but also exploitable; tweaked training data can cause bad behavior.

This LLM is tweaked to inject product placement for McDonalds into stuff. Not sure if I'm entertained or disturbed.  [1/3]
------
In the last couple of days, we talked a lot about extending the context window of transformer LLMs. Here's one more: "Extending Context Window of Large Language Models via Positional Interpolation" 

1/3
------
*the focus here is on improving modeling performance, not computational performance.
------
Rotary positional embeddings (aka RoPE) have been a recent cornerstone of modern LLM implementations since it supports flexible sequence lengths. In this paper, researchers propose Position Interpolation to increase RoPE-based context window sizes to 32,768 tokens 

2/3
------
This sounds a bit more humble than the recent 1M or 1B context lengths claims.

But this requires only minimal (1000 steps) finetuning. And it  allows long document summarization using LLaMA 7B and 65B models, for example.

Link to the paper here: https://arxiv.org/abs/2306.15595

3/3
------
What books are you reading this week? I'm currently reading Machine Learning Q and AI by 
@rasbt
 and I'm thoroughly enjoying it! 

Here is the book link: https://leanpub.com/machine-learning-q-and-ai…

Join our book club if you are interested in reading this book together: http://dsbookclub.github.io
------
We have seen a new wave of LLMs for longer contexts: 1) RMT, 2) Hyena LLM & 3) LongNet

There are several use-cases for such long LLMs but the elephant in the room is: How well do LLMs use these longer contexts? 
Turns out not so well if info is in the middle of the input.
1/5
------
2) To my knowledge, there is no specific inductive bias in transformer-based LLM architectures that explains why the retrieval performance should be worse for text in the middle of the document. 

4/5
------
I suspect it is all because of the training data and how humans write: the most important information is usually in the beginning or the end (think paper Abstracts and Conclusion sections), and it's then how LLMs parameterize the attention weights during training.

5/5
------
We had 1) the RMT paper on scaling Transformers to 1M tokens, and 2) the convolutional Hyena LLM for 1M tokens. 

How about 3) LONGNET: Scaling Transformers to 1 Billion Tokens (https://arxiv.org/abs/2307.02486)!?

It achieves linear (vs quadratic) scaling via dilated (vs self) attention.
------
Of course I couldn’t resist giving the other app (threads) a try. 

I am ‘sebastianraschka’ over there.
------
Felt the urge to write about computer vision for a change! 

Summarizing the main themes from CVPR:
1) Vision transformers
2) Diffusion models
3) NeRFs
4) Object detection & segmentation

Recapping it in more detail along with some paper highlights here:
------
Conda and the libmamba solver roll-out plan 2023:
- July (soon): conda-libmamba-solver will be included in the installers (Anaconda Distribution, miniconda, miniforge) for ease of use
- September (conda>=23.9): conda will switch to libmamba as default
------
The best minds of my generation are thinking about how to install Python.
------
What is "the right way" to install Python on a new M2 MacBook? I assume it isn't the system Python3 right? Maybe Homebrew?
------
It doesn't get much attention these days (in both senses) but a new version of 
@scikit_learn
, my favorite machine learning library, is out!

- PyTorch support for LinearDiscriminant Analysis
- Validation Curves
- Decision tree with N/A features
- and more
------
Just wanted to highlight a fantastic article by 
@rasbt
 and the 
@LightningAI
 team!

 Article: https://lightning.ai/pages/community/tutorial/pytorch-memory-vit-llm/…

 Repo: https://github.com/rasbt/pytorch-memory-optim…

If you've ever wanted an introduction to training optimization - this is it!

#generativeai #llms #lightningai
------
We use self-supervised learning to pretrain LLMs (e.g., next-word prediction). 

Here's an interesting take using self-supervised learning for evaluating LLMs: https://arxiv.org/abs//2306.13651
Turns out, there's correlation between self-supervised evaluations & human evaluations.
------
Excited for LLM Summer at #SDSC23! Don't miss 
@rasbt
 keynote on "LLMs for Everything & Everyone". Dive into top-tier #machinelearning insights. Check out his renowned Ahead of AI newsletter: http://magazine.sebastianraschka.com
------
"Python vs R" is now finally solved  
------
Hi #EconTwitter!

Exciting news: 

@robtibshirani-@daniela_witten-James-Hastie-Taylor have released the #python version of the influential 𝐄𝐥𝐞𝐦𝐞𝐧𝐭𝐬 𝐨𝐟 𝐒𝐭𝐚𝐭𝐢𝐬𝐭𝐢𝐜𝐚𝐥 𝐋𝐞𝐚𝐫𝐧𝐢𝐧𝐠!

Course, datasets, notebook (and more) included. 

Don't miss out!
------
 Loved 
@rasbt
's PyTorch tutorial at 
@SciPyConf
 today. I learned how to accelerate model training, organize PyTorch code, Lightning tips, and fine-tune models.

Check out the tutorial repository here  https://github.com/rasbt/scipy2023-deeplearning/…
------
Just arrived in Austin and looking forward to attending SciPy once more! 

Ready and excited for my workshop on “Modern Deep Learning with PyTorch” tomorrow! 

https://github.com/rasbt/scipy2023-deeplearning…
------
For those who complain about Google Search ads: Ads in 2024 will be become a lot more subtle ...
------
New paper: 
LLMs can be fine-tuned with tiny amounts of data. This makes LLMs useful, but also exploitable; tweaked training data can cause bad behavior.

This LLM is tweaked to inject product placement for McDonalds into stuff. Not sure if I'm entertained or disturbed.  [1/3]
------
In the last couple of days, we talked a lot about extending the context window of transformer LLMs. Here's one more: "Extending Context Window of Large Language Models via Positional Interpolation" 

1/3
------
*the focus here is on improving modeling performance, not computational performance.
------
Rotary positional embeddings (aka RoPE) have been a recent cornerstone of modern LLM implementations since it supports flexible sequence lengths. In this paper, researchers propose Position Interpolation to increase RoPE-based context window sizes to 32,768 tokens 

2/3
------
This sounds a bit more humble than the recent 1M or 1B context lengths claims.

But this requires only minimal (1000 steps) finetuning. And it  allows long document summarization using LLaMA 7B and 65B models, for example.

Link to the paper here: https://arxiv.org/abs/2306.15595

3/3
------
What books are you reading this week? I'm currently reading Machine Learning Q and AI by 
@rasbt
 and I'm thoroughly enjoying it! 

Here is the book link: https://leanpub.com/machine-learning-q-and-ai…

Join our book club if you are interested in reading this book together: http://dsbookclub.github.io
------
We have seen a new wave of LLMs for longer contexts: 1) RMT, 2) Hyena LLM & 3) LongNet

There are several use-cases for such long LLMs but the elephant in the room is: How well do LLMs use these longer contexts? 
Turns out not so well if info is in the middle of the input.
1/5
------
2) To my knowledge, there is no specific inductive bias in transformer-based LLM architectures that explains why the retrieval performance should be worse for text in the middle of the document. 

4/5
------
I suspect it is all because of the training data and how humans write: the most important information is usually in the beginning or the end (think paper Abstracts and Conclusion sections), and it's then how LLMs parameterize the attention weights during training.

5/5
------
We had 1) the RMT paper on scaling Transformers to 1M tokens, and 2) the convolutional Hyena LLM for 1M tokens. 

How about 3) LONGNET: Scaling Transformers to 1 Billion Tokens (https://arxiv.org/abs/2307.02486)!?

It achieves linear (vs quadratic) scaling via dilated (vs self) attention.
------
Of course I couldn’t resist giving the other app (threads) a try. 

I am ‘sebastianraschka’ over there.
------
Felt the urge to write about computer vision for a change! 

Summarizing the main themes from CVPR:
1) Vision transformers
2) Diffusion models
3) NeRFs
4) Object detection & segmentation

Recapping it in more detail along with some paper highlights here:
------
Conda and the libmamba solver roll-out plan 2023:
- July (soon): conda-libmamba-solver will be included in the installers (Anaconda Distribution, miniconda, miniforge) for ease of use
- September (conda>=23.9): conda will switch to libmamba as default
------
The best minds of my generation are thinking about how to install Python.
------
What is "the right way" to install Python on a new M2 MacBook? I assume it isn't the system Python3 right? Maybe Homebrew?
------
It doesn't get much attention these days (in both senses) but a new version of 
@scikit_learn
, my favorite machine learning library, is out!

- PyTorch support for LinearDiscriminant Analysis
- Validation Curves
- Decision tree with N/A features
- and more
------
Just wanted to highlight a fantastic article by 
@rasbt
 and the 
@LightningAI
 team!

 Article: https://lightning.ai/pages/community/tutorial/pytorch-memory-vit-llm/…

 Repo: https://github.com/rasbt/pytorch-memory-optim…

If you've ever wanted an introduction to training optimization - this is it!

#generativeai #llms #lightningai
------
One of the big bottlenecks with LLMs & Vision Transformers is GPU memory on consumer devices.   

Wrote about my favorite techniques for reducing peak memory in PyTorch: https://lightning.ai/pages/community/tutorial/pytorch-memory-vit-llm/…

Focused on techniques that don't require architecture changes! Suggestions welcome!
------
Seb (@rasbt) shares 8 techniques to train LLMs and vision transformers on consumer GPUs without compromising accuracy! Check it out  https://lightning.ai/pages/community/tutorial/pytorch-memory-vit-llm/…

#MachineLearning #LLM #opensource
------
This is weird to say out loud, but I actually am kinda an expert in rate limiting, so I'm gonna explain some stuff.

About half of incidents in large-scale production systems involve having more requests than you can serve. There are two categories of this kind of incident:
------
"Python vs R" is now finally solved  
------
Hi #EconTwitter!

Exciting news: 

@robtibshirani-@daniela_witten-James-Hastie-Taylor have released the #python version of the influential 𝐄𝐥𝐞𝐦𝐞𝐧𝐭𝐬 𝐨𝐟 𝐒𝐭𝐚𝐭𝐢𝐬𝐭𝐢𝐜𝐚𝐥 𝐋𝐞𝐚𝐫𝐧𝐢𝐧𝐠!

Course, datasets, notebook (and more) included. 

Don't miss out!
------
 Loved 
@rasbt
's PyTorch tutorial at 
@SciPyConf
 today. I learned how to accelerate model training, organize PyTorch code, Lightning tips, and fine-tune models.

Check out the tutorial repository here  https://github.com/rasbt/scipy2023-deeplearning/…
------
Just arrived in Austin and looking forward to attending SciPy once more! 

Ready and excited for my workshop on “Modern Deep Learning with PyTorch” tomorrow! 

https://github.com/rasbt/scipy2023-deeplearning…
------
For those who complain about Google Search ads: Ads in 2024 will be become a lot more subtle ...
------
New paper: 
LLMs can be fine-tuned with tiny amounts of data. This makes LLMs useful, but also exploitable; tweaked training data can cause bad behavior.

This LLM is tweaked to inject product placement for McDonalds into stuff. Not sure if I'm entertained or disturbed.  [1/3]
------
In the last couple of days, we talked a lot about extending the context window of transformer LLMs. Here's one more: "Extending Context Window of Large Language Models via Positional Interpolation" 

1/3
------
*the focus here is on improving modeling performance, not computational performance.
------
Rotary positional embeddings (aka RoPE) have been a recent cornerstone of modern LLM implementations since it supports flexible sequence lengths. In this paper, researchers propose Position Interpolation to increase RoPE-based context window sizes to 32,768 tokens 

2/3
------
This sounds a bit more humble than the recent 1M or 1B context lengths claims.

But this requires only minimal (1000 steps) finetuning. And it  allows long document summarization using LLaMA 7B and 65B models, for example.

Link to the paper here: https://arxiv.org/abs/2306.15595

3/3
------
What books are you reading this week? I'm currently reading Machine Learning Q and AI by 
@rasbt
 and I'm thoroughly enjoying it! 

Here is the book link: https://leanpub.com/machine-learning-q-and-ai…

Join our book club if you are interested in reading this book together: http://dsbookclub.github.io
------
We have seen a new wave of LLMs for longer contexts: 1) RMT, 2) Hyena LLM & 3) LongNet

There are several use-cases for such long LLMs but the elephant in the room is: How well do LLMs use these longer contexts? 
Turns out not so well if info is in the middle of the input.
1/5
------
2) To my knowledge, there is no specific inductive bias in transformer-based LLM architectures that explains why the retrieval performance should be worse for text in the middle of the document. 

4/5
------
I suspect it is all because of the training data and how humans write: the most important information is usually in the beginning or the end (think paper Abstracts and Conclusion sections), and it's then how LLMs parameterize the attention weights during training.

5/5
------
We had 1) the RMT paper on scaling Transformers to 1M tokens, and 2) the convolutional Hyena LLM for 1M tokens. 

How about 3) LONGNET: Scaling Transformers to 1 Billion Tokens (https://arxiv.org/abs/2307.02486)!?

It achieves linear (vs quadratic) scaling via dilated (vs self) attention.
------
Of course I couldn’t resist giving the other app (threads) a try. 

I am ‘sebastianraschka’ over there.
------
Felt the urge to write about computer vision for a change! 

Summarizing the main themes from CVPR:
1) Vision transformers
2) Diffusion models
3) NeRFs
4) Object detection & segmentation

Recapping it in more detail along with some paper highlights here:
------
Conda and the libmamba solver roll-out plan 2023:
- July (soon): conda-libmamba-solver will be included in the installers (Anaconda Distribution, miniconda, miniforge) for ease of use
- September (conda>=23.9): conda will switch to libmamba as default
------
The best minds of my generation are thinking about how to install Python.
------
What is "the right way" to install Python on a new M2 MacBook? I assume it isn't the system Python3 right? Maybe Homebrew?
------
It doesn't get much attention these days (in both senses) but a new version of 
@scikit_learn
, my favorite machine learning library, is out!

- PyTorch support for LinearDiscriminant Analysis
- Validation Curves
- Decision tree with N/A features
- and more
------
Just wanted to highlight a fantastic article by 
@rasbt
 and the 
@LightningAI
 team!

 Article: https://lightning.ai/pages/community/tutorial/pytorch-memory-vit-llm/…

 Repo: https://github.com/rasbt/pytorch-memory-optim…

If you've ever wanted an introduction to training optimization - this is it!

#generativeai #llms #lightningai
------
One of the big bottlenecks with LLMs & Vision Transformers is GPU memory on consumer devices.   

Wrote about my favorite techniques for reducing peak memory in PyTorch: https://lightning.ai/pages/community/tutorial/pytorch-memory-vit-llm/…

Focused on techniques that don't require architecture changes! Suggestions welcome!
------
Seb (@rasbt) shares 8 techniques to train LLMs and vision transformers on consumer GPUs without compromising accuracy! Check it out  https://lightning.ai/pages/community/tutorial/pytorch-memory-vit-llm/…

#MachineLearning #LLM #opensource
------
This is weird to say out loud, but I actually am kinda an expert in rate limiting, so I'm gonna explain some stuff.

About half of incidents in large-scale production systems involve having more requests than you can serve. There are two categories of this kind of incident:
------
In case your are looking for some content to read during the Twitter outage: https://magazine.sebastianraschka.com 
------
Pretty excited to me attending my favorite computing conference once more!

If I’ve been a bit quiet on socials lately that’s because I’m head down working on my deep learning tutorial for SciPy in a bit more than a week! Hope to see you there!
------
10 days left until #SciPy2023

From an intro to #pythonprogramming or a deep dive into creating visualizations of MASSIVE #data sets, our two-days of #tutorials has something for everybody! 

July 10-11, don't forget to register! https://scipy2023.scipy.org/tutorials
------
And the recent "How Far Can Camels Go" paper (https://arxiv.org/abs/2306.04751) is a nice example of how important the dataset for LLM finetuning is.
------
LLMs are usually too large for most contexts, but creating pruned versions of a model usually require retraining.

Here's a new straightforward alternative based on computing element-wise product between the weight magnitude and norm of input activations: https://arxiv.org/abs/2306.11695
------
Huge news! Big congrats to both teams 
@databricks
 & 
@MosaicML
 and everyone involved! 

I love all the recent research and work from 
@MosaicML
 and can’t wait to see what’s next!
------
Big news: we've agreed to acquire @MosaicML, a leading generative AI platform. I couldn’t be more excited to join forces once the deal closes. https://databricks.com/mosaic-news
------
"Accelerating PyTorch Model Training Using Mixed-Precision and Fully Sharded Data Parallelism"
https://magazine.sebastianraschka.com/p/accelerating-pytorch-model-training…

A little write-up of a talk I gave last week! 
------
In the last couple of days, we talked a lot about extending the context window of transformer LLMs. Here's one more: "Extending Context Window of Large Language Models via Positional Interpolation" 

1/3
------
*the focus here is on improving modeling performance, not computational performance.
------
Rotary positional embeddings (aka RoPE) have been a recent cornerstone of modern LLM implementations since it supports flexible sequence lengths. In this paper, researchers propose Position Interpolation to increase RoPE-based context window sizes to 32,768 tokens 

2/3
------
This sounds a bit more humble than the recent 1M or 1B context lengths claims.

But this requires only minimal (1000 steps) finetuning. And it  allows long document summarization using LLaMA 7B and 65B models, for example.

Link to the paper here: https://arxiv.org/abs/2306.15595

3/3
------
What books are you reading this week? I'm currently reading Machine Learning Q and AI by 
@rasbt
 and I'm thoroughly enjoying it! 

Here is the book link: https://leanpub.com/machine-learning-q-and-ai…

Join our book club if you are interested in reading this book together: http://dsbookclub.github.io
------
We have seen a new wave of LLMs for longer contexts: 1) RMT, 2) Hyena LLM & 3) LongNet

There are several use-cases for such long LLMs but the elephant in the room is: How well do LLMs use these longer contexts? 
Turns out not so well if info is in the middle of the input.
1/5
------
2) To my knowledge, there is no specific inductive bias in transformer-based LLM architectures that explains why the retrieval performance should be worse for text in the middle of the document. 

4/5
------
I suspect it is all because of the training data and how humans write: the most important information is usually in the beginning or the end (think paper Abstracts and Conclusion sections), and it's then how LLMs parameterize the attention weights during training.

5/5
------
We had 1) the RMT paper on scaling Transformers to 1M tokens, and 2) the convolutional Hyena LLM for 1M tokens. 

How about 3) LONGNET: Scaling Transformers to 1 Billion Tokens (https://arxiv.org/abs/2307.02486)!?

It achieves linear (vs quadratic) scaling via dilated (vs self) attention.
------
Of course I couldn’t resist giving the other app (threads) a try. 

I am ‘sebastianraschka’ over there.
------
Felt the urge to write about computer vision for a change! 

Summarizing the main themes from CVPR:
1) Vision transformers
2) Diffusion models
3) NeRFs
4) Object detection & segmentation

Recapping it in more detail along with some paper highlights here:
------
Conda and the libmamba solver roll-out plan 2023:
- July (soon): conda-libmamba-solver will be included in the installers (Anaconda Distribution, miniconda, miniforge) for ease of use
- September (conda>=23.9): conda will switch to libmamba as default
------
The best minds of my generation are thinking about how to install Python.
------
What is "the right way" to install Python on a new M2 MacBook? I assume it isn't the system Python3 right? Maybe Homebrew?
------
It doesn't get much attention these days (in both senses) but a new version of 
@scikit_learn
, my favorite machine learning library, is out!

- PyTorch support for LinearDiscriminant Analysis
- Validation Curves
- Decision tree with N/A features
- and more
------
Just wanted to highlight a fantastic article by 
@rasbt
 and the 
@LightningAI
 team!

 Article: https://lightning.ai/pages/community/tutorial/pytorch-memory-vit-llm/…

 Repo: https://github.com/rasbt/pytorch-memory-optim…

If you've ever wanted an introduction to training optimization - this is it!

#generativeai #llms #lightningai
------
One of the big bottlenecks with LLMs & Vision Transformers is GPU memory on consumer devices.   

Wrote about my favorite techniques for reducing peak memory in PyTorch: https://lightning.ai/pages/community/tutorial/pytorch-memory-vit-llm/…

Focused on techniques that don't require architecture changes! Suggestions welcome!
------
Seb (@rasbt) shares 8 techniques to train LLMs and vision transformers on consumer GPUs without compromising accuracy! Check it out  https://lightning.ai/pages/community/tutorial/pytorch-memory-vit-llm/…

#MachineLearning #LLM #opensource
------
This is weird to say out loud, but I actually am kinda an expert in rate limiting, so I'm gonna explain some stuff.

About half of incidents in large-scale production systems involve having more requests than you can serve. There are two categories of this kind of incident:
------
In case your are looking for some content to read during the Twitter outage: https://magazine.sebastianraschka.com 
------
Pretty excited to me attending my favorite computing conference once more!

If I’ve been a bit quiet on socials lately that’s because I’m head down working on my deep learning tutorial for SciPy in a bit more than a week! Hope to see you there!
------
10 days left until #SciPy2023

From an intro to #pythonprogramming or a deep dive into creating visualizations of MASSIVE #data sets, our two-days of #tutorials has something for everybody! 

July 10-11, don't forget to register! https://scipy2023.scipy.org/tutorials
------
And the recent "How Far Can Camels Go" paper (https://arxiv.org/abs/2306.04751) is a nice example of how important the dataset for LLM finetuning is.
------
LLMs are usually too large for most contexts, but creating pruned versions of a model usually require retraining.

Here's a new straightforward alternative based on computing element-wise product between the weight magnitude and norm of input activations: https://arxiv.org/abs/2306.11695
------
Huge news! Big congrats to both teams 
@databricks
 & 
@MosaicML
 and everyone involved! 

I love all the recent research and work from 
@MosaicML
 and can’t wait to see what’s next!
------
Big news: we've agreed to acquire @MosaicML, a leading generative AI platform. I couldn’t be more excited to join forces once the deal closes. https://databricks.com/mosaic-news
------
"Accelerating PyTorch Model Training Using Mixed-Precision and Fully Sharded Data Parallelism"
https://magazine.sebastianraschka.com/p/accelerating-pytorch-model-training…

A little write-up of a talk I gave last week! 
------
The "Sweet Lesson" is that using LLMs is simpler than we thought!

Instead of complicated symbolic computations, we download a pretrained open-source LLM and finetune it in supervised fashion via <100 lines of PyTorch code.
------
The "Bitter Lesson" is a statement about computation alone, but is it true without data? Chess is very unique, and Go was solved thanks to data (generated with compute in that case indeed, but that's more an exception than the rule) @RichardSSutton
------
Reading 
@rasbt
's insightful book. Absolutely loving his compelling description of self-supervised pre-training vs transfer-learning.
#MachineLearning A definite must-read! #CurrentReads 

https://leanpub.com/machine-learning-q-and-ai…
------
If you are at #CVPR2023, I'll be giving a short talk on "Scaling PyTorch Model Training With Minimal Code Changes" at 10:30 - 10:45 today (talk area of the Exhibit Hall)

(Will be at the Lightning AI booth afterwards & happy to chat as well )
------
Don’t miss @rasbt ’s CVPR talk on training vision transformers 10x faster using Lightning Fabric. Join us at 10:30 AM at Booth 1718!

#CVPR2023 #AI #ComputerVision
------
We have seen a new wave of LLMs for longer contexts: 1) RMT, 2) Hyena LLM & 3) LongNet

There are several use-cases for such long LLMs but the elephant in the room is: How well do LLMs use these longer contexts? 
Turns out not so well if info is in the middle of the input.
1/5
------
2) To my knowledge, there is no specific inductive bias in transformer-based LLM architectures that explains why the retrieval performance should be worse for text in the middle of the document. 

4/5
------
I suspect it is all because of the training data and how humans write: the most important information is usually in the beginning or the end (think paper Abstracts and Conclusion sections), and it's then how LLMs parameterize the attention weights during training.

5/5
------
We had 1) the RMT paper on scaling Transformers to 1M tokens, and 2) the convolutional Hyena LLM for 1M tokens. 

How about 3) LONGNET: Scaling Transformers to 1 Billion Tokens (https://arxiv.org/abs/2307.02486)!?

It achieves linear (vs quadratic) scaling via dilated (vs self) attention.
------
Of course I couldn’t resist giving the other app (threads) a try. 

I am ‘sebastianraschka’ over there.
------
Felt the urge to write about computer vision for a change! 

Summarizing the main themes from CVPR:
1) Vision transformers
2) Diffusion models
3) NeRFs
4) Object detection & segmentation

Recapping it in more detail along with some paper highlights here:
------
Conda and the libmamba solver roll-out plan 2023:
- July (soon): conda-libmamba-solver will be included in the installers (Anaconda Distribution, miniconda, miniforge) for ease of use
- September (conda>=23.9): conda will switch to libmamba as default
------
The best minds of my generation are thinking about how to install Python.
------
What is "the right way" to install Python on a new M2 MacBook? I assume it isn't the system Python3 right? Maybe Homebrew?
------
It doesn't get much attention these days (in both senses) but a new version of 
@scikit_learn
, my favorite machine learning library, is out!

- PyTorch support for LinearDiscriminant Analysis
- Validation Curves
- Decision tree with N/A features
- and more
------
Just wanted to highlight a fantastic article by 
@rasbt
 and the 
@LightningAI
 team!

 Article: https://lightning.ai/pages/community/tutorial/pytorch-memory-vit-llm/…

 Repo: https://github.com/rasbt/pytorch-memory-optim…

If you've ever wanted an introduction to training optimization - this is it!

#generativeai #llms #lightningai
------
One of the big bottlenecks with LLMs & Vision Transformers is GPU memory on consumer devices.   

Wrote about my favorite techniques for reducing peak memory in PyTorch: https://lightning.ai/pages/community/tutorial/pytorch-memory-vit-llm/…

Focused on techniques that don't require architecture changes! Suggestions welcome!
------
Seb (@rasbt) shares 8 techniques to train LLMs and vision transformers on consumer GPUs without compromising accuracy! Check it out  https://lightning.ai/pages/community/tutorial/pytorch-memory-vit-llm/…

#MachineLearning #LLM #opensource
------
This is weird to say out loud, but I actually am kinda an expert in rate limiting, so I'm gonna explain some stuff.

About half of incidents in large-scale production systems involve having more requests than you can serve. There are two categories of this kind of incident:
------
In case your are looking for some content to read during the Twitter outage: https://magazine.sebastianraschka.com 
------
Pretty excited to me attending my favorite computing conference once more!

If I’ve been a bit quiet on socials lately that’s because I’m head down working on my deep learning tutorial for SciPy in a bit more than a week! Hope to see you there!
------
10 days left until #SciPy2023

From an intro to #pythonprogramming or a deep dive into creating visualizations of MASSIVE #data sets, our two-days of #tutorials has something for everybody! 

July 10-11, don't forget to register! https://scipy2023.scipy.org/tutorials
------
And the recent "How Far Can Camels Go" paper (https://arxiv.org/abs/2306.04751) is a nice example of how important the dataset for LLM finetuning is.
------
LLMs are usually too large for most contexts, but creating pruned versions of a model usually require retraining.

Here's a new straightforward alternative based on computing element-wise product between the weight magnitude and norm of input activations: https://arxiv.org/abs/2306.11695
------
Huge news! Big congrats to both teams 
@databricks
 & 
@MosaicML
 and everyone involved! 

I love all the recent research and work from 
@MosaicML
 and can’t wait to see what’s next!
------
Big news: we've agreed to acquire @MosaicML, a leading generative AI platform. I couldn’t be more excited to join forces once the deal closes. https://databricks.com/mosaic-news
------
"Accelerating PyTorch Model Training Using Mixed-Precision and Fully Sharded Data Parallelism"
https://magazine.sebastianraschka.com/p/accelerating-pytorch-model-training…

A little write-up of a talk I gave last week! 
------
The "Sweet Lesson" is that using LLMs is simpler than we thought!

Instead of complicated symbolic computations, we download a pretrained open-source LLM and finetune it in supervised fashion via <100 lines of PyTorch code.
------
The "Bitter Lesson" is a statement about computation alone, but is it true without data? Chess is very unique, and Go was solved thanks to data (generated with compute in that case indeed, but that's more an exception than the rule) @RichardSSutton
------
Reading 
@rasbt
's insightful book. Absolutely loving his compelling description of self-supervised pre-training vs transfer-learning.
#MachineLearning A definite must-read! #CurrentReads 

https://leanpub.com/machine-learning-q-and-ai…
------
If you are at #CVPR2023, I'll be giving a short talk on "Scaling PyTorch Model Training With Minimal Code Changes" at 10:30 - 10:45 today (talk area of the Exhibit Hall)

(Will be at the Lightning AI booth afterwards & happy to chat as well )
------
Don’t miss @rasbt ’s CVPR talk on training vision transformers 10x faster using Lightning Fabric. Join us at 10:30 AM at Booth 1718!

#CVPR2023 #AI #ComputerVision
------
Thanks everyone for your interest ! I think someone recorded it — will share once I know more and am back from the conference! In the meantime I also have all the materials here on GitHub:
------
So much discussion about arxiv this morning. 

You can get a lot of positive value out of using arxiv if you use it with care. 

Eg does the original Transformer paper have inconsistencies (see LayerNorm) and is not super detailed? Sure. 

Is it a groundbreaking paper. Yes!
------
I am thankful that arxiv exists, but of course we need to take papers there with a grain of salt. But this is also generally true for peer-reviewed papers as well.

As always, you have to judge it on a case-by-case basis.
------
I often get requests to dispel some of the jargon behind transformers and LLMs!

So here we go, my new article on "Understanding Encoder and Decoder LLMs":
------
Am so glad about all the nice feedback about my Deep Learning course -- happy to hear that it was useful to tens of thousands of people!

Just added a final exam & completion badge for LinkedIn!

So, if you'd like to test your knowledge with 25 questions:
https://lightning.ai/pages/ai-education/deep-learning-fundamentals/certification/…
------
Today we're excited to announce our Deep Learning Fundamentals Certification 

Once you tackle @rasbt's course, learn how to train models, and explore the basics of LLMs, you'll be ready to pass the exam and share the badge on LinkedIn!

Get started! http://lightning.ai/pages/courses/deep-learning-fundamentals/…
------
Felt the urge to write about computer vision for a change! 

Summarizing the main themes from CVPR:
1) Vision transformers
2) Diffusion models
3) NeRFs
4) Object detection & segmentation

Recapping it in more detail along with some paper highlights here:
------
Conda and the libmamba solver roll-out plan 2023:
- July (soon): conda-libmamba-solver will be included in the installers (Anaconda Distribution, miniconda, miniforge) for ease of use
- September (conda>=23.9): conda will switch to libmamba as default
------
The best minds of my generation are thinking about how to install Python.
------
What is "the right way" to install Python on a new M2 MacBook? I assume it isn't the system Python3 right? Maybe Homebrew?
------
It doesn't get much attention these days (in both senses) but a new version of 
@scikit_learn
, my favorite machine learning library, is out!

- PyTorch support for LinearDiscriminant Analysis
- Validation Curves
- Decision tree with N/A features
- and more
------
Just wanted to highlight a fantastic article by 
@rasbt
 and the 
@LightningAI
 team!

 Article: https://lightning.ai/pages/community/tutorial/pytorch-memory-vit-llm/…

 Repo: https://github.com/rasbt/pytorch-memory-optim…

If you've ever wanted an introduction to training optimization - this is it!

#generativeai #llms #lightningai
------
One of the big bottlenecks with LLMs & Vision Transformers is GPU memory on consumer devices.   

Wrote about my favorite techniques for reducing peak memory in PyTorch: https://lightning.ai/pages/community/tutorial/pytorch-memory-vit-llm/…

Focused on techniques that don't require architecture changes! Suggestions welcome!
------
Seb (@rasbt) shares 8 techniques to train LLMs and vision transformers on consumer GPUs without compromising accuracy! Check it out  https://lightning.ai/pages/community/tutorial/pytorch-memory-vit-llm/…

#MachineLearning #LLM #opensource
------
This is weird to say out loud, but I actually am kinda an expert in rate limiting, so I'm gonna explain some stuff.

About half of incidents in large-scale production systems involve having more requests than you can serve. There are two categories of this kind of incident:
------
In case your are looking for some content to read during the Twitter outage: https://magazine.sebastianraschka.com 
------
Pretty excited to me attending my favorite computing conference once more!

If I’ve been a bit quiet on socials lately that’s because I’m head down working on my deep learning tutorial for SciPy in a bit more than a week! Hope to see you there!
------
10 days left until #SciPy2023

From an intro to #pythonprogramming or a deep dive into creating visualizations of MASSIVE #data sets, our two-days of #tutorials has something for everybody! 

July 10-11, don't forget to register! https://scipy2023.scipy.org/tutorials
------
And the recent "How Far Can Camels Go" paper (https://arxiv.org/abs/2306.04751) is a nice example of how important the dataset for LLM finetuning is.
------
LLMs are usually too large for most contexts, but creating pruned versions of a model usually require retraining.

Here's a new straightforward alternative based on computing element-wise product between the weight magnitude and norm of input activations: https://arxiv.org/abs/2306.11695
------
Huge news! Big congrats to both teams 
@databricks
 & 
@MosaicML
 and everyone involved! 

I love all the recent research and work from 
@MosaicML
 and can’t wait to see what’s next!
------
Big news: we've agreed to acquire @MosaicML, a leading generative AI platform. I couldn’t be more excited to join forces once the deal closes. https://databricks.com/mosaic-news
------
"Accelerating PyTorch Model Training Using Mixed-Precision and Fully Sharded Data Parallelism"
https://magazine.sebastianraschka.com/p/accelerating-pytorch-model-training…

A little write-up of a talk I gave last week! 
------
The "Sweet Lesson" is that using LLMs is simpler than we thought!

Instead of complicated symbolic computations, we download a pretrained open-source LLM and finetune it in supervised fashion via <100 lines of PyTorch code.
------
The "Bitter Lesson" is a statement about computation alone, but is it true without data? Chess is very unique, and Go was solved thanks to data (generated with compute in that case indeed, but that's more an exception than the rule) @RichardSSutton
------
Reading 
@rasbt
's insightful book. Absolutely loving his compelling description of self-supervised pre-training vs transfer-learning.
#MachineLearning A definite must-read! #CurrentReads 

https://leanpub.com/machine-learning-q-and-ai…
------
If you are at #CVPR2023, I'll be giving a short talk on "Scaling PyTorch Model Training With Minimal Code Changes" at 10:30 - 10:45 today (talk area of the Exhibit Hall)

(Will be at the Lightning AI booth afterwards & happy to chat as well )
------
Don’t miss @rasbt ’s CVPR talk on training vision transformers 10x faster using Lightning Fabric. Join us at 10:30 AM at Booth 1718!

#CVPR2023 #AI #ComputerVision
------
Thanks everyone for your interest ! I think someone recorded it — will share once I know more and am back from the conference! In the meantime I also have all the materials here on GitHub:
------
So much discussion about arxiv this morning. 

You can get a lot of positive value out of using arxiv if you use it with care. 

Eg does the original Transformer paper have inconsistencies (see LayerNorm) and is not super detailed? Sure. 

Is it a groundbreaking paper. Yes!
------
I am thankful that arxiv exists, but of course we need to take papers there with a grain of salt. But this is also generally true for peer-reviewed papers as well.

As always, you have to judge it on a case-by-case basis.
------
I often get requests to dispel some of the jargon behind transformers and LLMs!

So here we go, my new article on "Understanding Encoder and Decoder LLMs":
------
Am so glad about all the nice feedback about my Deep Learning course -- happy to hear that it was useful to tens of thousands of people!

Just added a final exam & completion badge for LinkedIn!

So, if you'd like to test your knowledge with 25 questions:
https://lightning.ai/pages/ai-education/deep-learning-fundamentals/certification/…
------
Today we're excited to announce our Deep Learning Fundamentals Certification 

Once you tackle @rasbt's course, learn how to train models, and explore the basics of LLMs, you'll be ready to pass the exam and share the badge on LinkedIn!

Get started! http://lightning.ai/pages/courses/deep-learning-fundamentals/…
------
Have been heads-down porting LoRA (low-rank-adaptation) to finetune Falcon more efficiently and ran some performance benchmarks.

My longer write-up here: https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

Long story short: you can finetune Falcon in 1 h on a 52k dataset using a single GPU with 16 GB RAM.
------
Wanna save  and ship your LLMs to prod faster?

Learn how @rasbt finetunes Falcon on 1 GPU in just 1 hour!  https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

#LLMs #MachineLearning #AI #opensource
------
Btw, it's also a big reduction in memory: 16 gigs instead of 6 x 40 GB.

Due to the reduced parameter counts in the backward pass:

Full finetuning: 7,217,189,760
Adapter: 1,365,330
Adapter v2: 3,839,186
LoRA: 3,506,176

(I used  LoRA rank of 16 to match Adapter v2 above.)
------
Susan & I found MMLU performance jump 6-10 points in the 40s by formatting multiple choice as (A) not A in MMLU (for internal model). All evaluation of LLM's are broken. Evaluating a task requires marginalizing across all prompts that describe the task, not point estimate of one.
------
Never insult a scientist by suggesting they're p-hacking benchmarks.   twitter.com/suchenzang/sta…
------
One of the big bottlenecks with LLMs & Vision Transformers is GPU memory on consumer devices.   

Wrote about my favorite techniques for reducing peak memory in PyTorch: https://lightning.ai/pages/community/tutorial/pytorch-memory-vit-llm/…

Focused on techniques that don't require architecture changes! Suggestions welcome!
------
Seb (@rasbt) shares 8 techniques to train LLMs and vision transformers on consumer GPUs without compromising accuracy! Check it out  https://lightning.ai/pages/community/tutorial/pytorch-memory-vit-llm/…

#MachineLearning #LLM #opensource
------
This is weird to say out loud, but I actually am kinda an expert in rate limiting, so I'm gonna explain some stuff.

About half of incidents in large-scale production systems involve having more requests than you can serve. There are two categories of this kind of incident:
------
In case your are looking for some content to read during the Twitter outage: https://magazine.sebastianraschka.com 
------
Pretty excited to me attending my favorite computing conference once more!

If I’ve been a bit quiet on socials lately that’s because I’m head down working on my deep learning tutorial for SciPy in a bit more than a week! Hope to see you there!
------
10 days left until #SciPy2023

From an intro to #pythonprogramming or a deep dive into creating visualizations of MASSIVE #data sets, our two-days of #tutorials has something for everybody! 

July 10-11, don't forget to register! https://scipy2023.scipy.org/tutorials
------
And the recent "How Far Can Camels Go" paper (https://arxiv.org/abs/2306.04751) is a nice example of how important the dataset for LLM finetuning is.
------
LLMs are usually too large for most contexts, but creating pruned versions of a model usually require retraining.

Here's a new straightforward alternative based on computing element-wise product between the weight magnitude and norm of input activations: https://arxiv.org/abs/2306.11695
------
Huge news! Big congrats to both teams 
@databricks
 & 
@MosaicML
 and everyone involved! 

I love all the recent research and work from 
@MosaicML
 and can’t wait to see what’s next!
------
Big news: we've agreed to acquire @MosaicML, a leading generative AI platform. I couldn’t be more excited to join forces once the deal closes. https://databricks.com/mosaic-news
------
"Accelerating PyTorch Model Training Using Mixed-Precision and Fully Sharded Data Parallelism"
https://magazine.sebastianraschka.com/p/accelerating-pytorch-model-training…

A little write-up of a talk I gave last week! 
------
The "Sweet Lesson" is that using LLMs is simpler than we thought!

Instead of complicated symbolic computations, we download a pretrained open-source LLM and finetune it in supervised fashion via <100 lines of PyTorch code.
------
The "Bitter Lesson" is a statement about computation alone, but is it true without data? Chess is very unique, and Go was solved thanks to data (generated with compute in that case indeed, but that's more an exception than the rule) @RichardSSutton
------
Reading 
@rasbt
's insightful book. Absolutely loving his compelling description of self-supervised pre-training vs transfer-learning.
#MachineLearning A definite must-read! #CurrentReads 

https://leanpub.com/machine-learning-q-and-ai…
------
If you are at #CVPR2023, I'll be giving a short talk on "Scaling PyTorch Model Training With Minimal Code Changes" at 10:30 - 10:45 today (talk area of the Exhibit Hall)

(Will be at the Lightning AI booth afterwards & happy to chat as well )
------
Don’t miss @rasbt ’s CVPR talk on training vision transformers 10x faster using Lightning Fabric. Join us at 10:30 AM at Booth 1718!

#CVPR2023 #AI #ComputerVision
------
Thanks everyone for your interest ! I think someone recorded it — will share once I know more and am back from the conference! In the meantime I also have all the materials here on GitHub:
------
So much discussion about arxiv this morning. 

You can get a lot of positive value out of using arxiv if you use it with care. 

Eg does the original Transformer paper have inconsistencies (see LayerNorm) and is not super detailed? Sure. 

Is it a groundbreaking paper. Yes!
------
I am thankful that arxiv exists, but of course we need to take papers there with a grain of salt. But this is also generally true for peer-reviewed papers as well.

As always, you have to judge it on a case-by-case basis.
------
I often get requests to dispel some of the jargon behind transformers and LLMs!

So here we go, my new article on "Understanding Encoder and Decoder LLMs":
------
Am so glad about all the nice feedback about my Deep Learning course -- happy to hear that it was useful to tens of thousands of people!

Just added a final exam & completion badge for LinkedIn!

So, if you'd like to test your knowledge with 25 questions:
https://lightning.ai/pages/ai-education/deep-learning-fundamentals/certification/…
------
Today we're excited to announce our Deep Learning Fundamentals Certification 

Once you tackle @rasbt's course, learn how to train models, and explore the basics of LLMs, you'll be ready to pass the exam and share the badge on LinkedIn!

Get started! http://lightning.ai/pages/courses/deep-learning-fundamentals/…
------
Have been heads-down porting LoRA (low-rank-adaptation) to finetune Falcon more efficiently and ran some performance benchmarks.

My longer write-up here: https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

Long story short: you can finetune Falcon in 1 h on a 52k dataset using a single GPU with 16 GB RAM.
------
Wanna save  and ship your LLMs to prod faster?

Learn how @rasbt finetunes Falcon on 1 GPU in just 1 hour!  https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

#LLMs #MachineLearning #AI #opensource
------
Btw, it's also a big reduction in memory: 16 gigs instead of 6 x 40 GB.

Due to the reduced parameter counts in the backward pass:

Full finetuning: 7,217,189,760
Adapter: 1,365,330
Adapter v2: 3,839,186
LoRA: 3,506,176

(I used  LoRA rank of 16 to match Adapter v2 above.)
------
Susan & I found MMLU performance jump 6-10 points in the 40s by formatting multiple choice as (A) not A in MMLU (for internal model). All evaluation of LLM's are broken. Evaluating a task requires marginalizing across all prompts that describe the task, not point estimate of one.
------
Never insult a scientist by suggesting they're p-hacking benchmarks.   twitter.com/suchenzang/sta…
------
I really like the LLaMA-Adapter method! 
In a nutshell, it's about finetuning LLMs more efficiently. 

In a nutshell, it's adding a tunable prefix to the key and value tensors in the self-attention layers.

Fun fact: it's not specific to LLaMA. Use it with any LLM!
------
Exciting news: LLaMA-Adapter is now fully unlocked! 6

 As a general-purpose #multimodal foundation model, it integrates various inputs like images, audio, text, video, and 3D point clouds, while providing image, text-based, and detection outputs. It uniquely accepts the… Show more
------
Reading  Machine Learning Q and AI from 
@rasbt
. Ended up buying after reading a free chapter on 
@leanpub
. Had such a rich references, that got me hooked. Looking forward to more bedtime reading.
------
Happy birthday, transformer! An awesome summary 
@DrJimFan
!

Also interesting to think about why we needed attention for RNNs (before transformers) in the first place.

Since we can't translate word-by-word, we needed a RNN encoder-decoder setup. But then, it's hard to remember.
------
Today 6 years ago, "Attention is All You Need" went on Arxiv! Happy birthday Transformer! 

Fun facts:
- Transformer did not invent attention, but pushed it to the extreme. The first attention paper was published 3 years prior (2014) and had an unassuming title: "Neural Machine… Show more
------
And the recent "How Far Can Camels Go" paper (https://arxiv.org/abs/2306.04751) is a nice example of how important the dataset for LLM finetuning is.
------
LLMs are usually too large for most contexts, but creating pruned versions of a model usually require retraining.

Here's a new straightforward alternative based on computing element-wise product between the weight magnitude and norm of input activations: https://arxiv.org/abs/2306.11695
------
Huge news! Big congrats to both teams 
@databricks
 & 
@MosaicML
 and everyone involved! 

I love all the recent research and work from 
@MosaicML
 and can’t wait to see what’s next!
------
Big news: we've agreed to acquire @MosaicML, a leading generative AI platform. I couldn’t be more excited to join forces once the deal closes. https://databricks.com/mosaic-news
------
"Accelerating PyTorch Model Training Using Mixed-Precision and Fully Sharded Data Parallelism"
https://magazine.sebastianraschka.com/p/accelerating-pytorch-model-training…

A little write-up of a talk I gave last week! 
------
The "Sweet Lesson" is that using LLMs is simpler than we thought!

Instead of complicated symbolic computations, we download a pretrained open-source LLM and finetune it in supervised fashion via <100 lines of PyTorch code.
------
The "Bitter Lesson" is a statement about computation alone, but is it true without data? Chess is very unique, and Go was solved thanks to data (generated with compute in that case indeed, but that's more an exception than the rule) @RichardSSutton
------
Reading 
@rasbt
's insightful book. Absolutely loving his compelling description of self-supervised pre-training vs transfer-learning.
#MachineLearning A definite must-read! #CurrentReads 

https://leanpub.com/machine-learning-q-and-ai…
------
If you are at #CVPR2023, I'll be giving a short talk on "Scaling PyTorch Model Training With Minimal Code Changes" at 10:30 - 10:45 today (talk area of the Exhibit Hall)

(Will be at the Lightning AI booth afterwards & happy to chat as well )
------
Don’t miss @rasbt ’s CVPR talk on training vision transformers 10x faster using Lightning Fabric. Join us at 10:30 AM at Booth 1718!

#CVPR2023 #AI #ComputerVision
------
Thanks everyone for your interest ! I think someone recorded it — will share once I know more and am back from the conference! In the meantime I also have all the materials here on GitHub:
------
So much discussion about arxiv this morning. 

You can get a lot of positive value out of using arxiv if you use it with care. 

Eg does the original Transformer paper have inconsistencies (see LayerNorm) and is not super detailed? Sure. 

Is it a groundbreaking paper. Yes!
------
I am thankful that arxiv exists, but of course we need to take papers there with a grain of salt. But this is also generally true for peer-reviewed papers as well.

As always, you have to judge it on a case-by-case basis.
------
I often get requests to dispel some of the jargon behind transformers and LLMs!

So here we go, my new article on "Understanding Encoder and Decoder LLMs":
------
Am so glad about all the nice feedback about my Deep Learning course -- happy to hear that it was useful to tens of thousands of people!

Just added a final exam & completion badge for LinkedIn!

So, if you'd like to test your knowledge with 25 questions:
https://lightning.ai/pages/ai-education/deep-learning-fundamentals/certification/…
------
Today we're excited to announce our Deep Learning Fundamentals Certification 

Once you tackle @rasbt's course, learn how to train models, and explore the basics of LLMs, you'll be ready to pass the exam and share the badge on LinkedIn!

Get started! http://lightning.ai/pages/courses/deep-learning-fundamentals/…
------
Have been heads-down porting LoRA (low-rank-adaptation) to finetune Falcon more efficiently and ran some performance benchmarks.

My longer write-up here: https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

Long story short: you can finetune Falcon in 1 h on a 52k dataset using a single GPU with 16 GB RAM.
------
Wanna save  and ship your LLMs to prod faster?

Learn how @rasbt finetunes Falcon on 1 GPU in just 1 hour!  https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

#LLMs #MachineLearning #AI #opensource
------
Btw, it's also a big reduction in memory: 16 gigs instead of 6 x 40 GB.

Due to the reduced parameter counts in the backward pass:

Full finetuning: 7,217,189,760
Adapter: 1,365,330
Adapter v2: 3,839,186
LoRA: 3,506,176

(I used  LoRA rank of 16 to match Adapter v2 above.)
------
Susan & I found MMLU performance jump 6-10 points in the 40s by formatting multiple choice as (A) not A in MMLU (for internal model). All evaluation of LLM's are broken. Evaluating a task requires marginalizing across all prompts that describe the task, not point estimate of one.
------
Never insult a scientist by suggesting they're p-hacking benchmarks.   twitter.com/suchenzang/sta…
------
I really like the LLaMA-Adapter method! 
In a nutshell, it's about finetuning LLMs more efficiently. 

In a nutshell, it's adding a tunable prefix to the key and value tensors in the self-attention layers.

Fun fact: it's not specific to LLaMA. Use it with any LLM!
------
Exciting news: LLaMA-Adapter is now fully unlocked! 6

 As a general-purpose #multimodal foundation model, it integrates various inputs like images, audio, text, video, and 3D point clouds, while providing image, text-based, and detection outputs. It uniquely accepts the… Show more
------
Reading  Machine Learning Q and AI from 
@rasbt
. Ended up buying after reading a free chapter on 
@leanpub
. Had such a rich references, that got me hooked. Looking forward to more bedtime reading.
------
Happy birthday, transformer! An awesome summary 
@DrJimFan
!

Also interesting to think about why we needed attention for RNNs (before transformers) in the first place.

Since we can't translate word-by-word, we needed a RNN encoder-decoder setup. But then, it's hard to remember.
------
Today 6 years ago, "Attention is All You Need" went on Arxiv! Happy birthday Transformer! 

Fun facts:
- Transformer did not invent attention, but pushed it to the extreme. The first attention paper was published 3 years prior (2014) and had an unassuming title: "Neural Machine… Show more
------
"Accelerating PyTorch Model Training Using Mixed-Precision and Fully Sharded Data Parallelism"
https://magazine.sebastianraschka.com/p/accelerating-pytorch-model-training…

A little write-up of a talk I gave last week! 
------
The "Sweet Lesson" is that using LLMs is simpler than we thought!

Instead of complicated symbolic computations, we download a pretrained open-source LLM and finetune it in supervised fashion via <100 lines of PyTorch code.
------
The "Bitter Lesson" is a statement about computation alone, but is it true without data? Chess is very unique, and Go was solved thanks to data (generated with compute in that case indeed, but that's more an exception than the rule) @RichardSSutton
------
Reading 
@rasbt
's insightful book. Absolutely loving his compelling description of self-supervised pre-training vs transfer-learning.
#MachineLearning A definite must-read! #CurrentReads 

https://leanpub.com/machine-learning-q-and-ai…
------
If you are at #CVPR2023, I'll be giving a short talk on "Scaling PyTorch Model Training With Minimal Code Changes" at 10:30 - 10:45 today (talk area of the Exhibit Hall)

(Will be at the Lightning AI booth afterwards & happy to chat as well )
------
Don’t miss @rasbt ’s CVPR talk on training vision transformers 10x faster using Lightning Fabric. Join us at 10:30 AM at Booth 1718!

#CVPR2023 #AI #ComputerVision
------
Thanks everyone for your interest ! I think someone recorded it — will share once I know more and am back from the conference! In the meantime I also have all the materials here on GitHub:
------
So much discussion about arxiv this morning. 

You can get a lot of positive value out of using arxiv if you use it with care. 

Eg does the original Transformer paper have inconsistencies (see LayerNorm) and is not super detailed? Sure. 

Is it a groundbreaking paper. Yes!
------
I am thankful that arxiv exists, but of course we need to take papers there with a grain of salt. But this is also generally true for peer-reviewed papers as well.

As always, you have to judge it on a case-by-case basis.
------
I often get requests to dispel some of the jargon behind transformers and LLMs!

So here we go, my new article on "Understanding Encoder and Decoder LLMs":
------
Am so glad about all the nice feedback about my Deep Learning course -- happy to hear that it was useful to tens of thousands of people!

Just added a final exam & completion badge for LinkedIn!

So, if you'd like to test your knowledge with 25 questions:
https://lightning.ai/pages/ai-education/deep-learning-fundamentals/certification/…
------
Today we're excited to announce our Deep Learning Fundamentals Certification 

Once you tackle @rasbt's course, learn how to train models, and explore the basics of LLMs, you'll be ready to pass the exam and share the badge on LinkedIn!

Get started! http://lightning.ai/pages/courses/deep-learning-fundamentals/…
------
Have been heads-down porting LoRA (low-rank-adaptation) to finetune Falcon more efficiently and ran some performance benchmarks.

My longer write-up here: https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

Long story short: you can finetune Falcon in 1 h on a 52k dataset using a single GPU with 16 GB RAM.
------
Wanna save  and ship your LLMs to prod faster?

Learn how @rasbt finetunes Falcon on 1 GPU in just 1 hour!  https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

#LLMs #MachineLearning #AI #opensource
------
Btw, it's also a big reduction in memory: 16 gigs instead of 6 x 40 GB.

Due to the reduced parameter counts in the backward pass:

Full finetuning: 7,217,189,760
Adapter: 1,365,330
Adapter v2: 3,839,186
LoRA: 3,506,176

(I used  LoRA rank of 16 to match Adapter v2 above.)
------
Susan & I found MMLU performance jump 6-10 points in the 40s by formatting multiple choice as (A) not A in MMLU (for internal model). All evaluation of LLM's are broken. Evaluating a task requires marginalizing across all prompts that describe the task, not point estimate of one.
------
Never insult a scientist by suggesting they're p-hacking benchmarks.   twitter.com/suchenzang/sta…
------
I really like the LLaMA-Adapter method! 
In a nutshell, it's about finetuning LLMs more efficiently. 

In a nutshell, it's adding a tunable prefix to the key and value tensors in the self-attention layers.

Fun fact: it's not specific to LLaMA. Use it with any LLM!
------
Exciting news: LLaMA-Adapter is now fully unlocked! 6

 As a general-purpose #multimodal foundation model, it integrates various inputs like images, audio, text, video, and 3D point clouds, while providing image, text-based, and detection outputs. It uniquely accepts the… Show more
------
Reading  Machine Learning Q and AI from 
@rasbt
. Ended up buying after reading a free chapter on 
@leanpub
. Had such a rich references, that got me hooked. Looking forward to more bedtime reading.
------
Happy birthday, transformer! An awesome summary 
@DrJimFan
!

Also interesting to think about why we needed attention for RNNs (before transformers) in the first place.

Since we can't translate word-by-word, we needed a RNN encoder-decoder setup. But then, it's hard to remember.
------
Today 6 years ago, "Attention is All You Need" went on Arxiv! Happy birthday Transformer! 

Fun facts:
- Transformer did not invent attention, but pushed it to the extreme. The first attention paper was published 3 years prior (2014) and had an unassuming title: "Neural Machine… Show more
------
Thanks everyone for your interest ! I think someone recorded it — will share once I know more and am back from the conference! In the meantime I also have all the materials here on GitHub:
------
So much discussion about arxiv this morning. 

You can get a lot of positive value out of using arxiv if you use it with care. 

Eg does the original Transformer paper have inconsistencies (see LayerNorm) and is not super detailed? Sure. 

Is it a groundbreaking paper. Yes!
------
I am thankful that arxiv exists, but of course we need to take papers there with a grain of salt. But this is also generally true for peer-reviewed papers as well.

As always, you have to judge it on a case-by-case basis.
------
I often get requests to dispel some of the jargon behind transformers and LLMs!

So here we go, my new article on "Understanding Encoder and Decoder LLMs":
------
Am so glad about all the nice feedback about my Deep Learning course -- happy to hear that it was useful to tens of thousands of people!

Just added a final exam & completion badge for LinkedIn!

So, if you'd like to test your knowledge with 25 questions:
https://lightning.ai/pages/ai-education/deep-learning-fundamentals/certification/…
------
Today we're excited to announce our Deep Learning Fundamentals Certification 

Once you tackle @rasbt's course, learn how to train models, and explore the basics of LLMs, you'll be ready to pass the exam and share the badge on LinkedIn!

Get started! http://lightning.ai/pages/courses/deep-learning-fundamentals/…
------
Have been heads-down porting LoRA (low-rank-adaptation) to finetune Falcon more efficiently and ran some performance benchmarks.

My longer write-up here: https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

Long story short: you can finetune Falcon in 1 h on a 52k dataset using a single GPU with 16 GB RAM.
------
Wanna save  and ship your LLMs to prod faster?

Learn how @rasbt finetunes Falcon on 1 GPU in just 1 hour!  https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

#LLMs #MachineLearning #AI #opensource
------
Btw, it's also a big reduction in memory: 16 gigs instead of 6 x 40 GB.

Due to the reduced parameter counts in the backward pass:

Full finetuning: 7,217,189,760
Adapter: 1,365,330
Adapter v2: 3,839,186
LoRA: 3,506,176

(I used  LoRA rank of 16 to match Adapter v2 above.)
------
Susan & I found MMLU performance jump 6-10 points in the 40s by formatting multiple choice as (A) not A in MMLU (for internal model). All evaluation of LLM's are broken. Evaluating a task requires marginalizing across all prompts that describe the task, not point estimate of one.
------
Never insult a scientist by suggesting they're p-hacking benchmarks.   twitter.com/suchenzang/sta…
------
I really like the LLaMA-Adapter method! 
In a nutshell, it's about finetuning LLMs more efficiently. 

In a nutshell, it's adding a tunable prefix to the key and value tensors in the self-attention layers.

Fun fact: it's not specific to LLaMA. Use it with any LLM!
------
Exciting news: LLaMA-Adapter is now fully unlocked! 6

 As a general-purpose #multimodal foundation model, it integrates various inputs like images, audio, text, video, and 3D point clouds, while providing image, text-based, and detection outputs. It uniquely accepts the… Show more
------
Reading  Machine Learning Q and AI from 
@rasbt
. Ended up buying after reading a free chapter on 
@leanpub
. Had such a rich references, that got me hooked. Looking forward to more bedtime reading.
------
Happy birthday, transformer! An awesome summary 
@DrJimFan
!

Also interesting to think about why we needed attention for RNNs (before transformers) in the first place.

Since we can't translate word-by-word, we needed a RNN encoder-decoder setup. But then, it's hard to remember.
------
Today 6 years ago, "Attention is All You Need" went on Arxiv! Happy birthday Transformer! 

Fun facts:
- Transformer did not invent attention, but pushed it to the extreme. The first attention paper was published 3 years prior (2014) and had an unassuming title: "Neural Machine… Show more
------
Thanks everyone for your interest ! I think someone recorded it — will share once I know more and am back from the conference! In the meantime I also have all the materials here on GitHub:
------
So much discussion about arxiv this morning. 

You can get a lot of positive value out of using arxiv if you use it with care. 

Eg does the original Transformer paper have inconsistencies (see LayerNorm) and is not super detailed? Sure. 

Is it a groundbreaking paper. Yes!
------
I am thankful that arxiv exists, but of course we need to take papers there with a grain of salt. But this is also generally true for peer-reviewed papers as well.

As always, you have to judge it on a case-by-case basis.
------
I often get requests to dispel some of the jargon behind transformers and LLMs!

So here we go, my new article on "Understanding Encoder and Decoder LLMs":
------
Am so glad about all the nice feedback about my Deep Learning course -- happy to hear that it was useful to tens of thousands of people!

Just added a final exam & completion badge for LinkedIn!

So, if you'd like to test your knowledge with 25 questions:
https://lightning.ai/pages/ai-education/deep-learning-fundamentals/certification/…
------
Today we're excited to announce our Deep Learning Fundamentals Certification 

Once you tackle @rasbt's course, learn how to train models, and explore the basics of LLMs, you'll be ready to pass the exam and share the badge on LinkedIn!

Get started! http://lightning.ai/pages/courses/deep-learning-fundamentals/…
------
Have been heads-down porting LoRA (low-rank-adaptation) to finetune Falcon more efficiently and ran some performance benchmarks.

My longer write-up here: https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

Long story short: you can finetune Falcon in 1 h on a 52k dataset using a single GPU with 16 GB RAM.
------
Wanna save  and ship your LLMs to prod faster?

Learn how @rasbt finetunes Falcon on 1 GPU in just 1 hour!  https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

#LLMs #MachineLearning #AI #opensource
------
Btw, it's also a big reduction in memory: 16 gigs instead of 6 x 40 GB.

Due to the reduced parameter counts in the backward pass:

Full finetuning: 7,217,189,760
Adapter: 1,365,330
Adapter v2: 3,839,186
LoRA: 3,506,176

(I used  LoRA rank of 16 to match Adapter v2 above.)
------
Susan & I found MMLU performance jump 6-10 points in the 40s by formatting multiple choice as (A) not A in MMLU (for internal model). All evaluation of LLM's are broken. Evaluating a task requires marginalizing across all prompts that describe the task, not point estimate of one.
------
Never insult a scientist by suggesting they're p-hacking benchmarks.   twitter.com/suchenzang/sta…
------
I really like the LLaMA-Adapter method! 
In a nutshell, it's about finetuning LLMs more efficiently. 

In a nutshell, it's adding a tunable prefix to the key and value tensors in the self-attention layers.

Fun fact: it's not specific to LLaMA. Use it with any LLM!
------
Exciting news: LLaMA-Adapter is now fully unlocked! 6

 As a general-purpose #multimodal foundation model, it integrates various inputs like images, audio, text, video, and 3D point clouds, while providing image, text-based, and detection outputs. It uniquely accepts the… Show more
------
Reading  Machine Learning Q and AI from 
@rasbt
. Ended up buying after reading a free chapter on 
@leanpub
. Had such a rich references, that got me hooked. Looking forward to more bedtime reading.
------
Happy birthday, transformer! An awesome summary 
@DrJimFan
!

Also interesting to think about why we needed attention for RNNs (before transformers) in the first place.

Since we can't translate word-by-word, we needed a RNN encoder-decoder setup. But then, it's hard to remember.
------
Today 6 years ago, "Attention is All You Need" went on Arxiv! Happy birthday Transformer! 

Fun facts:
- Transformer did not invent attention, but pushed it to the extreme. The first attention paper was published 3 years prior (2014) and had an unassuming title: "Neural Machine… Show more
------
Thanks everyone for your interest ! I think someone recorded it — will share once I know more and am back from the conference! In the meantime I also have all the materials here on GitHub:
------
So much discussion about arxiv this morning. 

You can get a lot of positive value out of using arxiv if you use it with care. 

Eg does the original Transformer paper have inconsistencies (see LayerNorm) and is not super detailed? Sure. 

Is it a groundbreaking paper. Yes!
------
I am thankful that arxiv exists, but of course we need to take papers there with a grain of salt. But this is also generally true for peer-reviewed papers as well.

As always, you have to judge it on a case-by-case basis.
------
I often get requests to dispel some of the jargon behind transformers and LLMs!

So here we go, my new article on "Understanding Encoder and Decoder LLMs":
------
Am so glad about all the nice feedback about my Deep Learning course -- happy to hear that it was useful to tens of thousands of people!

Just added a final exam & completion badge for LinkedIn!

So, if you'd like to test your knowledge with 25 questions:
https://lightning.ai/pages/ai-education/deep-learning-fundamentals/certification/…
------
Today we're excited to announce our Deep Learning Fundamentals Certification 

Once you tackle @rasbt's course, learn how to train models, and explore the basics of LLMs, you'll be ready to pass the exam and share the badge on LinkedIn!

Get started! http://lightning.ai/pages/courses/deep-learning-fundamentals/…
------
Have been heads-down porting LoRA (low-rank-adaptation) to finetune Falcon more efficiently and ran some performance benchmarks.

My longer write-up here: https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

Long story short: you can finetune Falcon in 1 h on a 52k dataset using a single GPU with 16 GB RAM.
------
Wanna save  and ship your LLMs to prod faster?

Learn how @rasbt finetunes Falcon on 1 GPU in just 1 hour!  https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

#LLMs #MachineLearning #AI #opensource
------
Btw, it's also a big reduction in memory: 16 gigs instead of 6 x 40 GB.

Due to the reduced parameter counts in the backward pass:

Full finetuning: 7,217,189,760
Adapter: 1,365,330
Adapter v2: 3,839,186
LoRA: 3,506,176

(I used  LoRA rank of 16 to match Adapter v2 above.)
------
Susan & I found MMLU performance jump 6-10 points in the 40s by formatting multiple choice as (A) not A in MMLU (for internal model). All evaluation of LLM's are broken. Evaluating a task requires marginalizing across all prompts that describe the task, not point estimate of one.
------
Never insult a scientist by suggesting they're p-hacking benchmarks.   twitter.com/suchenzang/sta…
------
I really like the LLaMA-Adapter method! 
In a nutshell, it's about finetuning LLMs more efficiently. 

In a nutshell, it's adding a tunable prefix to the key and value tensors in the self-attention layers.

Fun fact: it's not specific to LLaMA. Use it with any LLM!
------
Exciting news: LLaMA-Adapter is now fully unlocked! 6

 As a general-purpose #multimodal foundation model, it integrates various inputs like images, audio, text, video, and 3D point clouds, while providing image, text-based, and detection outputs. It uniquely accepts the… Show more
------
Reading  Machine Learning Q and AI from 
@rasbt
. Ended up buying after reading a free chapter on 
@leanpub
. Had such a rich references, that got me hooked. Looking forward to more bedtime reading.
------
Happy birthday, transformer! An awesome summary 
@DrJimFan
!

Also interesting to think about why we needed attention for RNNs (before transformers) in the first place.

Since we can't translate word-by-word, we needed a RNN encoder-decoder setup. But then, it's hard to remember.
------
Today 6 years ago, "Attention is All You Need" went on Arxiv! Happy birthday Transformer! 

Fun facts:
- Transformer did not invent attention, but pushed it to the extreme. The first attention paper was published 3 years prior (2014) and had an unassuming title: "Neural Machine… Show more
------
Thanks everyone for your interest ! I think someone recorded it — will share once I know more and am back from the conference! In the meantime I also have all the materials here on GitHub:
------
So much discussion about arxiv this morning. 

You can get a lot of positive value out of using arxiv if you use it with care. 

Eg does the original Transformer paper have inconsistencies (see LayerNorm) and is not super detailed? Sure. 

Is it a groundbreaking paper. Yes!
------
I am thankful that arxiv exists, but of course we need to take papers there with a grain of salt. But this is also generally true for peer-reviewed papers as well.

As always, you have to judge it on a case-by-case basis.
------
I often get requests to dispel some of the jargon behind transformers and LLMs!

So here we go, my new article on "Understanding Encoder and Decoder LLMs":
------
Am so glad about all the nice feedback about my Deep Learning course -- happy to hear that it was useful to tens of thousands of people!

Just added a final exam & completion badge for LinkedIn!

So, if you'd like to test your knowledge with 25 questions:
https://lightning.ai/pages/ai-education/deep-learning-fundamentals/certification/…
------
Today we're excited to announce our Deep Learning Fundamentals Certification 

Once you tackle @rasbt's course, learn how to train models, and explore the basics of LLMs, you'll be ready to pass the exam and share the badge on LinkedIn!

Get started! http://lightning.ai/pages/courses/deep-learning-fundamentals/…
------
Have been heads-down porting LoRA (low-rank-adaptation) to finetune Falcon more efficiently and ran some performance benchmarks.

My longer write-up here: https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

Long story short: you can finetune Falcon in 1 h on a 52k dataset using a single GPU with 16 GB RAM.
------
Wanna save  and ship your LLMs to prod faster?

Learn how @rasbt finetunes Falcon on 1 GPU in just 1 hour!  https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

#LLMs #MachineLearning #AI #opensource
------
Btw, it's also a big reduction in memory: 16 gigs instead of 6 x 40 GB.

Due to the reduced parameter counts in the backward pass:

Full finetuning: 7,217,189,760
Adapter: 1,365,330
Adapter v2: 3,839,186
LoRA: 3,506,176

(I used  LoRA rank of 16 to match Adapter v2 above.)
------
Susan & I found MMLU performance jump 6-10 points in the 40s by formatting multiple choice as (A) not A in MMLU (for internal model). All evaluation of LLM's are broken. Evaluating a task requires marginalizing across all prompts that describe the task, not point estimate of one.
------
Never insult a scientist by suggesting they're p-hacking benchmarks.   twitter.com/suchenzang/sta…
------
I really like the LLaMA-Adapter method! 
In a nutshell, it's about finetuning LLMs more efficiently. 

In a nutshell, it's adding a tunable prefix to the key and value tensors in the self-attention layers.

Fun fact: it's not specific to LLaMA. Use it with any LLM!
------
Exciting news: LLaMA-Adapter is now fully unlocked! 6

 As a general-purpose #multimodal foundation model, it integrates various inputs like images, audio, text, video, and 3D point clouds, while providing image, text-based, and detection outputs. It uniquely accepts the… Show more
------
Reading  Machine Learning Q and AI from 
@rasbt
. Ended up buying after reading a free chapter on 
@leanpub
. Had such a rich references, that got me hooked. Looking forward to more bedtime reading.
------
Happy birthday, transformer! An awesome summary 
@DrJimFan
!

Also interesting to think about why we needed attention for RNNs (before transformers) in the first place.

Since we can't translate word-by-word, we needed a RNN encoder-decoder setup. But then, it's hard to remember.
------
Today 6 years ago, "Attention is All You Need" went on Arxiv! Happy birthday Transformer! 

Fun facts:
- Transformer did not invent attention, but pushed it to the extreme. The first attention paper was published 3 years prior (2014) and had an unassuming title: "Neural Machine… Show more
------
Thanks everyone for your interest ! I think someone recorded it — will share once I know more and am back from the conference! In the meantime I also have all the materials here on GitHub:
------
So much discussion about arxiv this morning. 

You can get a lot of positive value out of using arxiv if you use it with care. 

Eg does the original Transformer paper have inconsistencies (see LayerNorm) and is not super detailed? Sure. 

Is it a groundbreaking paper. Yes!
------
I am thankful that arxiv exists, but of course we need to take papers there with a grain of salt. But this is also generally true for peer-reviewed papers as well.

As always, you have to judge it on a case-by-case basis.
------
I often get requests to dispel some of the jargon behind transformers and LLMs!

So here we go, my new article on "Understanding Encoder and Decoder LLMs":
------
Am so glad about all the nice feedback about my Deep Learning course -- happy to hear that it was useful to tens of thousands of people!

Just added a final exam & completion badge for LinkedIn!

So, if you'd like to test your knowledge with 25 questions:
https://lightning.ai/pages/ai-education/deep-learning-fundamentals/certification/…
------
Today we're excited to announce our Deep Learning Fundamentals Certification 

Once you tackle @rasbt's course, learn how to train models, and explore the basics of LLMs, you'll be ready to pass the exam and share the badge on LinkedIn!

Get started! http://lightning.ai/pages/courses/deep-learning-fundamentals/…
------
Have been heads-down porting LoRA (low-rank-adaptation) to finetune Falcon more efficiently and ran some performance benchmarks.

My longer write-up here: https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

Long story short: you can finetune Falcon in 1 h on a 52k dataset using a single GPU with 16 GB RAM.
------
Wanna save  and ship your LLMs to prod faster?

Learn how @rasbt finetunes Falcon on 1 GPU in just 1 hour!  https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

#LLMs #MachineLearning #AI #opensource
------
Btw, it's also a big reduction in memory: 16 gigs instead of 6 x 40 GB.

Due to the reduced parameter counts in the backward pass:

Full finetuning: 7,217,189,760
Adapter: 1,365,330
Adapter v2: 3,839,186
LoRA: 3,506,176

(I used  LoRA rank of 16 to match Adapter v2 above.)
------
Susan & I found MMLU performance jump 6-10 points in the 40s by formatting multiple choice as (A) not A in MMLU (for internal model). All evaluation of LLM's are broken. Evaluating a task requires marginalizing across all prompts that describe the task, not point estimate of one.
------
Never insult a scientist by suggesting they're p-hacking benchmarks.   twitter.com/suchenzang/sta…
------
I really like the LLaMA-Adapter method! 
In a nutshell, it's about finetuning LLMs more efficiently. 

In a nutshell, it's adding a tunable prefix to the key and value tensors in the self-attention layers.

Fun fact: it's not specific to LLaMA. Use it with any LLM!
------
Exciting news: LLaMA-Adapter is now fully unlocked! 6

 As a general-purpose #multimodal foundation model, it integrates various inputs like images, audio, text, video, and 3D point clouds, while providing image, text-based, and detection outputs. It uniquely accepts the… Show more
------
Reading  Machine Learning Q and AI from 
@rasbt
. Ended up buying after reading a free chapter on 
@leanpub
. Had such a rich references, that got me hooked. Looking forward to more bedtime reading.
------
Happy birthday, transformer! An awesome summary 
@DrJimFan
!

Also interesting to think about why we needed attention for RNNs (before transformers) in the first place.

Since we can't translate word-by-word, we needed a RNN encoder-decoder setup. But then, it's hard to remember.
------
Today 6 years ago, "Attention is All You Need" went on Arxiv! Happy birthday Transformer! 

Fun facts:
- Transformer did not invent attention, but pushed it to the extreme. The first attention paper was published 3 years prior (2014) and had an unassuming title: "Neural Machine… Show more
------
Thanks everyone for your interest ! I think someone recorded it — will share once I know more and am back from the conference! In the meantime I also have all the materials here on GitHub:
------
So much discussion about arxiv this morning. 

You can get a lot of positive value out of using arxiv if you use it with care. 

Eg does the original Transformer paper have inconsistencies (see LayerNorm) and is not super detailed? Sure. 

Is it a groundbreaking paper. Yes!
------
I am thankful that arxiv exists, but of course we need to take papers there with a grain of salt. But this is also generally true for peer-reviewed papers as well.

As always, you have to judge it on a case-by-case basis.
------
I often get requests to dispel some of the jargon behind transformers and LLMs!

So here we go, my new article on "Understanding Encoder and Decoder LLMs":
------
Am so glad about all the nice feedback about my Deep Learning course -- happy to hear that it was useful to tens of thousands of people!

Just added a final exam & completion badge for LinkedIn!

So, if you'd like to test your knowledge with 25 questions:
https://lightning.ai/pages/ai-education/deep-learning-fundamentals/certification/…
------
Today we're excited to announce our Deep Learning Fundamentals Certification 

Once you tackle @rasbt's course, learn how to train models, and explore the basics of LLMs, you'll be ready to pass the exam and share the badge on LinkedIn!

Get started! http://lightning.ai/pages/courses/deep-learning-fundamentals/…
------
Have been heads-down porting LoRA (low-rank-adaptation) to finetune Falcon more efficiently and ran some performance benchmarks.

My longer write-up here: https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

Long story short: you can finetune Falcon in 1 h on a 52k dataset using a single GPU with 16 GB RAM.
------
Wanna save  and ship your LLMs to prod faster?

Learn how @rasbt finetunes Falcon on 1 GPU in just 1 hour!  https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

#LLMs #MachineLearning #AI #opensource
------
Btw, it's also a big reduction in memory: 16 gigs instead of 6 x 40 GB.

Due to the reduced parameter counts in the backward pass:

Full finetuning: 7,217,189,760
Adapter: 1,365,330
Adapter v2: 3,839,186
LoRA: 3,506,176

(I used  LoRA rank of 16 to match Adapter v2 above.)
------
Susan & I found MMLU performance jump 6-10 points in the 40s by formatting multiple choice as (A) not A in MMLU (for internal model). All evaluation of LLM's are broken. Evaluating a task requires marginalizing across all prompts that describe the task, not point estimate of one.
------
Never insult a scientist by suggesting they're p-hacking benchmarks.   twitter.com/suchenzang/sta…
------
I really like the LLaMA-Adapter method! 
In a nutshell, it's about finetuning LLMs more efficiently. 

In a nutshell, it's adding a tunable prefix to the key and value tensors in the self-attention layers.

Fun fact: it's not specific to LLaMA. Use it with any LLM!
------
Exciting news: LLaMA-Adapter is now fully unlocked! 6

 As a general-purpose #multimodal foundation model, it integrates various inputs like images, audio, text, video, and 3D point clouds, while providing image, text-based, and detection outputs. It uniquely accepts the… Show more
------
Reading  Machine Learning Q and AI from 
@rasbt
. Ended up buying after reading a free chapter on 
@leanpub
. Had such a rich references, that got me hooked. Looking forward to more bedtime reading.
------
Happy birthday, transformer! An awesome summary 
@DrJimFan
!

Also interesting to think about why we needed attention for RNNs (before transformers) in the first place.

Since we can't translate word-by-word, we needed a RNN encoder-decoder setup. But then, it's hard to remember.
------
Today 6 years ago, "Attention is All You Need" went on Arxiv! Happy birthday Transformer! 

Fun facts:
- Transformer did not invent attention, but pushed it to the extreme. The first attention paper was published 3 years prior (2014) and had an unassuming title: "Neural Machine… Show more
------
Thanks everyone for your interest ! I think someone recorded it — will share once I know more and am back from the conference! In the meantime I also have all the materials here on GitHub:
------
So much discussion about arxiv this morning. 

You can get a lot of positive value out of using arxiv if you use it with care. 

Eg does the original Transformer paper have inconsistencies (see LayerNorm) and is not super detailed? Sure. 

Is it a groundbreaking paper. Yes!
------
I am thankful that arxiv exists, but of course we need to take papers there with a grain of salt. But this is also generally true for peer-reviewed papers as well.

As always, you have to judge it on a case-by-case basis.
------
I often get requests to dispel some of the jargon behind transformers and LLMs!

So here we go, my new article on "Understanding Encoder and Decoder LLMs":
------
Am so glad about all the nice feedback about my Deep Learning course -- happy to hear that it was useful to tens of thousands of people!

Just added a final exam & completion badge for LinkedIn!

So, if you'd like to test your knowledge with 25 questions:
https://lightning.ai/pages/ai-education/deep-learning-fundamentals/certification/…
------
Today we're excited to announce our Deep Learning Fundamentals Certification 

Once you tackle @rasbt's course, learn how to train models, and explore the basics of LLMs, you'll be ready to pass the exam and share the badge on LinkedIn!

Get started! http://lightning.ai/pages/courses/deep-learning-fundamentals/…
------
Have been heads-down porting LoRA (low-rank-adaptation) to finetune Falcon more efficiently and ran some performance benchmarks.

My longer write-up here: https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

Long story short: you can finetune Falcon in 1 h on a 52k dataset using a single GPU with 16 GB RAM.
------
Wanna save  and ship your LLMs to prod faster?

Learn how @rasbt finetunes Falcon on 1 GPU in just 1 hour!  https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

#LLMs #MachineLearning #AI #opensource
------
Btw, it's also a big reduction in memory: 16 gigs instead of 6 x 40 GB.

Due to the reduced parameter counts in the backward pass:

Full finetuning: 7,217,189,760
Adapter: 1,365,330
Adapter v2: 3,839,186
LoRA: 3,506,176

(I used  LoRA rank of 16 to match Adapter v2 above.)
------
Susan & I found MMLU performance jump 6-10 points in the 40s by formatting multiple choice as (A) not A in MMLU (for internal model). All evaluation of LLM's are broken. Evaluating a task requires marginalizing across all prompts that describe the task, not point estimate of one.
------
Never insult a scientist by suggesting they're p-hacking benchmarks.   twitter.com/suchenzang/sta…
------
I really like the LLaMA-Adapter method! 
In a nutshell, it's about finetuning LLMs more efficiently. 

In a nutshell, it's adding a tunable prefix to the key and value tensors in the self-attention layers.

Fun fact: it's not specific to LLaMA. Use it with any LLM!
------
Exciting news: LLaMA-Adapter is now fully unlocked! 6

 As a general-purpose #multimodal foundation model, it integrates various inputs like images, audio, text, video, and 3D point clouds, while providing image, text-based, and detection outputs. It uniquely accepts the… Show more
------
Reading  Machine Learning Q and AI from 
@rasbt
. Ended up buying after reading a free chapter on 
@leanpub
. Had such a rich references, that got me hooked. Looking forward to more bedtime reading.
------
Happy birthday, transformer! An awesome summary 
@DrJimFan
!

Also interesting to think about why we needed attention for RNNs (before transformers) in the first place.

Since we can't translate word-by-word, we needed a RNN encoder-decoder setup. But then, it's hard to remember.
------
Today 6 years ago, "Attention is All You Need" went on Arxiv! Happy birthday Transformer! 

Fun facts:
- Transformer did not invent attention, but pushed it to the extreme. The first attention paper was published 3 years prior (2014) and had an unassuming title: "Neural Machine… Show more
------
Thanks everyone for your interest ! I think someone recorded it — will share once I know more and am back from the conference! In the meantime I also have all the materials here on GitHub:
------
So much discussion about arxiv this morning. 

You can get a lot of positive value out of using arxiv if you use it with care. 

Eg does the original Transformer paper have inconsistencies (see LayerNorm) and is not super detailed? Sure. 

Is it a groundbreaking paper. Yes!
------
I am thankful that arxiv exists, but of course we need to take papers there with a grain of salt. But this is also generally true for peer-reviewed papers as well.

As always, you have to judge it on a case-by-case basis.
------
I often get requests to dispel some of the jargon behind transformers and LLMs!

So here we go, my new article on "Understanding Encoder and Decoder LLMs":
------
Am so glad about all the nice feedback about my Deep Learning course -- happy to hear that it was useful to tens of thousands of people!

Just added a final exam & completion badge for LinkedIn!

So, if you'd like to test your knowledge with 25 questions:
https://lightning.ai/pages/ai-education/deep-learning-fundamentals/certification/…
------
Today we're excited to announce our Deep Learning Fundamentals Certification 

Once you tackle @rasbt's course, learn how to train models, and explore the basics of LLMs, you'll be ready to pass the exam and share the badge on LinkedIn!

Get started! http://lightning.ai/pages/courses/deep-learning-fundamentals/…
------
Have been heads-down porting LoRA (low-rank-adaptation) to finetune Falcon more efficiently and ran some performance benchmarks.

My longer write-up here: https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

Long story short: you can finetune Falcon in 1 h on a 52k dataset using a single GPU with 16 GB RAM.
------
Wanna save  and ship your LLMs to prod faster?

Learn how @rasbt finetunes Falcon on 1 GPU in just 1 hour!  https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

#LLMs #MachineLearning #AI #opensource
------
Btw, it's also a big reduction in memory: 16 gigs instead of 6 x 40 GB.

Due to the reduced parameter counts in the backward pass:

Full finetuning: 7,217,189,760
Adapter: 1,365,330
Adapter v2: 3,839,186
LoRA: 3,506,176

(I used  LoRA rank of 16 to match Adapter v2 above.)
------
Susan & I found MMLU performance jump 6-10 points in the 40s by formatting multiple choice as (A) not A in MMLU (for internal model). All evaluation of LLM's are broken. Evaluating a task requires marginalizing across all prompts that describe the task, not point estimate of one.
------
Never insult a scientist by suggesting they're p-hacking benchmarks.   twitter.com/suchenzang/sta…
------
I really like the LLaMA-Adapter method! 
In a nutshell, it's about finetuning LLMs more efficiently. 

In a nutshell, it's adding a tunable prefix to the key and value tensors in the self-attention layers.

Fun fact: it's not specific to LLaMA. Use it with any LLM!
------
Exciting news: LLaMA-Adapter is now fully unlocked! 6

 As a general-purpose #multimodal foundation model, it integrates various inputs like images, audio, text, video, and 3D point clouds, while providing image, text-based, and detection outputs. It uniquely accepts the… Show more
------
Reading  Machine Learning Q and AI from 
@rasbt
. Ended up buying after reading a free chapter on 
@leanpub
. Had such a rich references, that got me hooked. Looking forward to more bedtime reading.
------
Happy birthday, transformer! An awesome summary 
@DrJimFan
!

Also interesting to think about why we needed attention for RNNs (before transformers) in the first place.

Since we can't translate word-by-word, we needed a RNN encoder-decoder setup. But then, it's hard to remember.
------
Today 6 years ago, "Attention is All You Need" went on Arxiv! Happy birthday Transformer! 

Fun facts:
- Transformer did not invent attention, but pushed it to the extreme. The first attention paper was published 3 years prior (2014) and had an unassuming title: "Neural Machine… Show more
------
Thanks everyone for your interest ! I think someone recorded it — will share once I know more and am back from the conference! In the meantime I also have all the materials here on GitHub:
------
So much discussion about arxiv this morning. 

You can get a lot of positive value out of using arxiv if you use it with care. 

Eg does the original Transformer paper have inconsistencies (see LayerNorm) and is not super detailed? Sure. 

Is it a groundbreaking paper. Yes!
------
I am thankful that arxiv exists, but of course we need to take papers there with a grain of salt. But this is also generally true for peer-reviewed papers as well.

As always, you have to judge it on a case-by-case basis.
------
I often get requests to dispel some of the jargon behind transformers and LLMs!

So here we go, my new article on "Understanding Encoder and Decoder LLMs":
------
Am so glad about all the nice feedback about my Deep Learning course -- happy to hear that it was useful to tens of thousands of people!

Just added a final exam & completion badge for LinkedIn!

So, if you'd like to test your knowledge with 25 questions:
https://lightning.ai/pages/ai-education/deep-learning-fundamentals/certification/…
------
Today we're excited to announce our Deep Learning Fundamentals Certification 

Once you tackle @rasbt's course, learn how to train models, and explore the basics of LLMs, you'll be ready to pass the exam and share the badge on LinkedIn!

Get started! http://lightning.ai/pages/courses/deep-learning-fundamentals/…
------
Have been heads-down porting LoRA (low-rank-adaptation) to finetune Falcon more efficiently and ran some performance benchmarks.

My longer write-up here: https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

Long story short: you can finetune Falcon in 1 h on a 52k dataset using a single GPU with 16 GB RAM.
------
Wanna save  and ship your LLMs to prod faster?

Learn how @rasbt finetunes Falcon on 1 GPU in just 1 hour!  https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

#LLMs #MachineLearning #AI #opensource
------
Btw, it's also a big reduction in memory: 16 gigs instead of 6 x 40 GB.

Due to the reduced parameter counts in the backward pass:

Full finetuning: 7,217,189,760
Adapter: 1,365,330
Adapter v2: 3,839,186
LoRA: 3,506,176

(I used  LoRA rank of 16 to match Adapter v2 above.)
------
Susan & I found MMLU performance jump 6-10 points in the 40s by formatting multiple choice as (A) not A in MMLU (for internal model). All evaluation of LLM's are broken. Evaluating a task requires marginalizing across all prompts that describe the task, not point estimate of one.
------
Never insult a scientist by suggesting they're p-hacking benchmarks.   twitter.com/suchenzang/sta…
------
I really like the LLaMA-Adapter method! 
In a nutshell, it's about finetuning LLMs more efficiently. 

In a nutshell, it's adding a tunable prefix to the key and value tensors in the self-attention layers.

Fun fact: it's not specific to LLaMA. Use it with any LLM!
------
Exciting news: LLaMA-Adapter is now fully unlocked! 6

 As a general-purpose #multimodal foundation model, it integrates various inputs like images, audio, text, video, and 3D point clouds, while providing image, text-based, and detection outputs. It uniquely accepts the… Show more
------
Reading  Machine Learning Q and AI from 
@rasbt
. Ended up buying after reading a free chapter on 
@leanpub
. Had such a rich references, that got me hooked. Looking forward to more bedtime reading.
------
Happy birthday, transformer! An awesome summary 
@DrJimFan
!

Also interesting to think about why we needed attention for RNNs (before transformers) in the first place.

Since we can't translate word-by-word, we needed a RNN encoder-decoder setup. But then, it's hard to remember.
------
Today 6 years ago, "Attention is All You Need" went on Arxiv! Happy birthday Transformer! 

Fun facts:
- Transformer did not invent attention, but pushed it to the extreme. The first attention paper was published 3 years prior (2014) and had an unassuming title: "Neural Machine… Show more
------
Thanks everyone for your interest ! I think someone recorded it — will share once I know more and am back from the conference! In the meantime I also have all the materials here on GitHub:
------
So much discussion about arxiv this morning. 

You can get a lot of positive value out of using arxiv if you use it with care. 

Eg does the original Transformer paper have inconsistencies (see LayerNorm) and is not super detailed? Sure. 

Is it a groundbreaking paper. Yes!
------
I am thankful that arxiv exists, but of course we need to take papers there with a grain of salt. But this is also generally true for peer-reviewed papers as well.

As always, you have to judge it on a case-by-case basis.
------
I often get requests to dispel some of the jargon behind transformers and LLMs!

So here we go, my new article on "Understanding Encoder and Decoder LLMs":
------
Am so glad about all the nice feedback about my Deep Learning course -- happy to hear that it was useful to tens of thousands of people!

Just added a final exam & completion badge for LinkedIn!

So, if you'd like to test your knowledge with 25 questions:
https://lightning.ai/pages/ai-education/deep-learning-fundamentals/certification/…
------
Today we're excited to announce our Deep Learning Fundamentals Certification 

Once you tackle @rasbt's course, learn how to train models, and explore the basics of LLMs, you'll be ready to pass the exam and share the badge on LinkedIn!

Get started! http://lightning.ai/pages/courses/deep-learning-fundamentals/…
------
Have been heads-down porting LoRA (low-rank-adaptation) to finetune Falcon more efficiently and ran some performance benchmarks.

My longer write-up here: https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

Long story short: you can finetune Falcon in 1 h on a 52k dataset using a single GPU with 16 GB RAM.
------
Wanna save  and ship your LLMs to prod faster?

Learn how @rasbt finetunes Falcon on 1 GPU in just 1 hour!  https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

#LLMs #MachineLearning #AI #opensource
------
Btw, it's also a big reduction in memory: 16 gigs instead of 6 x 40 GB.

Due to the reduced parameter counts in the backward pass:

Full finetuning: 7,217,189,760
Adapter: 1,365,330
Adapter v2: 3,839,186
LoRA: 3,506,176

(I used  LoRA rank of 16 to match Adapter v2 above.)
------
Susan & I found MMLU performance jump 6-10 points in the 40s by formatting multiple choice as (A) not A in MMLU (for internal model). All evaluation of LLM's are broken. Evaluating a task requires marginalizing across all prompts that describe the task, not point estimate of one.
------
Never insult a scientist by suggesting they're p-hacking benchmarks.   twitter.com/suchenzang/sta…
------
I really like the LLaMA-Adapter method! 
In a nutshell, it's about finetuning LLMs more efficiently. 

In a nutshell, it's adding a tunable prefix to the key and value tensors in the self-attention layers.

Fun fact: it's not specific to LLaMA. Use it with any LLM!
------
Exciting news: LLaMA-Adapter is now fully unlocked! 6

 As a general-purpose #multimodal foundation model, it integrates various inputs like images, audio, text, video, and 3D point clouds, while providing image, text-based, and detection outputs. It uniquely accepts the… Show more
------
Reading  Machine Learning Q and AI from 
@rasbt
. Ended up buying after reading a free chapter on 
@leanpub
. Had such a rich references, that got me hooked. Looking forward to more bedtime reading.
------
Happy birthday, transformer! An awesome summary 
@DrJimFan
!

Also interesting to think about why we needed attention for RNNs (before transformers) in the first place.

Since we can't translate word-by-word, we needed a RNN encoder-decoder setup. But then, it's hard to remember.
------
Today 6 years ago, "Attention is All You Need" went on Arxiv! Happy birthday Transformer! 

Fun facts:
- Transformer did not invent attention, but pushed it to the extreme. The first attention paper was published 3 years prior (2014) and had an unassuming title: "Neural Machine… Show more
------
Thanks everyone for your interest ! I think someone recorded it — will share once I know more and am back from the conference! In the meantime I also have all the materials here on GitHub:
------
So much discussion about arxiv this morning. 

You can get a lot of positive value out of using arxiv if you use it with care. 

Eg does the original Transformer paper have inconsistencies (see LayerNorm) and is not super detailed? Sure. 

Is it a groundbreaking paper. Yes!
------
I am thankful that arxiv exists, but of course we need to take papers there with a grain of salt. But this is also generally true for peer-reviewed papers as well.

As always, you have to judge it on a case-by-case basis.
------
I often get requests to dispel some of the jargon behind transformers and LLMs!

So here we go, my new article on "Understanding Encoder and Decoder LLMs":
------
Am so glad about all the nice feedback about my Deep Learning course -- happy to hear that it was useful to tens of thousands of people!

Just added a final exam & completion badge for LinkedIn!

So, if you'd like to test your knowledge with 25 questions:
https://lightning.ai/pages/ai-education/deep-learning-fundamentals/certification/…
------
Today we're excited to announce our Deep Learning Fundamentals Certification 

Once you tackle @rasbt's course, learn how to train models, and explore the basics of LLMs, you'll be ready to pass the exam and share the badge on LinkedIn!

Get started! http://lightning.ai/pages/courses/deep-learning-fundamentals/…
------
Have been heads-down porting LoRA (low-rank-adaptation) to finetune Falcon more efficiently and ran some performance benchmarks.

My longer write-up here: https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

Long story short: you can finetune Falcon in 1 h on a 52k dataset using a single GPU with 16 GB RAM.
------
Wanna save  and ship your LLMs to prod faster?

Learn how @rasbt finetunes Falcon on 1 GPU in just 1 hour!  https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

#LLMs #MachineLearning #AI #opensource
------
Btw, it's also a big reduction in memory: 16 gigs instead of 6 x 40 GB.

Due to the reduced parameter counts in the backward pass:

Full finetuning: 7,217,189,760
Adapter: 1,365,330
Adapter v2: 3,839,186
LoRA: 3,506,176

(I used  LoRA rank of 16 to match Adapter v2 above.)
------
Susan & I found MMLU performance jump 6-10 points in the 40s by formatting multiple choice as (A) not A in MMLU (for internal model). All evaluation of LLM's are broken. Evaluating a task requires marginalizing across all prompts that describe the task, not point estimate of one.
------
Never insult a scientist by suggesting they're p-hacking benchmarks.   twitter.com/suchenzang/sta…
------
I really like the LLaMA-Adapter method! 
In a nutshell, it's about finetuning LLMs more efficiently. 

In a nutshell, it's adding a tunable prefix to the key and value tensors in the self-attention layers.

Fun fact: it's not specific to LLaMA. Use it with any LLM!
------
Exciting news: LLaMA-Adapter is now fully unlocked! 6

 As a general-purpose #multimodal foundation model, it integrates various inputs like images, audio, text, video, and 3D point clouds, while providing image, text-based, and detection outputs. It uniquely accepts the… Show more
------
Reading  Machine Learning Q and AI from 
@rasbt
. Ended up buying after reading a free chapter on 
@leanpub
. Had such a rich references, that got me hooked. Looking forward to more bedtime reading.
------
Happy birthday, transformer! An awesome summary 
@DrJimFan
!

Also interesting to think about why we needed attention for RNNs (before transformers) in the first place.

Since we can't translate word-by-word, we needed a RNN encoder-decoder setup. But then, it's hard to remember.
------
Today 6 years ago, "Attention is All You Need" went on Arxiv! Happy birthday Transformer! 

Fun facts:
- Transformer did not invent attention, but pushed it to the extreme. The first attention paper was published 3 years prior (2014) and had an unassuming title: "Neural Machine… Show more
------
Thanks everyone for your interest ! I think someone recorded it — will share once I know more and am back from the conference! In the meantime I also have all the materials here on GitHub:
------
So much discussion about arxiv this morning. 

You can get a lot of positive value out of using arxiv if you use it with care. 

Eg does the original Transformer paper have inconsistencies (see LayerNorm) and is not super detailed? Sure. 

Is it a groundbreaking paper. Yes!
------
I am thankful that arxiv exists, but of course we need to take papers there with a grain of salt. But this is also generally true for peer-reviewed papers as well.

As always, you have to judge it on a case-by-case basis.
------
I often get requests to dispel some of the jargon behind transformers and LLMs!

So here we go, my new article on "Understanding Encoder and Decoder LLMs":
------
Am so glad about all the nice feedback about my Deep Learning course -- happy to hear that it was useful to tens of thousands of people!

Just added a final exam & completion badge for LinkedIn!

So, if you'd like to test your knowledge with 25 questions:
https://lightning.ai/pages/ai-education/deep-learning-fundamentals/certification/…
------
Today we're excited to announce our Deep Learning Fundamentals Certification 

Once you tackle @rasbt's course, learn how to train models, and explore the basics of LLMs, you'll be ready to pass the exam and share the badge on LinkedIn!

Get started! http://lightning.ai/pages/courses/deep-learning-fundamentals/…
------
Have been heads-down porting LoRA (low-rank-adaptation) to finetune Falcon more efficiently and ran some performance benchmarks.

My longer write-up here: https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

Long story short: you can finetune Falcon in 1 h on a 52k dataset using a single GPU with 16 GB RAM.
------
Wanna save  and ship your LLMs to prod faster?

Learn how @rasbt finetunes Falcon on 1 GPU in just 1 hour!  https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

#LLMs #MachineLearning #AI #opensource
------
Btw, it's also a big reduction in memory: 16 gigs instead of 6 x 40 GB.

Due to the reduced parameter counts in the backward pass:

Full finetuning: 7,217,189,760
Adapter: 1,365,330
Adapter v2: 3,839,186
LoRA: 3,506,176

(I used  LoRA rank of 16 to match Adapter v2 above.)
------
Susan & I found MMLU performance jump 6-10 points in the 40s by formatting multiple choice as (A) not A in MMLU (for internal model). All evaluation of LLM's are broken. Evaluating a task requires marginalizing across all prompts that describe the task, not point estimate of one.
------
Never insult a scientist by suggesting they're p-hacking benchmarks.   twitter.com/suchenzang/sta…
------
I really like the LLaMA-Adapter method! 
In a nutshell, it's about finetuning LLMs more efficiently. 

In a nutshell, it's adding a tunable prefix to the key and value tensors in the self-attention layers.

Fun fact: it's not specific to LLaMA. Use it with any LLM!
------
Exciting news: LLaMA-Adapter is now fully unlocked! 6

 As a general-purpose #multimodal foundation model, it integrates various inputs like images, audio, text, video, and 3D point clouds, while providing image, text-based, and detection outputs. It uniquely accepts the… Show more
------
Reading  Machine Learning Q and AI from 
@rasbt
. Ended up buying after reading a free chapter on 
@leanpub
. Had such a rich references, that got me hooked. Looking forward to more bedtime reading.
------
Happy birthday, transformer! An awesome summary 
@DrJimFan
!

Also interesting to think about why we needed attention for RNNs (before transformers) in the first place.

Since we can't translate word-by-word, we needed a RNN encoder-decoder setup. But then, it's hard to remember.
------
Today 6 years ago, "Attention is All You Need" went on Arxiv! Happy birthday Transformer! 

Fun facts:
- Transformer did not invent attention, but pushed it to the extreme. The first attention paper was published 3 years prior (2014) and had an unassuming title: "Neural Machine… Show more
------
Thanks everyone for your interest ! I think someone recorded it — will share once I know more and am back from the conference! In the meantime I also have all the materials here on GitHub:
------
So much discussion about arxiv this morning. 

You can get a lot of positive value out of using arxiv if you use it with care. 

Eg does the original Transformer paper have inconsistencies (see LayerNorm) and is not super detailed? Sure. 

Is it a groundbreaking paper. Yes!
------
I am thankful that arxiv exists, but of course we need to take papers there with a grain of salt. But this is also generally true for peer-reviewed papers as well.

As always, you have to judge it on a case-by-case basis.
------
I often get requests to dispel some of the jargon behind transformers and LLMs!

So here we go, my new article on "Understanding Encoder and Decoder LLMs":
------
Am so glad about all the nice feedback about my Deep Learning course -- happy to hear that it was useful to tens of thousands of people!

Just added a final exam & completion badge for LinkedIn!

So, if you'd like to test your knowledge with 25 questions:
https://lightning.ai/pages/ai-education/deep-learning-fundamentals/certification/…
------
Today we're excited to announce our Deep Learning Fundamentals Certification 

Once you tackle @rasbt's course, learn how to train models, and explore the basics of LLMs, you'll be ready to pass the exam and share the badge on LinkedIn!

Get started! http://lightning.ai/pages/courses/deep-learning-fundamentals/…
------
Have been heads-down porting LoRA (low-rank-adaptation) to finetune Falcon more efficiently and ran some performance benchmarks.

My longer write-up here: https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

Long story short: you can finetune Falcon in 1 h on a 52k dataset using a single GPU with 16 GB RAM.
------
Wanna save  and ship your LLMs to prod faster?

Learn how @rasbt finetunes Falcon on 1 GPU in just 1 hour!  https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

#LLMs #MachineLearning #AI #opensource
------
Btw, it's also a big reduction in memory: 16 gigs instead of 6 x 40 GB.

Due to the reduced parameter counts in the backward pass:

Full finetuning: 7,217,189,760
Adapter: 1,365,330
Adapter v2: 3,839,186
LoRA: 3,506,176

(I used  LoRA rank of 16 to match Adapter v2 above.)
------
Susan & I found MMLU performance jump 6-10 points in the 40s by formatting multiple choice as (A) not A in MMLU (for internal model). All evaluation of LLM's are broken. Evaluating a task requires marginalizing across all prompts that describe the task, not point estimate of one.
------
Never insult a scientist by suggesting they're p-hacking benchmarks.   twitter.com/suchenzang/sta…
------
I really like the LLaMA-Adapter method! 
In a nutshell, it's about finetuning LLMs more efficiently. 

In a nutshell, it's adding a tunable prefix to the key and value tensors in the self-attention layers.

Fun fact: it's not specific to LLaMA. Use it with any LLM!
------
Exciting news: LLaMA-Adapter is now fully unlocked! 6

 As a general-purpose #multimodal foundation model, it integrates various inputs like images, audio, text, video, and 3D point clouds, while providing image, text-based, and detection outputs. It uniquely accepts the… Show more
------
Reading  Machine Learning Q and AI from 
@rasbt
. Ended up buying after reading a free chapter on 
@leanpub
. Had such a rich references, that got me hooked. Looking forward to more bedtime reading.
------
Happy birthday, transformer! An awesome summary 
@DrJimFan
!

Also interesting to think about why we needed attention for RNNs (before transformers) in the first place.

Since we can't translate word-by-word, we needed a RNN encoder-decoder setup. But then, it's hard to remember.
------
Today 6 years ago, "Attention is All You Need" went on Arxiv! Happy birthday Transformer! 

Fun facts:
- Transformer did not invent attention, but pushed it to the extreme. The first attention paper was published 3 years prior (2014) and had an unassuming title: "Neural Machine… Show more
------
Thanks everyone for your interest ! I think someone recorded it — will share once I know more and am back from the conference! In the meantime I also have all the materials here on GitHub:
------
So much discussion about arxiv this morning. 

You can get a lot of positive value out of using arxiv if you use it with care. 

Eg does the original Transformer paper have inconsistencies (see LayerNorm) and is not super detailed? Sure. 

Is it a groundbreaking paper. Yes!
------
I am thankful that arxiv exists, but of course we need to take papers there with a grain of salt. But this is also generally true for peer-reviewed papers as well.

As always, you have to judge it on a case-by-case basis.
------
I often get requests to dispel some of the jargon behind transformers and LLMs!

So here we go, my new article on "Understanding Encoder and Decoder LLMs":
------
Am so glad about all the nice feedback about my Deep Learning course -- happy to hear that it was useful to tens of thousands of people!

Just added a final exam & completion badge for LinkedIn!

So, if you'd like to test your knowledge with 25 questions:
https://lightning.ai/pages/ai-education/deep-learning-fundamentals/certification/…
------
Today we're excited to announce our Deep Learning Fundamentals Certification 

Once you tackle @rasbt's course, learn how to train models, and explore the basics of LLMs, you'll be ready to pass the exam and share the badge on LinkedIn!

Get started! http://lightning.ai/pages/courses/deep-learning-fundamentals/…
------
Have been heads-down porting LoRA (low-rank-adaptation) to finetune Falcon more efficiently and ran some performance benchmarks.

My longer write-up here: https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

Long story short: you can finetune Falcon in 1 h on a 52k dataset using a single GPU with 16 GB RAM.
------
Wanna save  and ship your LLMs to prod faster?

Learn how @rasbt finetunes Falcon on 1 GPU in just 1 hour!  https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

#LLMs #MachineLearning #AI #opensource
------
Btw, it's also a big reduction in memory: 16 gigs instead of 6 x 40 GB.

Due to the reduced parameter counts in the backward pass:

Full finetuning: 7,217,189,760
Adapter: 1,365,330
Adapter v2: 3,839,186
LoRA: 3,506,176

(I used  LoRA rank of 16 to match Adapter v2 above.)
------
Susan & I found MMLU performance jump 6-10 points in the 40s by formatting multiple choice as (A) not A in MMLU (for internal model). All evaluation of LLM's are broken. Evaluating a task requires marginalizing across all prompts that describe the task, not point estimate of one.
------
Never insult a scientist by suggesting they're p-hacking benchmarks.   twitter.com/suchenzang/sta…
------
I really like the LLaMA-Adapter method! 
In a nutshell, it's about finetuning LLMs more efficiently. 

In a nutshell, it's adding a tunable prefix to the key and value tensors in the self-attention layers.

Fun fact: it's not specific to LLaMA. Use it with any LLM!
------
Exciting news: LLaMA-Adapter is now fully unlocked! 6

 As a general-purpose #multimodal foundation model, it integrates various inputs like images, audio, text, video, and 3D point clouds, while providing image, text-based, and detection outputs. It uniquely accepts the… Show more
------
Reading  Machine Learning Q and AI from 
@rasbt
. Ended up buying after reading a free chapter on 
@leanpub
. Had such a rich references, that got me hooked. Looking forward to more bedtime reading.
------
Happy birthday, transformer! An awesome summary 
@DrJimFan
!

Also interesting to think about why we needed attention for RNNs (before transformers) in the first place.

Since we can't translate word-by-word, we needed a RNN encoder-decoder setup. But then, it's hard to remember.
------
Today 6 years ago, "Attention is All You Need" went on Arxiv! Happy birthday Transformer! 

Fun facts:
- Transformer did not invent attention, but pushed it to the extreme. The first attention paper was published 3 years prior (2014) and had an unassuming title: "Neural Machine… Show more
------
Thanks everyone for your interest ! I think someone recorded it — will share once I know more and am back from the conference! In the meantime I also have all the materials here on GitHub:
------
So much discussion about arxiv this morning. 

You can get a lot of positive value out of using arxiv if you use it with care. 

Eg does the original Transformer paper have inconsistencies (see LayerNorm) and is not super detailed? Sure. 

Is it a groundbreaking paper. Yes!
------
I am thankful that arxiv exists, but of course we need to take papers there with a grain of salt. But this is also generally true for peer-reviewed papers as well.

As always, you have to judge it on a case-by-case basis.
------
I often get requests to dispel some of the jargon behind transformers and LLMs!

So here we go, my new article on "Understanding Encoder and Decoder LLMs":
------
Am so glad about all the nice feedback about my Deep Learning course -- happy to hear that it was useful to tens of thousands of people!

Just added a final exam & completion badge for LinkedIn!

So, if you'd like to test your knowledge with 25 questions:
https://lightning.ai/pages/ai-education/deep-learning-fundamentals/certification/…
------
Today we're excited to announce our Deep Learning Fundamentals Certification 

Once you tackle @rasbt's course, learn how to train models, and explore the basics of LLMs, you'll be ready to pass the exam and share the badge on LinkedIn!

Get started! http://lightning.ai/pages/courses/deep-learning-fundamentals/…
------
Have been heads-down porting LoRA (low-rank-adaptation) to finetune Falcon more efficiently and ran some performance benchmarks.

My longer write-up here: https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

Long story short: you can finetune Falcon in 1 h on a 52k dataset using a single GPU with 16 GB RAM.
------
Wanna save  and ship your LLMs to prod faster?

Learn how @rasbt finetunes Falcon on 1 GPU in just 1 hour!  https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

#LLMs #MachineLearning #AI #opensource
------
Btw, it's also a big reduction in memory: 16 gigs instead of 6 x 40 GB.

Due to the reduced parameter counts in the backward pass:

Full finetuning: 7,217,189,760
Adapter: 1,365,330
Adapter v2: 3,839,186
LoRA: 3,506,176

(I used  LoRA rank of 16 to match Adapter v2 above.)
------
Susan & I found MMLU performance jump 6-10 points in the 40s by formatting multiple choice as (A) not A in MMLU (for internal model). All evaluation of LLM's are broken. Evaluating a task requires marginalizing across all prompts that describe the task, not point estimate of one.
------
Never insult a scientist by suggesting they're p-hacking benchmarks.   twitter.com/suchenzang/sta…
------
I really like the LLaMA-Adapter method! 
In a nutshell, it's about finetuning LLMs more efficiently. 

In a nutshell, it's adding a tunable prefix to the key and value tensors in the self-attention layers.

Fun fact: it's not specific to LLaMA. Use it with any LLM!
------
Exciting news: LLaMA-Adapter is now fully unlocked! 6

 As a general-purpose #multimodal foundation model, it integrates various inputs like images, audio, text, video, and 3D point clouds, while providing image, text-based, and detection outputs. It uniquely accepts the… Show more
------
Reading  Machine Learning Q and AI from 
@rasbt
. Ended up buying after reading a free chapter on 
@leanpub
. Had such a rich references, that got me hooked. Looking forward to more bedtime reading.
------
Happy birthday, transformer! An awesome summary 
@DrJimFan
!

Also interesting to think about why we needed attention for RNNs (before transformers) in the first place.

Since we can't translate word-by-word, we needed a RNN encoder-decoder setup. But then, it's hard to remember.
------
Today 6 years ago, "Attention is All You Need" went on Arxiv! Happy birthday Transformer! 

Fun facts:
- Transformer did not invent attention, but pushed it to the extreme. The first attention paper was published 3 years prior (2014) and had an unassuming title: "Neural Machine… Show more
------
Thanks everyone for your interest ! I think someone recorded it — will share once I know more and am back from the conference! In the meantime I also have all the materials here on GitHub:
------
So much discussion about arxiv this morning. 

You can get a lot of positive value out of using arxiv if you use it with care. 

Eg does the original Transformer paper have inconsistencies (see LayerNorm) and is not super detailed? Sure. 

Is it a groundbreaking paper. Yes!
------
I am thankful that arxiv exists, but of course we need to take papers there with a grain of salt. But this is also generally true for peer-reviewed papers as well.

As always, you have to judge it on a case-by-case basis.
------
I often get requests to dispel some of the jargon behind transformers and LLMs!

So here we go, my new article on "Understanding Encoder and Decoder LLMs":
------
Am so glad about all the nice feedback about my Deep Learning course -- happy to hear that it was useful to tens of thousands of people!

Just added a final exam & completion badge for LinkedIn!

So, if you'd like to test your knowledge with 25 questions:
https://lightning.ai/pages/ai-education/deep-learning-fundamentals/certification/…
------
Today we're excited to announce our Deep Learning Fundamentals Certification 

Once you tackle @rasbt's course, learn how to train models, and explore the basics of LLMs, you'll be ready to pass the exam and share the badge on LinkedIn!

Get started! http://lightning.ai/pages/courses/deep-learning-fundamentals/…
------
Have been heads-down porting LoRA (low-rank-adaptation) to finetune Falcon more efficiently and ran some performance benchmarks.

My longer write-up here: https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

Long story short: you can finetune Falcon in 1 h on a 52k dataset using a single GPU with 16 GB RAM.
------
Wanna save  and ship your LLMs to prod faster?

Learn how @rasbt finetunes Falcon on 1 GPU in just 1 hour!  https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

#LLMs #MachineLearning #AI #opensource
------
Btw, it's also a big reduction in memory: 16 gigs instead of 6 x 40 GB.

Due to the reduced parameter counts in the backward pass:

Full finetuning: 7,217,189,760
Adapter: 1,365,330
Adapter v2: 3,839,186
LoRA: 3,506,176

(I used  LoRA rank of 16 to match Adapter v2 above.)
------
Susan & I found MMLU performance jump 6-10 points in the 40s by formatting multiple choice as (A) not A in MMLU (for internal model). All evaluation of LLM's are broken. Evaluating a task requires marginalizing across all prompts that describe the task, not point estimate of one.
------
Never insult a scientist by suggesting they're p-hacking benchmarks.   twitter.com/suchenzang/sta…
------
I really like the LLaMA-Adapter method! 
In a nutshell, it's about finetuning LLMs more efficiently. 

In a nutshell, it's adding a tunable prefix to the key and value tensors in the self-attention layers.

Fun fact: it's not specific to LLaMA. Use it with any LLM!
------
Exciting news: LLaMA-Adapter is now fully unlocked! 6

 As a general-purpose #multimodal foundation model, it integrates various inputs like images, audio, text, video, and 3D point clouds, while providing image, text-based, and detection outputs. It uniquely accepts the… Show more
------
Reading  Machine Learning Q and AI from 
@rasbt
. Ended up buying after reading a free chapter on 
@leanpub
. Had such a rich references, that got me hooked. Looking forward to more bedtime reading.
------
Happy birthday, transformer! An awesome summary 
@DrJimFan
!

Also interesting to think about why we needed attention for RNNs (before transformers) in the first place.

Since we can't translate word-by-word, we needed a RNN encoder-decoder setup. But then, it's hard to remember.
------
Today 6 years ago, "Attention is All You Need" went on Arxiv! Happy birthday Transformer! 

Fun facts:
- Transformer did not invent attention, but pushed it to the extreme. The first attention paper was published 3 years prior (2014) and had an unassuming title: "Neural Machine… Show more
------
Thanks everyone for your interest ! I think someone recorded it — will share once I know more and am back from the conference! In the meantime I also have all the materials here on GitHub:
------
So much discussion about arxiv this morning. 

You can get a lot of positive value out of using arxiv if you use it with care. 

Eg does the original Transformer paper have inconsistencies (see LayerNorm) and is not super detailed? Sure. 

Is it a groundbreaking paper. Yes!
------
I am thankful that arxiv exists, but of course we need to take papers there with a grain of salt. But this is also generally true for peer-reviewed papers as well.

As always, you have to judge it on a case-by-case basis.
------
I often get requests to dispel some of the jargon behind transformers and LLMs!

So here we go, my new article on "Understanding Encoder and Decoder LLMs":
------
Am so glad about all the nice feedback about my Deep Learning course -- happy to hear that it was useful to tens of thousands of people!

Just added a final exam & completion badge for LinkedIn!

So, if you'd like to test your knowledge with 25 questions:
https://lightning.ai/pages/ai-education/deep-learning-fundamentals/certification/…
------
Today we're excited to announce our Deep Learning Fundamentals Certification 

Once you tackle @rasbt's course, learn how to train models, and explore the basics of LLMs, you'll be ready to pass the exam and share the badge on LinkedIn!

Get started! http://lightning.ai/pages/courses/deep-learning-fundamentals/…
------
Have been heads-down porting LoRA (low-rank-adaptation) to finetune Falcon more efficiently and ran some performance benchmarks.

My longer write-up here: https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

Long story short: you can finetune Falcon in 1 h on a 52k dataset using a single GPU with 16 GB RAM.
------
Wanna save  and ship your LLMs to prod faster?

Learn how @rasbt finetunes Falcon on 1 GPU in just 1 hour!  https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

#LLMs #MachineLearning #AI #opensource
------
Btw, it's also a big reduction in memory: 16 gigs instead of 6 x 40 GB.

Due to the reduced parameter counts in the backward pass:

Full finetuning: 7,217,189,760
Adapter: 1,365,330
Adapter v2: 3,839,186
LoRA: 3,506,176

(I used  LoRA rank of 16 to match Adapter v2 above.)
------
Susan & I found MMLU performance jump 6-10 points in the 40s by formatting multiple choice as (A) not A in MMLU (for internal model). All evaluation of LLM's are broken. Evaluating a task requires marginalizing across all prompts that describe the task, not point estimate of one.
------
Never insult a scientist by suggesting they're p-hacking benchmarks.   twitter.com/suchenzang/sta…
------
I really like the LLaMA-Adapter method! 
In a nutshell, it's about finetuning LLMs more efficiently. 

In a nutshell, it's adding a tunable prefix to the key and value tensors in the self-attention layers.

Fun fact: it's not specific to LLaMA. Use it with any LLM!
------
Exciting news: LLaMA-Adapter is now fully unlocked! 6

 As a general-purpose #multimodal foundation model, it integrates various inputs like images, audio, text, video, and 3D point clouds, while providing image, text-based, and detection outputs. It uniquely accepts the… Show more
------
Reading  Machine Learning Q and AI from 
@rasbt
. Ended up buying after reading a free chapter on 
@leanpub
. Had such a rich references, that got me hooked. Looking forward to more bedtime reading.
------
Happy birthday, transformer! An awesome summary 
@DrJimFan
!

Also interesting to think about why we needed attention for RNNs (before transformers) in the first place.

Since we can't translate word-by-word, we needed a RNN encoder-decoder setup. But then, it's hard to remember.
------
Today 6 years ago, "Attention is All You Need" went on Arxiv! Happy birthday Transformer! 

Fun facts:
- Transformer did not invent attention, but pushed it to the extreme. The first attention paper was published 3 years prior (2014) and had an unassuming title: "Neural Machine… Show more
------
Thanks everyone for your interest ! I think someone recorded it — will share once I know more and am back from the conference! In the meantime I also have all the materials here on GitHub:
------
So much discussion about arxiv this morning. 

You can get a lot of positive value out of using arxiv if you use it with care. 

Eg does the original Transformer paper have inconsistencies (see LayerNorm) and is not super detailed? Sure. 

Is it a groundbreaking paper. Yes!
------
I am thankful that arxiv exists, but of course we need to take papers there with a grain of salt. But this is also generally true for peer-reviewed papers as well.

As always, you have to judge it on a case-by-case basis.
------
I often get requests to dispel some of the jargon behind transformers and LLMs!

So here we go, my new article on "Understanding Encoder and Decoder LLMs":
------
Am so glad about all the nice feedback about my Deep Learning course -- happy to hear that it was useful to tens of thousands of people!

Just added a final exam & completion badge for LinkedIn!

So, if you'd like to test your knowledge with 25 questions:
https://lightning.ai/pages/ai-education/deep-learning-fundamentals/certification/…
------
Today we're excited to announce our Deep Learning Fundamentals Certification 

Once you tackle @rasbt's course, learn how to train models, and explore the basics of LLMs, you'll be ready to pass the exam and share the badge on LinkedIn!

Get started! http://lightning.ai/pages/courses/deep-learning-fundamentals/…
------
Have been heads-down porting LoRA (low-rank-adaptation) to finetune Falcon more efficiently and ran some performance benchmarks.

My longer write-up here: https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

Long story short: you can finetune Falcon in 1 h on a 52k dataset using a single GPU with 16 GB RAM.
------
Wanna save  and ship your LLMs to prod faster?

Learn how @rasbt finetunes Falcon on 1 GPU in just 1 hour!  https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

#LLMs #MachineLearning #AI #opensource
------
Btw, it's also a big reduction in memory: 16 gigs instead of 6 x 40 GB.

Due to the reduced parameter counts in the backward pass:

Full finetuning: 7,217,189,760
Adapter: 1,365,330
Adapter v2: 3,839,186
LoRA: 3,506,176

(I used  LoRA rank of 16 to match Adapter v2 above.)
------
Susan & I found MMLU performance jump 6-10 points in the 40s by formatting multiple choice as (A) not A in MMLU (for internal model). All evaluation of LLM's are broken. Evaluating a task requires marginalizing across all prompts that describe the task, not point estimate of one.
------
Never insult a scientist by suggesting they're p-hacking benchmarks.   twitter.com/suchenzang/sta…
------
I really like the LLaMA-Adapter method! 
In a nutshell, it's about finetuning LLMs more efficiently. 

In a nutshell, it's adding a tunable prefix to the key and value tensors in the self-attention layers.

Fun fact: it's not specific to LLaMA. Use it with any LLM!
------
Exciting news: LLaMA-Adapter is now fully unlocked! 6

 As a general-purpose #multimodal foundation model, it integrates various inputs like images, audio, text, video, and 3D point clouds, while providing image, text-based, and detection outputs. It uniquely accepts the… Show more
------
Reading  Machine Learning Q and AI from 
@rasbt
. Ended up buying after reading a free chapter on 
@leanpub
. Had such a rich references, that got me hooked. Looking forward to more bedtime reading.
------
Happy birthday, transformer! An awesome summary 
@DrJimFan
!

Also interesting to think about why we needed attention for RNNs (before transformers) in the first place.

Since we can't translate word-by-word, we needed a RNN encoder-decoder setup. But then, it's hard to remember.
------
Today 6 years ago, "Attention is All You Need" went on Arxiv! Happy birthday Transformer! 

Fun facts:
- Transformer did not invent attention, but pushed it to the extreme. The first attention paper was published 3 years prior (2014) and had an unassuming title: "Neural Machine… Show more
------
Thanks everyone for your interest ! I think someone recorded it — will share once I know more and am back from the conference! In the meantime I also have all the materials here on GitHub:
------
So much discussion about arxiv this morning. 

You can get a lot of positive value out of using arxiv if you use it with care. 

Eg does the original Transformer paper have inconsistencies (see LayerNorm) and is not super detailed? Sure. 

Is it a groundbreaking paper. Yes!
------
I am thankful that arxiv exists, but of course we need to take papers there with a grain of salt. But this is also generally true for peer-reviewed papers as well.

As always, you have to judge it on a case-by-case basis.
------
I often get requests to dispel some of the jargon behind transformers and LLMs!

So here we go, my new article on "Understanding Encoder and Decoder LLMs":
------
Am so glad about all the nice feedback about my Deep Learning course -- happy to hear that it was useful to tens of thousands of people!

Just added a final exam & completion badge for LinkedIn!

So, if you'd like to test your knowledge with 25 questions:
https://lightning.ai/pages/ai-education/deep-learning-fundamentals/certification/…
------
Today we're excited to announce our Deep Learning Fundamentals Certification 

Once you tackle @rasbt's course, learn how to train models, and explore the basics of LLMs, you'll be ready to pass the exam and share the badge on LinkedIn!

Get started! http://lightning.ai/pages/courses/deep-learning-fundamentals/…
------
Have been heads-down porting LoRA (low-rank-adaptation) to finetune Falcon more efficiently and ran some performance benchmarks.

My longer write-up here: https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

Long story short: you can finetune Falcon in 1 h on a 52k dataset using a single GPU with 16 GB RAM.
------
Wanna save  and ship your LLMs to prod faster?

Learn how @rasbt finetunes Falcon on 1 GPU in just 1 hour!  https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

#LLMs #MachineLearning #AI #opensource
------
Btw, it's also a big reduction in memory: 16 gigs instead of 6 x 40 GB.

Due to the reduced parameter counts in the backward pass:

Full finetuning: 7,217,189,760
Adapter: 1,365,330
Adapter v2: 3,839,186
LoRA: 3,506,176

(I used  LoRA rank of 16 to match Adapter v2 above.)
------
Susan & I found MMLU performance jump 6-10 points in the 40s by formatting multiple choice as (A) not A in MMLU (for internal model). All evaluation of LLM's are broken. Evaluating a task requires marginalizing across all prompts that describe the task, not point estimate of one.
------
Never insult a scientist by suggesting they're p-hacking benchmarks.   twitter.com/suchenzang/sta…
------
I really like the LLaMA-Adapter method! 
In a nutshell, it's about finetuning LLMs more efficiently. 

In a nutshell, it's adding a tunable prefix to the key and value tensors in the self-attention layers.

Fun fact: it's not specific to LLaMA. Use it with any LLM!
------
Exciting news: LLaMA-Adapter is now fully unlocked! 6

 As a general-purpose #multimodal foundation model, it integrates various inputs like images, audio, text, video, and 3D point clouds, while providing image, text-based, and detection outputs. It uniquely accepts the… Show more
------
Reading  Machine Learning Q and AI from 
@rasbt
. Ended up buying after reading a free chapter on 
@leanpub
. Had such a rich references, that got me hooked. Looking forward to more bedtime reading.
------
Happy birthday, transformer! An awesome summary 
@DrJimFan
!

Also interesting to think about why we needed attention for RNNs (before transformers) in the first place.

Since we can't translate word-by-word, we needed a RNN encoder-decoder setup. But then, it's hard to remember.
------
Today 6 years ago, "Attention is All You Need" went on Arxiv! Happy birthday Transformer! 

Fun facts:
- Transformer did not invent attention, but pushed it to the extreme. The first attention paper was published 3 years prior (2014) and had an unassuming title: "Neural Machine… Show more
------
Thanks everyone for your interest ! I think someone recorded it — will share once I know more and am back from the conference! In the meantime I also have all the materials here on GitHub:
------
So much discussion about arxiv this morning. 

You can get a lot of positive value out of using arxiv if you use it with care. 

Eg does the original Transformer paper have inconsistencies (see LayerNorm) and is not super detailed? Sure. 

Is it a groundbreaking paper. Yes!
------
I am thankful that arxiv exists, but of course we need to take papers there with a grain of salt. But this is also generally true for peer-reviewed papers as well.

As always, you have to judge it on a case-by-case basis.
------
I often get requests to dispel some of the jargon behind transformers and LLMs!

So here we go, my new article on "Understanding Encoder and Decoder LLMs":
------
Am so glad about all the nice feedback about my Deep Learning course -- happy to hear that it was useful to tens of thousands of people!

Just added a final exam & completion badge for LinkedIn!

So, if you'd like to test your knowledge with 25 questions:
https://lightning.ai/pages/ai-education/deep-learning-fundamentals/certification/…
------
Today we're excited to announce our Deep Learning Fundamentals Certification 

Once you tackle @rasbt's course, learn how to train models, and explore the basics of LLMs, you'll be ready to pass the exam and share the badge on LinkedIn!

Get started! http://lightning.ai/pages/courses/deep-learning-fundamentals/…
------
Have been heads-down porting LoRA (low-rank-adaptation) to finetune Falcon more efficiently and ran some performance benchmarks.

My longer write-up here: https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

Long story short: you can finetune Falcon in 1 h on a 52k dataset using a single GPU with 16 GB RAM.
------
Wanna save  and ship your LLMs to prod faster?

Learn how @rasbt finetunes Falcon on 1 GPU in just 1 hour!  https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

#LLMs #MachineLearning #AI #opensource
------
Btw, it's also a big reduction in memory: 16 gigs instead of 6 x 40 GB.

Due to the reduced parameter counts in the backward pass:

Full finetuning: 7,217,189,760
Adapter: 1,365,330
Adapter v2: 3,839,186
LoRA: 3,506,176

(I used  LoRA rank of 16 to match Adapter v2 above.)
------
Susan & I found MMLU performance jump 6-10 points in the 40s by formatting multiple choice as (A) not A in MMLU (for internal model). All evaluation of LLM's are broken. Evaluating a task requires marginalizing across all prompts that describe the task, not point estimate of one.
------
Never insult a scientist by suggesting they're p-hacking benchmarks.   twitter.com/suchenzang/sta…
------
I really like the LLaMA-Adapter method! 
In a nutshell, it's about finetuning LLMs more efficiently. 

In a nutshell, it's adding a tunable prefix to the key and value tensors in the self-attention layers.

Fun fact: it's not specific to LLaMA. Use it with any LLM!
------
Exciting news: LLaMA-Adapter is now fully unlocked! 6

 As a general-purpose #multimodal foundation model, it integrates various inputs like images, audio, text, video, and 3D point clouds, while providing image, text-based, and detection outputs. It uniquely accepts the… Show more
------
Reading  Machine Learning Q and AI from 
@rasbt
. Ended up buying after reading a free chapter on 
@leanpub
. Had such a rich references, that got me hooked. Looking forward to more bedtime reading.
------
Happy birthday, transformer! An awesome summary 
@DrJimFan
!

Also interesting to think about why we needed attention for RNNs (before transformers) in the first place.

Since we can't translate word-by-word, we needed a RNN encoder-decoder setup. But then, it's hard to remember.
------
Today 6 years ago, "Attention is All You Need" went on Arxiv! Happy birthday Transformer! 

Fun facts:
- Transformer did not invent attention, but pushed it to the extreme. The first attention paper was published 3 years prior (2014) and had an unassuming title: "Neural Machine… Show more
------
Thanks everyone for your interest ! I think someone recorded it — will share once I know more and am back from the conference! In the meantime I also have all the materials here on GitHub:
------
So much discussion about arxiv this morning. 

You can get a lot of positive value out of using arxiv if you use it with care. 

Eg does the original Transformer paper have inconsistencies (see LayerNorm) and is not super detailed? Sure. 

Is it a groundbreaking paper. Yes!
------
I am thankful that arxiv exists, but of course we need to take papers there with a grain of salt. But this is also generally true for peer-reviewed papers as well.

As always, you have to judge it on a case-by-case basis.
------
I often get requests to dispel some of the jargon behind transformers and LLMs!

So here we go, my new article on "Understanding Encoder and Decoder LLMs":
------
Am so glad about all the nice feedback about my Deep Learning course -- happy to hear that it was useful to tens of thousands of people!

Just added a final exam & completion badge for LinkedIn!

So, if you'd like to test your knowledge with 25 questions:
https://lightning.ai/pages/ai-education/deep-learning-fundamentals/certification/…
------
Today we're excited to announce our Deep Learning Fundamentals Certification 

Once you tackle @rasbt's course, learn how to train models, and explore the basics of LLMs, you'll be ready to pass the exam and share the badge on LinkedIn!

Get started! http://lightning.ai/pages/courses/deep-learning-fundamentals/…
------
Have been heads-down porting LoRA (low-rank-adaptation) to finetune Falcon more efficiently and ran some performance benchmarks.

My longer write-up here: https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

Long story short: you can finetune Falcon in 1 h on a 52k dataset using a single GPU with 16 GB RAM.
------
Wanna save  and ship your LLMs to prod faster?

Learn how @rasbt finetunes Falcon on 1 GPU in just 1 hour!  https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

#LLMs #MachineLearning #AI #opensource
------
Btw, it's also a big reduction in memory: 16 gigs instead of 6 x 40 GB.

Due to the reduced parameter counts in the backward pass:

Full finetuning: 7,217,189,760
Adapter: 1,365,330
Adapter v2: 3,839,186
LoRA: 3,506,176

(I used  LoRA rank of 16 to match Adapter v2 above.)
------
Susan & I found MMLU performance jump 6-10 points in the 40s by formatting multiple choice as (A) not A in MMLU (for internal model). All evaluation of LLM's are broken. Evaluating a task requires marginalizing across all prompts that describe the task, not point estimate of one.
------
Never insult a scientist by suggesting they're p-hacking benchmarks.   twitter.com/suchenzang/sta…
------
I really like the LLaMA-Adapter method! 
In a nutshell, it's about finetuning LLMs more efficiently. 

In a nutshell, it's adding a tunable prefix to the key and value tensors in the self-attention layers.

Fun fact: it's not specific to LLaMA. Use it with any LLM!
------
Exciting news: LLaMA-Adapter is now fully unlocked! 6

 As a general-purpose #multimodal foundation model, it integrates various inputs like images, audio, text, video, and 3D point clouds, while providing image, text-based, and detection outputs. It uniquely accepts the… Show more
------
Reading  Machine Learning Q and AI from 
@rasbt
. Ended up buying after reading a free chapter on 
@leanpub
. Had such a rich references, that got me hooked. Looking forward to more bedtime reading.
------
Happy birthday, transformer! An awesome summary 
@DrJimFan
!

Also interesting to think about why we needed attention for RNNs (before transformers) in the first place.

Since we can't translate word-by-word, we needed a RNN encoder-decoder setup. But then, it's hard to remember.
------
Today 6 years ago, "Attention is All You Need" went on Arxiv! Happy birthday Transformer! 

Fun facts:
- Transformer did not invent attention, but pushed it to the extreme. The first attention paper was published 3 years prior (2014) and had an unassuming title: "Neural Machine… Show more
------
Thanks everyone for your interest ! I think someone recorded it — will share once I know more and am back from the conference! In the meantime I also have all the materials here on GitHub:
------
So much discussion about arxiv this morning. 

You can get a lot of positive value out of using arxiv if you use it with care. 

Eg does the original Transformer paper have inconsistencies (see LayerNorm) and is not super detailed? Sure. 

Is it a groundbreaking paper. Yes!
------
I am thankful that arxiv exists, but of course we need to take papers there with a grain of salt. But this is also generally true for peer-reviewed papers as well.

As always, you have to judge it on a case-by-case basis.
------
I often get requests to dispel some of the jargon behind transformers and LLMs!

So here we go, my new article on "Understanding Encoder and Decoder LLMs":
------
Am so glad about all the nice feedback about my Deep Learning course -- happy to hear that it was useful to tens of thousands of people!

Just added a final exam & completion badge for LinkedIn!

So, if you'd like to test your knowledge with 25 questions:
https://lightning.ai/pages/ai-education/deep-learning-fundamentals/certification/…
------
Today we're excited to announce our Deep Learning Fundamentals Certification 

Once you tackle @rasbt's course, learn how to train models, and explore the basics of LLMs, you'll be ready to pass the exam and share the badge on LinkedIn!

Get started! http://lightning.ai/pages/courses/deep-learning-fundamentals/…
------
Have been heads-down porting LoRA (low-rank-adaptation) to finetune Falcon more efficiently and ran some performance benchmarks.

My longer write-up here: https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

Long story short: you can finetune Falcon in 1 h on a 52k dataset using a single GPU with 16 GB RAM.
------
Wanna save  and ship your LLMs to prod faster?

Learn how @rasbt finetunes Falcon on 1 GPU in just 1 hour!  https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

#LLMs #MachineLearning #AI #opensource
------
Btw, it's also a big reduction in memory: 16 gigs instead of 6 x 40 GB.

Due to the reduced parameter counts in the backward pass:

Full finetuning: 7,217,189,760
Adapter: 1,365,330
Adapter v2: 3,839,186
LoRA: 3,506,176

(I used  LoRA rank of 16 to match Adapter v2 above.)
------
Susan & I found MMLU performance jump 6-10 points in the 40s by formatting multiple choice as (A) not A in MMLU (for internal model). All evaluation of LLM's are broken. Evaluating a task requires marginalizing across all prompts that describe the task, not point estimate of one.
------
Never insult a scientist by suggesting they're p-hacking benchmarks.   twitter.com/suchenzang/sta…
------
I really like the LLaMA-Adapter method! 
In a nutshell, it's about finetuning LLMs more efficiently. 

In a nutshell, it's adding a tunable prefix to the key and value tensors in the self-attention layers.

Fun fact: it's not specific to LLaMA. Use it with any LLM!
------
Exciting news: LLaMA-Adapter is now fully unlocked! 6

 As a general-purpose #multimodal foundation model, it integrates various inputs like images, audio, text, video, and 3D point clouds, while providing image, text-based, and detection outputs. It uniquely accepts the… Show more
------
Reading  Machine Learning Q and AI from 
@rasbt
. Ended up buying after reading a free chapter on 
@leanpub
. Had such a rich references, that got me hooked. Looking forward to more bedtime reading.
------
Happy birthday, transformer! An awesome summary 
@DrJimFan
!

Also interesting to think about why we needed attention for RNNs (before transformers) in the first place.

Since we can't translate word-by-word, we needed a RNN encoder-decoder setup. But then, it's hard to remember.
------
Today 6 years ago, "Attention is All You Need" went on Arxiv! Happy birthday Transformer! 

Fun facts:
- Transformer did not invent attention, but pushed it to the extreme. The first attention paper was published 3 years prior (2014) and had an unassuming title: "Neural Machine… Show more
------
Thanks everyone for your interest ! I think someone recorded it — will share once I know more and am back from the conference! In the meantime I also have all the materials here on GitHub:
------
So much discussion about arxiv this morning. 

You can get a lot of positive value out of using arxiv if you use it with care. 

Eg does the original Transformer paper have inconsistencies (see LayerNorm) and is not super detailed? Sure. 

Is it a groundbreaking paper. Yes!
------
I am thankful that arxiv exists, but of course we need to take papers there with a grain of salt. But this is also generally true for peer-reviewed papers as well.

As always, you have to judge it on a case-by-case basis.
------
I often get requests to dispel some of the jargon behind transformers and LLMs!

So here we go, my new article on "Understanding Encoder and Decoder LLMs":
------
Am so glad about all the nice feedback about my Deep Learning course -- happy to hear that it was useful to tens of thousands of people!

Just added a final exam & completion badge for LinkedIn!

So, if you'd like to test your knowledge with 25 questions:
https://lightning.ai/pages/ai-education/deep-learning-fundamentals/certification/…
------
Today we're excited to announce our Deep Learning Fundamentals Certification 

Once you tackle @rasbt's course, learn how to train models, and explore the basics of LLMs, you'll be ready to pass the exam and share the badge on LinkedIn!

Get started! http://lightning.ai/pages/courses/deep-learning-fundamentals/…
------
Have been heads-down porting LoRA (low-rank-adaptation) to finetune Falcon more efficiently and ran some performance benchmarks.

My longer write-up here: https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

Long story short: you can finetune Falcon in 1 h on a 52k dataset using a single GPU with 16 GB RAM.
------
Wanna save  and ship your LLMs to prod faster?

Learn how @rasbt finetunes Falcon on 1 GPU in just 1 hour!  https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

#LLMs #MachineLearning #AI #opensource
------
Btw, it's also a big reduction in memory: 16 gigs instead of 6 x 40 GB.

Due to the reduced parameter counts in the backward pass:

Full finetuning: 7,217,189,760
Adapter: 1,365,330
Adapter v2: 3,839,186
LoRA: 3,506,176

(I used  LoRA rank of 16 to match Adapter v2 above.)
------
Susan & I found MMLU performance jump 6-10 points in the 40s by formatting multiple choice as (A) not A in MMLU (for internal model). All evaluation of LLM's are broken. Evaluating a task requires marginalizing across all prompts that describe the task, not point estimate of one.
------
Never insult a scientist by suggesting they're p-hacking benchmarks.   twitter.com/suchenzang/sta…
------
I really like the LLaMA-Adapter method! 
In a nutshell, it's about finetuning LLMs more efficiently. 

In a nutshell, it's adding a tunable prefix to the key and value tensors in the self-attention layers.

Fun fact: it's not specific to LLaMA. Use it with any LLM!
------
Exciting news: LLaMA-Adapter is now fully unlocked! 6

 As a general-purpose #multimodal foundation model, it integrates various inputs like images, audio, text, video, and 3D point clouds, while providing image, text-based, and detection outputs. It uniquely accepts the… Show more
------
Reading  Machine Learning Q and AI from 
@rasbt
. Ended up buying after reading a free chapter on 
@leanpub
. Had such a rich references, that got me hooked. Looking forward to more bedtime reading.
------
Happy birthday, transformer! An awesome summary 
@DrJimFan
!

Also interesting to think about why we needed attention for RNNs (before transformers) in the first place.

Since we can't translate word-by-word, we needed a RNN encoder-decoder setup. But then, it's hard to remember.
------
Today 6 years ago, "Attention is All You Need" went on Arxiv! Happy birthday Transformer! 

Fun facts:
- Transformer did not invent attention, but pushed it to the extreme. The first attention paper was published 3 years prior (2014) and had an unassuming title: "Neural Machine… Show more
------
Today is the day! 
My new book Machine Learning Q and AI is now complete!

 https://leanpub.com/machine-learning-q-and-ai/…

Covering 
- Explanations of multi-GPU training paradigms.
- Using and finetuning transformers.
- Differences between encoder- and decoder-style LLMs.
- And many more!

1/3
------
It took me almost a year and countless weekends, and a lot of discipline (writing 1 chapter almost every weekend). But I loved (almost) every minute of it!

2/3
------
I am really excited with how it turned out. Finally a resource for those who are interested in a bit more advanced machine learning, deep learning, and AI topics.

PS: I also partnered with a publisher to also release print version later this summer. Stay tuned!

3/3
------
Ok, so it's now official: using the official MMLU evaluation, LLaMA 1) indeed matches the score in the original LLaMA paper and 2) has quite the lead on Falcon. Ofc it's just one benchmark, but it shows how much details matter when evaluating LLMs.
------
Guys, I know you want watch toe-to-toe battles. Here you go:

Under official MMLU prompts, default huggingface generate() function, fp16, no fancy prompt engineering, no more complication:

LLaMA v.s Falcon = 63.64 v.s 49.08

Happy? Disappointed? Good? Bad? Win? Lose?

code +… Show more
------
2005: too bad I can’t find any research paper on that concept yet.

2010: There is now a paper that analyzes this concept!

2018: in which order should I best read all these papers on that topic?

2023: which subset of all these papers with contradictory results is worth reading?
------
Paper writing protip:

Most papers are not read end-to-end. Ain't nobody got time. Write with that in mind. Make sections, figs, tables and their captions as self-contained and "guessable" as reasonably possible.

Example: call your models Foo-M and Foo-S instead of Foo and Foo*
------
It's been a another wild month in AI & Deep Learning research. 
I curated and summarized noteworthy papers here: https://magazine.sebastianraschka.com/p/ai-research-highlights-in-3-sentences-2a1/…

Ranging from new optimizers for LLMs to new scaling laws for vision transformers.
------
Just saw that the LIMA dataset is now available! https://huggingface.co/datasets/GAIR/lima…
------
"LIMA: Less Is More for Alignment" might be a gamechanger for researchers & tinkerers who want to develop capable LLMs. 

The researchers showed that a 65B LLaMA model finetuned on only 1000 examples (in supervised fashion) is not too far behind bigger models like ChatGPT!

1/6
------
Fun fact: LLaMA-Adapter is not LLaMA-specific. You can use it to finetune any LLM.

Below, I finetuned a 40B Falcon model using LLaMA-Adapter (it also works on a single GPU with 20 GB RAM).

Have been heads down tinkering & will follow up with more benchmarks!
------
Finetuning Falcon 40B on the Alpaca instruction dataset takes 30 hours on 8 A100s. Cut it to 30 minutes using LLaMA Adapter 

Try it at https://github.com/Lightning-AI/lit-parrot…

#LLM #MachineLearning
------
"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding".

This method is pretty cool, but what is this new trend where papers don't contain any type of quantitative model evaluation?

Looks like researchers are in a rush.

 https://arxiv.org/abs/2306.02858
------
Saw the discussions regarding Stability AI today, & I am a bit surprised that the paper behind it was allegedly swept under the rug.
I do think it was known that it was based on Latent Diffusion? 
Covered it back in 2022 in an early Ahead of AI issue:

https://magazine.sebastianraschka.com/p/ahead-of-ai-4-a-big-year-for-ai…
------
Oh, and I should have emphasized that you can use our CORN (Conditional Ordinal Regression for Neural Networks) method to finetune your favorite LLM for prediction tasks, or course!

An example for TripAdvisor customer reviews here: https://github.com/Raschka-research-group/coral-pytorch/blob/main/docs/tutorials/pytorch_lightning/distilbert-corn-tripadvisor.ipynb…
------
I often like to share AI research articles I find interesting. Let me change it up a bit and share one of our own articles that just passed peer review &  got accepted.

Our method allows you to use any model (LLM, CNN, ViT, ...) for ordinal regression on ordered labels.

1/6
------
Plus this Falcon implementation features the same readable & hackable code as Lit-LLaMA (https://github.com/Lightning-AI/lit-parrot…)

More analyses will follow soon! (And pls feel free to reach out if you have questions about the current implementation)
------
We couldn't wait to tinker with the best performing opensource LLM (Falcon 40B) 

We integrated it into Lit-Parrot and sped it up by 20%! We'll keep adding optimizations to squeeze out more performance!

Check it out: https://github.com/Lightning-AI/lit-parrot…
------
Am so glad about all the nice feedback about my Deep Learning course -- happy to hear that it was useful to tens of thousands of people!

Just added a final exam & completion badge for LinkedIn!

So, if you'd like to test your knowledge with 25 questions:
https://lightning.ai/pages/ai-education/deep-learning-fundamentals/certification/…
------
Today we're excited to announce our Deep Learning Fundamentals Certification 

Once you tackle @rasbt's course, learn how to train models, and explore the basics of LLMs, you'll be ready to pass the exam and share the badge on LinkedIn!

Get started! http://lightning.ai/pages/courses/deep-learning-fundamentals/…
------
Have been heads-down porting LoRA (low-rank-adaptation) to finetune Falcon more efficiently and ran some performance benchmarks.

My longer write-up here: https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

Long story short: you can finetune Falcon in 1 h on a 52k dataset using a single GPU with 16 GB RAM.
------
Wanna save  and ship your LLMs to prod faster?

Learn how @rasbt finetunes Falcon on 1 GPU in just 1 hour!  https://lightning.ai/pages/community/finetuning-falcon-efficiently/…

#LLMs #MachineLearning #AI #opensource
------
Btw, it's also a big reduction in memory: 16 gigs instead of 6 x 40 GB.

Due to the reduced parameter counts in the backward pass:

Full finetuning: 7,217,189,760
Adapter: 1,365,330
Adapter v2: 3,839,186
LoRA: 3,506,176

(I used  LoRA rank of 16 to match Adapter v2 above.)
------
Susan & I found MMLU performance jump 6-10 points in the 40s by formatting multiple choice as (A) not A in MMLU (for internal model). All evaluation of LLM's are broken. Evaluating a task requires marginalizing across all prompts that describe the task, not point estimate of one.
------
Never insult a scientist by suggesting they're p-hacking benchmarks.   twitter.com/suchenzang/sta…
------
I really like the LLaMA-Adapter method! 
In a nutshell, it's about finetuning LLMs more efficiently. 

In a nutshell, it's adding a tunable prefix to the key and value tensors in the self-attention layers.

Fun fact: it's not specific to LLaMA. Use it with any LLM!
------
Exciting news: LLaMA-Adapter is now fully unlocked! 6

 As a general-purpose #multimodal foundation model, it integrates various inputs like images, audio, text, video, and 3D point clouds, while providing image, text-based, and detection outputs. It uniquely accepts the… Show more
------
Reading  Machine Learning Q and AI from 
@rasbt
. Ended up buying after reading a free chapter on 
@leanpub
. Had such a rich references, that got me hooked. Looking forward to more bedtime reading.
------
Happy birthday, transformer! An awesome summary 
@DrJimFan
!

Also interesting to think about why we needed attention for RNNs (before transformers) in the first place.

Since we can't translate word-by-word, we needed a RNN encoder-decoder setup. But then, it's hard to remember.
------
Today 6 years ago, "Attention is All You Need" went on Arxiv! Happy birthday Transformer! 

Fun facts:
- Transformer did not invent attention, but pushed it to the extreme. The first attention paper was published 3 years prior (2014) and had an unassuming title: "Neural Machine… Show more
------
Today is the day! 
My new book Machine Learning Q and AI is now complete!

 https://leanpub.com/machine-learning-q-and-ai/…

Covering 
- Explanations of multi-GPU training paradigms.
- Using and finetuning transformers.
- Differences between encoder- and decoder-style LLMs.
- And many more!

1/3
------
It took me almost a year and countless weekends, and a lot of discipline (writing 1 chapter almost every weekend). But I loved (almost) every minute of it!

2/3
------
I am really excited with how it turned out. Finally a resource for those who are interested in a bit more advanced machine learning, deep learning, and AI topics.

PS: I also partnered with a publisher to also release print version later this summer. Stay tuned!

3/3
------
Ok, so it's now official: using the official MMLU evaluation, LLaMA 1) indeed matches the score in the original LLaMA paper and 2) has quite the lead on Falcon. Ofc it's just one benchmark, but it shows how much details matter when evaluating LLMs.
------
Guys, I know you want watch toe-to-toe battles. Here you go:

Under official MMLU prompts, default huggingface generate() function, fp16, no fancy prompt engineering, no more complication:

LLaMA v.s Falcon = 63.64 v.s 49.08

Happy? Disappointed? Good? Bad? Win? Lose?

code +… Show more
------
2005: too bad I can’t find any research paper on that concept yet.

2010: There is now a paper that analyzes this concept!

2018: in which order should I best read all these papers on that topic?

2023: which subset of all these papers with contradictory results is worth reading?
------
Paper writing protip:

Most papers are not read end-to-end. Ain't nobody got time. Write with that in mind. Make sections, figs, tables and their captions as self-contained and "guessable" as reasonably possible.

Example: call your models Foo-M and Foo-S instead of Foo and Foo*
------
It's been a another wild month in AI & Deep Learning research. 
I curated and summarized noteworthy papers here: https://magazine.sebastianraschka.com/p/ai-research-highlights-in-3-sentences-2a1/…

Ranging from new optimizers for LLMs to new scaling laws for vision transformers.
------
Just saw that the LIMA dataset is now available! https://huggingface.co/datasets/GAIR/lima…
------
"LIMA: Less Is More for Alignment" might be a gamechanger for researchers & tinkerers who want to develop capable LLMs. 

The researchers showed that a 65B LLaMA model finetuned on only 1000 examples (in supervised fashion) is not too far behind bigger models like ChatGPT!

1/6
------
Fun fact: LLaMA-Adapter is not LLaMA-specific. You can use it to finetune any LLM.

Below, I finetuned a 40B Falcon model using LLaMA-Adapter (it also works on a single GPU with 20 GB RAM).

Have been heads down tinkering & will follow up with more benchmarks!
------
Finetuning Falcon 40B on the Alpaca instruction dataset takes 30 hours on 8 A100s. Cut it to 30 minutes using LLaMA Adapter 

Try it at https://github.com/Lightning-AI/lit-parrot…

#LLM #MachineLearning
------
"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding".

This method is pretty cool, but what is this new trend where papers don't contain any type of quantitative model evaluation?

Looks like researchers are in a rush.

 https://arxiv.org/abs/2306.02858
------
Saw the discussions regarding Stability AI today, & I am a bit surprised that the paper behind it was allegedly swept under the rug.
I do think it was known that it was based on Latent Diffusion? 
Covered it back in 2022 in an early Ahead of AI issue:

https://magazine.sebastianraschka.com/p/ahead-of-ai-4-a-big-year-for-ai…
------
Oh, and I should have emphasized that you can use our CORN (Conditional Ordinal Regression for Neural Networks) method to finetune your favorite LLM for prediction tasks, or course!

An example for TripAdvisor customer reviews here: https://github.com/Raschka-research-group/coral-pytorch/blob/main/docs/tutorials/pytorch_lightning/distilbert-corn-tripadvisor.ipynb…
------
I often like to share AI research articles I find interesting. Let me change it up a bit and share one of our own articles that just passed peer review &  got accepted.

Our method allows you to use any model (LLM, CNN, ViT, ...) for ordinal regression on ordered labels.

1/6
------
Plus this Falcon implementation features the same readable & hackable code as Lit-LLaMA (https://github.com/Lightning-AI/lit-parrot…)

More analyses will follow soon! (And pls feel free to reach out if you have questions about the current implementation)
------
We couldn't wait to tinker with the best performing opensource LLM (Falcon 40B) 

We integrated it into Lit-Parrot and sped it up by 20%! We'll keep adding optimizations to squeeze out more performance!

Check it out: https://github.com/Lightning-AI/lit-parrot…
------
I often like to share AI research articles I find interesting. Let me change it up a bit and share one of our own articles that just passed peer review &  got accepted.

Our method allows you to use any model (LLM, CNN, ViT, ...) for ordinal regression on ordered labels.

1/6
------
Btw, in code, this is also super easy to implement. It requires just 3 small changes to an existing PyTorch model. 

The corn_loss used here can be imported from the coral-pytorch library (https://github.com/Raschka-research-group/coral-pytorch…)

4/5
------
Susan & I found MMLU performance jump 6-10 points in the 40s by formatting multiple choice as (A) not A in MMLU (for internal model). All evaluation of LLM's are broken. Evaluating a task requires marginalizing across all prompts that describe the task, not point estimate of one.
------
Never insult a scientist by suggesting they're p-hacking benchmarks.   twitter.com/suchenzang/sta…
------
I really like the LLaMA-Adapter method! 
In a nutshell, it's about finetuning LLMs more efficiently. 

In a nutshell, it's adding a tunable prefix to the key and value tensors in the self-attention layers.

Fun fact: it's not specific to LLaMA. Use it with any LLM!
------
Exciting news: LLaMA-Adapter is now fully unlocked! 6

 As a general-purpose #multimodal foundation model, it integrates various inputs like images, audio, text, video, and 3D point clouds, while providing image, text-based, and detection outputs. It uniquely accepts the… Show more
------
Reading  Machine Learning Q and AI from 
@rasbt
. Ended up buying after reading a free chapter on 
@leanpub
. Had such a rich references, that got me hooked. Looking forward to more bedtime reading.
------
Happy birthday, transformer! An awesome summary 
@DrJimFan
!

Also interesting to think about why we needed attention for RNNs (before transformers) in the first place.

Since we can't translate word-by-word, we needed a RNN encoder-decoder setup. But then, it's hard to remember.
------
Today 6 years ago, "Attention is All You Need" went on Arxiv! Happy birthday Transformer! 

Fun facts:
- Transformer did not invent attention, but pushed it to the extreme. The first attention paper was published 3 years prior (2014) and had an unassuming title: "Neural Machine… Show more
------
Today is the day! 
My new book Machine Learning Q and AI is now complete!

 https://leanpub.com/machine-learning-q-and-ai/…

Covering 
- Explanations of multi-GPU training paradigms.
- Using and finetuning transformers.
- Differences between encoder- and decoder-style LLMs.
- And many more!

1/3
------
It took me almost a year and countless weekends, and a lot of discipline (writing 1 chapter almost every weekend). But I loved (almost) every minute of it!

2/3
------
I am really excited with how it turned out. Finally a resource for those who are interested in a bit more advanced machine learning, deep learning, and AI topics.

PS: I also partnered with a publisher to also release print version later this summer. Stay tuned!

3/3
------
Ok, so it's now official: using the official MMLU evaluation, LLaMA 1) indeed matches the score in the original LLaMA paper and 2) has quite the lead on Falcon. Ofc it's just one benchmark, but it shows how much details matter when evaluating LLMs.
------
Guys, I know you want watch toe-to-toe battles. Here you go:

Under official MMLU prompts, default huggingface generate() function, fp16, no fancy prompt engineering, no more complication:

LLaMA v.s Falcon = 63.64 v.s 49.08

Happy? Disappointed? Good? Bad? Win? Lose?

code +… Show more
------
2005: too bad I can’t find any research paper on that concept yet.

2010: There is now a paper that analyzes this concept!

2018: in which order should I best read all these papers on that topic?

2023: which subset of all these papers with contradictory results is worth reading?
------
Paper writing protip:

Most papers are not read end-to-end. Ain't nobody got time. Write with that in mind. Make sections, figs, tables and their captions as self-contained and "guessable" as reasonably possible.

Example: call your models Foo-M and Foo-S instead of Foo and Foo*
------
It's been a another wild month in AI & Deep Learning research. 
I curated and summarized noteworthy papers here: https://magazine.sebastianraschka.com/p/ai-research-highlights-in-3-sentences-2a1/…

Ranging from new optimizers for LLMs to new scaling laws for vision transformers.
------
Just saw that the LIMA dataset is now available! https://huggingface.co/datasets/GAIR/lima…
------
"LIMA: Less Is More for Alignment" might be a gamechanger for researchers & tinkerers who want to develop capable LLMs. 

The researchers showed that a 65B LLaMA model finetuned on only 1000 examples (in supervised fashion) is not too far behind bigger models like ChatGPT!

1/6
------
Fun fact: LLaMA-Adapter is not LLaMA-specific. You can use it to finetune any LLM.

Below, I finetuned a 40B Falcon model using LLaMA-Adapter (it also works on a single GPU with 20 GB RAM).

Have been heads down tinkering & will follow up with more benchmarks!
------
Finetuning Falcon 40B on the Alpaca instruction dataset takes 30 hours on 8 A100s. Cut it to 30 minutes using LLaMA Adapter 

Try it at https://github.com/Lightning-AI/lit-parrot…

#LLM #MachineLearning
------
"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding".

This method is pretty cool, but what is this new trend where papers don't contain any type of quantitative model evaluation?

Looks like researchers are in a rush.

 https://arxiv.org/abs/2306.02858
------
Saw the discussions regarding Stability AI today, & I am a bit surprised that the paper behind it was allegedly swept under the rug.
I do think it was known that it was based on Latent Diffusion? 
Covered it back in 2022 in an early Ahead of AI issue:

https://magazine.sebastianraschka.com/p/ahead-of-ai-4-a-big-year-for-ai…
------
Oh, and I should have emphasized that you can use our CORN (Conditional Ordinal Regression for Neural Networks) method to finetune your favorite LLM for prediction tasks, or course!

An example for TripAdvisor customer reviews here: https://github.com/Raschka-research-group/coral-pytorch/blob/main/docs/tutorials/pytorch_lightning/distilbert-corn-tripadvisor.ipynb…
------
I often like to share AI research articles I find interesting. Let me change it up a bit and share one of our own articles that just passed peer review &  got accepted.

Our method allows you to use any model (LLM, CNN, ViT, ...) for ordinal regression on ordered labels.

1/6
------
Plus this Falcon implementation features the same readable & hackable code as Lit-LLaMA (https://github.com/Lightning-AI/lit-parrot…)

More analyses will follow soon! (And pls feel free to reach out if you have questions about the current implementation)
------
We couldn't wait to tinker with the best performing opensource LLM (Falcon 40B) 

We integrated it into Lit-Parrot and sped it up by 20%! We'll keep adding optimizations to squeeze out more performance!

Check it out: https://github.com/Lightning-AI/lit-parrot…
------
I often like to share AI research articles I find interesting. Let me change it up a bit and share one of our own articles that just passed peer review &  got accepted.

Our method allows you to use any model (LLM, CNN, ViT, ...) for ordinal regression on ordered labels.

1/6
------
Btw, in code, this is also super easy to implement. It requires just 3 small changes to an existing PyTorch model. 

The corn_loss used here can be imported from the coral-pytorch library (https://github.com/Raschka-research-group/coral-pytorch…)

4/5
------
For more details, check out the full paper at https://arxiv.org/abs/2111.08851

5/5
------
A new Ahead of AI issue is out, where I am covering the latest research highlights concerning LLM tuning and dataset efficiency:
------
By the way, if you are looking for research directions, I highlighted a few interesting things and open questions in bold:
------
One of the reasons why I joined 
@kaggle
 in the first place was to move away from subjective based peer reviews and participate in competitive competitions with objective metrics without human intervention.

Now Kaggle is hosting a competition where you have to write a research… Show more
------
Today is the day! 
My new book Machine Learning Q and AI is now complete!

 https://leanpub.com/machine-learning-q-and-ai/…

Covering 
- Explanations of multi-GPU training paradigms.
- Using and finetuning transformers.
- Differences between encoder- and decoder-style LLMs.
- And many more!

1/3
------
It took me almost a year and countless weekends, and a lot of discipline (writing 1 chapter almost every weekend). But I loved (almost) every minute of it!

2/3
------
I am really excited with how it turned out. Finally a resource for those who are interested in a bit more advanced machine learning, deep learning, and AI topics.

PS: I also partnered with a publisher to also release print version later this summer. Stay tuned!

3/3
------
Ok, so it's now official: using the official MMLU evaluation, LLaMA 1) indeed matches the score in the original LLaMA paper and 2) has quite the lead on Falcon. Ofc it's just one benchmark, but it shows how much details matter when evaluating LLMs.
------
Guys, I know you want watch toe-to-toe battles. Here you go:

Under official MMLU prompts, default huggingface generate() function, fp16, no fancy prompt engineering, no more complication:

LLaMA v.s Falcon = 63.64 v.s 49.08

Happy? Disappointed? Good? Bad? Win? Lose?

code +… Show more
------
2005: too bad I can’t find any research paper on that concept yet.

2010: There is now a paper that analyzes this concept!

2018: in which order should I best read all these papers on that topic?

2023: which subset of all these papers with contradictory results is worth reading?
------
Paper writing protip:

Most papers are not read end-to-end. Ain't nobody got time. Write with that in mind. Make sections, figs, tables and their captions as self-contained and "guessable" as reasonably possible.

Example: call your models Foo-M and Foo-S instead of Foo and Foo*
------
It's been a another wild month in AI & Deep Learning research. 
I curated and summarized noteworthy papers here: https://magazine.sebastianraschka.com/p/ai-research-highlights-in-3-sentences-2a1/…

Ranging from new optimizers for LLMs to new scaling laws for vision transformers.
------
Just saw that the LIMA dataset is now available! https://huggingface.co/datasets/GAIR/lima…
------
"LIMA: Less Is More for Alignment" might be a gamechanger for researchers & tinkerers who want to develop capable LLMs. 

The researchers showed that a 65B LLaMA model finetuned on only 1000 examples (in supervised fashion) is not too far behind bigger models like ChatGPT!

1/6
------
Fun fact: LLaMA-Adapter is not LLaMA-specific. You can use it to finetune any LLM.

Below, I finetuned a 40B Falcon model using LLaMA-Adapter (it also works on a single GPU with 20 GB RAM).

Have been heads down tinkering & will follow up with more benchmarks!
------
Finetuning Falcon 40B on the Alpaca instruction dataset takes 30 hours on 8 A100s. Cut it to 30 minutes using LLaMA Adapter 

Try it at https://github.com/Lightning-AI/lit-parrot…

#LLM #MachineLearning
------
"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding".

This method is pretty cool, but what is this new trend where papers don't contain any type of quantitative model evaluation?

Looks like researchers are in a rush.

 https://arxiv.org/abs/2306.02858
------
Saw the discussions regarding Stability AI today, & I am a bit surprised that the paper behind it was allegedly swept under the rug.
I do think it was known that it was based on Latent Diffusion? 
Covered it back in 2022 in an early Ahead of AI issue:

https://magazine.sebastianraschka.com/p/ahead-of-ai-4-a-big-year-for-ai…
------
Oh, and I should have emphasized that you can use our CORN (Conditional Ordinal Regression for Neural Networks) method to finetune your favorite LLM for prediction tasks, or course!

An example for TripAdvisor customer reviews here: https://github.com/Raschka-research-group/coral-pytorch/blob/main/docs/tutorials/pytorch_lightning/distilbert-corn-tripadvisor.ipynb…
------
I often like to share AI research articles I find interesting. Let me change it up a bit and share one of our own articles that just passed peer review &  got accepted.

Our method allows you to use any model (LLM, CNN, ViT, ...) for ordinal regression on ordered labels.

1/6
------
Plus this Falcon implementation features the same readable & hackable code as Lit-LLaMA (https://github.com/Lightning-AI/lit-parrot…)

More analyses will follow soon! (And pls feel free to reach out if you have questions about the current implementation)
------
We couldn't wait to tinker with the best performing opensource LLM (Falcon 40B) 

We integrated it into Lit-Parrot and sped it up by 20%! We'll keep adding optimizations to squeeze out more performance!

Check it out: https://github.com/Lightning-AI/lit-parrot…
------
I often like to share AI research articles I find interesting. Let me change it up a bit and share one of our own articles that just passed peer review &  got accepted.

Our method allows you to use any model (LLM, CNN, ViT, ...) for ordinal regression on ordered labels.

1/6
------
Btw, in code, this is also super easy to implement. It requires just 3 small changes to an existing PyTorch model. 

The corn_loss used here can be imported from the coral-pytorch library (https://github.com/Raschka-research-group/coral-pytorch…)

4/5
------
For more details, check out the full paper at https://arxiv.org/abs/2111.08851

5/5
------
A new Ahead of AI issue is out, where I am covering the latest research highlights concerning LLM tuning and dataset efficiency:
------
By the way, if you are looking for research directions, I highlighted a few interesting things and open questions in bold:
------
One of the reasons why I joined 
@kaggle
 in the first place was to move away from subjective based peer reviews and participate in competitive competitions with objective metrics without human intervention.

Now Kaggle is hosting a competition where you have to write a research… Show more
------
> "The unfortunate thing is that the scientists as well as the policymakers, the people who are making decisions or creating these advances, are only being either positively or negatively excited by such advances, not being critical about it."

Well said!
------
More AI "doomer" pushback. Thanks to @kchonyc for talking to me for this one. https://venturebeat.com/ai/top-ai-researcher-dismisses-ai-extinction-fears-challenges-hero-scientist-narrative/…
------
Bored by decoder-only transformers? Let’s revisit encoder-style BERT models! 
Traditionally, BERT LLMs were pretrained with a 15% masking rate.  
Turns out varying the masking rate between 15-30% during pretraining improves accuracy & time to convergence: https://arxiv.org/abs/2305.15096
------
*Yes, decoder-only is a misnomer  (as I discuss in my Machine Learning Q and AI book)
------
Stop using positional encoding (PE) in Transformer decoders (e.g. GPTs). Our work shows 𝗡𝗼𝗣𝗘 (no positional encoding) outperforms all variants like absolute, relative, ALiBi, Rotary. A decoder can learn PE in its representation (see proof). Time for 𝗡𝗼𝗣𝗘 𝗟𝗟𝗠𝘀[1/n]
------
Excited to share Unit 10, the finale of my latest course!  

https://lightning.ai/pages/courses/deep-learning-fundamentals/10.0-overview-the-finale-our-next-steps-after-ai-model-training/…

My aim is to provide a modern take on learning deep learning & AI using open-source libraries. 
And I hope you enjoyed the journey from backpropagation to implementing LLMs in PyTorch!
------
My goal with this course was to provide a modern take on learning deep learning & AI using open-source libraries. Moreover, I tried to be concise but also go beyond the typical topics covered in university classes

1/2
------
And what was also particularly important to me was to also include important topics that are super relevant in practice: learning rate schedulers, mixed-precision training, distributed multi-GPU training strategies, and more!

2/2
------
It's been a another wild month in AI & Deep Learning research. 
I curated and summarized noteworthy papers here: https://magazine.sebastianraschka.com/p/ai-research-highlights-in-3-sentences-2a1/…

Ranging from new optimizers for LLMs to new scaling laws for vision transformers.
------
Just saw that the LIMA dataset is now available! https://huggingface.co/datasets/GAIR/lima…
------
"LIMA: Less Is More for Alignment" might be a gamechanger for researchers & tinkerers who want to develop capable LLMs. 

The researchers showed that a 65B LLaMA model finetuned on only 1000 examples (in supervised fashion) is not too far behind bigger models like ChatGPT!

1/6
------
Fun fact: LLaMA-Adapter is not LLaMA-specific. You can use it to finetune any LLM.

Below, I finetuned a 40B Falcon model using LLaMA-Adapter (it also works on a single GPU with 20 GB RAM).

Have been heads down tinkering & will follow up with more benchmarks!
------
Finetuning Falcon 40B on the Alpaca instruction dataset takes 30 hours on 8 A100s. Cut it to 30 minutes using LLaMA Adapter 

Try it at https://github.com/Lightning-AI/lit-parrot…

#LLM #MachineLearning
------
"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding".

This method is pretty cool, but what is this new trend where papers don't contain any type of quantitative model evaluation?

Looks like researchers are in a rush.

 https://arxiv.org/abs/2306.02858
------
Saw the discussions regarding Stability AI today, & I am a bit surprised that the paper behind it was allegedly swept under the rug.
I do think it was known that it was based on Latent Diffusion? 
Covered it back in 2022 in an early Ahead of AI issue:

https://magazine.sebastianraschka.com/p/ahead-of-ai-4-a-big-year-for-ai…
------
Oh, and I should have emphasized that you can use our CORN (Conditional Ordinal Regression for Neural Networks) method to finetune your favorite LLM for prediction tasks, or course!

An example for TripAdvisor customer reviews here: https://github.com/Raschka-research-group/coral-pytorch/blob/main/docs/tutorials/pytorch_lightning/distilbert-corn-tripadvisor.ipynb…
------
I often like to share AI research articles I find interesting. Let me change it up a bit and share one of our own articles that just passed peer review &  got accepted.

Our method allows you to use any model (LLM, CNN, ViT, ...) for ordinal regression on ordered labels.

1/6
------
Plus this Falcon implementation features the same readable & hackable code as Lit-LLaMA (https://github.com/Lightning-AI/lit-parrot…)

More analyses will follow soon! (And pls feel free to reach out if you have questions about the current implementation)
------
We couldn't wait to tinker with the best performing opensource LLM (Falcon 40B) 

We integrated it into Lit-Parrot and sped it up by 20%! We'll keep adding optimizations to squeeze out more performance!

Check it out: https://github.com/Lightning-AI/lit-parrot…
------
I often like to share AI research articles I find interesting. Let me change it up a bit and share one of our own articles that just passed peer review &  got accepted.

Our method allows you to use any model (LLM, CNN, ViT, ...) for ordinal regression on ordered labels.

1/6
------
Btw, in code, this is also super easy to implement. It requires just 3 small changes to an existing PyTorch model. 

The corn_loss used here can be imported from the coral-pytorch library (https://github.com/Raschka-research-group/coral-pytorch…)

4/5
------
For more details, check out the full paper at https://arxiv.org/abs/2111.08851

5/5
------
A new Ahead of AI issue is out, where I am covering the latest research highlights concerning LLM tuning and dataset efficiency:
------
By the way, if you are looking for research directions, I highlighted a few interesting things and open questions in bold:
------
One of the reasons why I joined 
@kaggle
 in the first place was to move away from subjective based peer reviews and participate in competitive competitions with objective metrics without human intervention.

Now Kaggle is hosting a competition where you have to write a research… Show more
------
> "The unfortunate thing is that the scientists as well as the policymakers, the people who are making decisions or creating these advances, are only being either positively or negatively excited by such advances, not being critical about it."

Well said!
------
More AI "doomer" pushback. Thanks to @kchonyc for talking to me for this one. https://venturebeat.com/ai/top-ai-researcher-dismisses-ai-extinction-fears-challenges-hero-scientist-narrative/…
------
Bored by decoder-only transformers? Let’s revisit encoder-style BERT models! 
Traditionally, BERT LLMs were pretrained with a 15% masking rate.  
Turns out varying the masking rate between 15-30% during pretraining improves accuracy & time to convergence: https://arxiv.org/abs/2305.15096
------
*Yes, decoder-only is a misnomer  (as I discuss in my Machine Learning Q and AI book)
------
Stop using positional encoding (PE) in Transformer decoders (e.g. GPTs). Our work shows 𝗡𝗼𝗣𝗘 (no positional encoding) outperforms all variants like absolute, relative, ALiBi, Rotary. A decoder can learn PE in its representation (see proof). Time for 𝗡𝗼𝗣𝗘 𝗟𝗟𝗠𝘀[1/n]
------
Excited to share Unit 10, the finale of my latest course!  

https://lightning.ai/pages/courses/deep-learning-fundamentals/10.0-overview-the-finale-our-next-steps-after-ai-model-training/…

My aim is to provide a modern take on learning deep learning & AI using open-source libraries. 
And I hope you enjoyed the journey from backpropagation to implementing LLMs in PyTorch!
------
My goal with this course was to provide a modern take on learning deep learning & AI using open-source libraries. Moreover, I tried to be concise but also go beyond the typical topics covered in university classes

1/2
------
And what was also particularly important to me was to also include important topics that are super relevant in practice: learning rate schedulers, mixed-precision training, distributed multi-GPU training strategies, and more!

2/2
------
Was just reading about the latest Python packaging recommendations in 2023 (via https://drivendata.co/blog/python-packaging-2023…). 

Takeway: `setup. py` & `setup.cfg` are now deprecated in favor of a single `pyproject.toml` file

Has anyone done the "conversion" yet and? Any experiences, and/or gotchas?
------
I just heard the news that Falcon dropped the revenue reporting & sharing requirements, converting the license into a regular Apache 2.0 License. That’s great!
------
The new Falcon LLM, beating LLaMA in benchmarks, sounds exciting. Looking forward to reading the paper in detail. 

But a BIG WARNING if you want to use it for anything commercial: the license requires paying 10% of the revenue. Which requires disclosing financial company info !?  twitter.com/omarsar0/statu…
------
And that would be finetuning an LLM on a MacBook under an hour on the 1000-example "LIMA: Less Is More for Alignment" dataset (https://arxiv.org/abs/2305.11206). 

(PS: Eagerly waiting for the authors to share it :))
------
Thanks to our community you can now fine-tune LLMs on your laptop 

Lit-Parrot now supports fine-tuning on Apple’s metal accelerator (MPS) 

Fine-tune a large language model with Alpaca within 40 hours on your laptop now https://lightning.ai/pages/blog/how-to-finetune-gpt-like-large-language-models-on-a-custom-dataset/…

Kudos to agmo1993 for this… Show more
------
A new alternative to RLHF just dropped!
In Direct Preference Optimization: Your Language Model is Secretly a Reward Model (https://arxiv.org/abs/2305.18290), the researchers show that the cross-entropy loss for fitting the reward model in RLHF can be used directly to finetune the LLM.
------
Saw the discussions regarding Stability AI today, & I am a bit surprised that the paper behind it was allegedly swept under the rug.
I do think it was known that it was based on Latent Diffusion? 
Covered it back in 2022 in an early Ahead of AI issue:

https://magazine.sebastianraschka.com/p/ahead-of-ai-4-a-big-year-for-ai…
------
Oh, and I should have emphasized that you can use our CORN (Conditional Ordinal Regression for Neural Networks) method to finetune your favorite LLM for prediction tasks, or course!

An example for TripAdvisor customer reviews here: https://github.com/Raschka-research-group/coral-pytorch/blob/main/docs/tutorials/pytorch_lightning/distilbert-corn-tripadvisor.ipynb…
------
I often like to share AI research articles I find interesting. Let me change it up a bit and share one of our own articles that just passed peer review &  got accepted.

Our method allows you to use any model (LLM, CNN, ViT, ...) for ordinal regression on ordered labels.

1/6
------
Plus this Falcon implementation features the same readable & hackable code as Lit-LLaMA (https://github.com/Lightning-AI/lit-parrot…)

More analyses will follow soon! (And pls feel free to reach out if you have questions about the current implementation)
------
We couldn't wait to tinker with the best performing opensource LLM (Falcon 40B) 

We integrated it into Lit-Parrot and sped it up by 20%! We'll keep adding optimizations to squeeze out more performance!

Check it out: https://github.com/Lightning-AI/lit-parrot…
------
I often like to share AI research articles I find interesting. Let me change it up a bit and share one of our own articles that just passed peer review &  got accepted.

Our method allows you to use any model (LLM, CNN, ViT, ...) for ordinal regression on ordered labels.

1/6
------
Btw, in code, this is also super easy to implement. It requires just 3 small changes to an existing PyTorch model. 

The corn_loss used here can be imported from the coral-pytorch library (https://github.com/Raschka-research-group/coral-pytorch…)

4/5
------
For more details, check out the full paper at https://arxiv.org/abs/2111.08851

5/5
------
A new Ahead of AI issue is out, where I am covering the latest research highlights concerning LLM tuning and dataset efficiency:
------
By the way, if you are looking for research directions, I highlighted a few interesting things and open questions in bold:
------
One of the reasons why I joined 
@kaggle
 in the first place was to move away from subjective based peer reviews and participate in competitive competitions with objective metrics without human intervention.

Now Kaggle is hosting a competition where you have to write a research… Show more
------
> "The unfortunate thing is that the scientists as well as the policymakers, the people who are making decisions or creating these advances, are only being either positively or negatively excited by such advances, not being critical about it."

Well said!
------
More AI "doomer" pushback. Thanks to @kchonyc for talking to me for this one. https://venturebeat.com/ai/top-ai-researcher-dismisses-ai-extinction-fears-challenges-hero-scientist-narrative/…
------
Bored by decoder-only transformers? Let’s revisit encoder-style BERT models! 
Traditionally, BERT LLMs were pretrained with a 15% masking rate.  
Turns out varying the masking rate between 15-30% during pretraining improves accuracy & time to convergence: https://arxiv.org/abs/2305.15096
------
*Yes, decoder-only is a misnomer  (as I discuss in my Machine Learning Q and AI book)
------
Stop using positional encoding (PE) in Transformer decoders (e.g. GPTs). Our work shows 𝗡𝗼𝗣𝗘 (no positional encoding) outperforms all variants like absolute, relative, ALiBi, Rotary. A decoder can learn PE in its representation (see proof). Time for 𝗡𝗼𝗣𝗘 𝗟𝗟𝗠𝘀[1/n]
------
Excited to share Unit 10, the finale of my latest course!  

https://lightning.ai/pages/courses/deep-learning-fundamentals/10.0-overview-the-finale-our-next-steps-after-ai-model-training/…

My aim is to provide a modern take on learning deep learning & AI using open-source libraries. 
And I hope you enjoyed the journey from backpropagation to implementing LLMs in PyTorch!
------
My goal with this course was to provide a modern take on learning deep learning & AI using open-source libraries. Moreover, I tried to be concise but also go beyond the typical topics covered in university classes

1/2
------
And what was also particularly important to me was to also include important topics that are super relevant in practice: learning rate schedulers, mixed-precision training, distributed multi-GPU training strategies, and more!

2/2
------
Was just reading about the latest Python packaging recommendations in 2023 (via https://drivendata.co/blog/python-packaging-2023…). 

Takeway: `setup. py` & `setup.cfg` are now deprecated in favor of a single `pyproject.toml` file

Has anyone done the "conversion" yet and? Any experiences, and/or gotchas?
------
I just heard the news that Falcon dropped the revenue reporting & sharing requirements, converting the license into a regular Apache 2.0 License. That’s great!
------
The new Falcon LLM, beating LLaMA in benchmarks, sounds exciting. Looking forward to reading the paper in detail. 

But a BIG WARNING if you want to use it for anything commercial: the license requires paying 10% of the revenue. Which requires disclosing financial company info !?  twitter.com/omarsar0/statu…
------
And that would be finetuning an LLM on a MacBook under an hour on the 1000-example "LIMA: Less Is More for Alignment" dataset (https://arxiv.org/abs/2305.11206). 

(PS: Eagerly waiting for the authors to share it :))
------
Thanks to our community you can now fine-tune LLMs on your laptop 

Lit-Parrot now supports fine-tuning on Apple’s metal accelerator (MPS) 

Fine-tune a large language model with Alpaca within 40 hours on your laptop now https://lightning.ai/pages/blog/how-to-finetune-gpt-like-large-language-models-on-a-custom-dataset/…

Kudos to agmo1993 for this… Show more
------
A new alternative to RLHF just dropped!
In Direct Preference Optimization: Your Language Model is Secretly a Reward Model (https://arxiv.org/abs/2305.18290), the researchers show that the cross-entropy loss for fitting the reward model in RLHF can be used directly to finetune the LLM.
------
We did a small ablations in SantaCoder (StarCoder's little cousin): https://arxiv.org/abs/2301.03988

TLDR: it hurts a bit (given everything else is the same) but if you need to generate long sequences you save a ton of GPU memory in the KV cache and don't want to ever go back
------
What happens if we train LLMs for multiple epochs?

The question I asked multiple times in the past finally got answered in this new preprint,
"To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis".

1/6
------
Caveat, the ablation study is missing two crucial entries!

5/6
------
However, the takeaway is that the key question was answered: LLMs are susceptible to overfitting when repeating the training data for multiple epochs, degrading the task performance.

Link to the paper: https://arxiv.org/abs/2305.13230

6/6
------
Most recent large transformer decoders use this trick of having multiple heads for the queries, but only one for the keys/values.

I always thought it’s a small not well documented trick of the trade. But no, there’s a nice paper about „multi query attention“, of course by Noam.
------
When all you need is one write-head, not a co-author or many citations
------
Plus this Falcon implementation features the same readable & hackable code as Lit-LLaMA (https://github.com/Lightning-AI/lit-parrot…)

More analyses will follow soon! (And pls feel free to reach out if you have questions about the current implementation)
------
We couldn't wait to tinker with the best performing opensource LLM (Falcon 40B) 

We integrated it into Lit-Parrot and sped it up by 20%! We'll keep adding optimizations to squeeze out more performance!

Check it out: https://github.com/Lightning-AI/lit-parrot…
------
I often like to share AI research articles I find interesting. Let me change it up a bit and share one of our own articles that just passed peer review &  got accepted.

Our method allows you to use any model (LLM, CNN, ViT, ...) for ordinal regression on ordered labels.

1/6
------
Btw, in code, this is also super easy to implement. It requires just 3 small changes to an existing PyTorch model. 

The corn_loss used here can be imported from the coral-pytorch library (https://github.com/Raschka-research-group/coral-pytorch…)

4/5
------
For more details, check out the full paper at https://arxiv.org/abs/2111.08851

5/5
------
A new Ahead of AI issue is out, where I am covering the latest research highlights concerning LLM tuning and dataset efficiency:
------
By the way, if you are looking for research directions, I highlighted a few interesting things and open questions in bold:
------
One of the reasons why I joined 
@kaggle
 in the first place was to move away from subjective based peer reviews and participate in competitive competitions with objective metrics without human intervention.

Now Kaggle is hosting a competition where you have to write a research… Show more
------
> "The unfortunate thing is that the scientists as well as the policymakers, the people who are making decisions or creating these advances, are only being either positively or negatively excited by such advances, not being critical about it."

Well said!
------
More AI "doomer" pushback. Thanks to @kchonyc for talking to me for this one. https://venturebeat.com/ai/top-ai-researcher-dismisses-ai-extinction-fears-challenges-hero-scientist-narrative/…
------
Bored by decoder-only transformers? Let’s revisit encoder-style BERT models! 
Traditionally, BERT LLMs were pretrained with a 15% masking rate.  
Turns out varying the masking rate between 15-30% during pretraining improves accuracy & time to convergence: https://arxiv.org/abs/2305.15096
------
*Yes, decoder-only is a misnomer  (as I discuss in my Machine Learning Q and AI book)
------
Stop using positional encoding (PE) in Transformer decoders (e.g. GPTs). Our work shows 𝗡𝗼𝗣𝗘 (no positional encoding) outperforms all variants like absolute, relative, ALiBi, Rotary. A decoder can learn PE in its representation (see proof). Time for 𝗡𝗼𝗣𝗘 𝗟𝗟𝗠𝘀[1/n]
------
Excited to share Unit 10, the finale of my latest course!  

https://lightning.ai/pages/courses/deep-learning-fundamentals/10.0-overview-the-finale-our-next-steps-after-ai-model-training/…

My aim is to provide a modern take on learning deep learning & AI using open-source libraries. 
And I hope you enjoyed the journey from backpropagation to implementing LLMs in PyTorch!
------
My goal with this course was to provide a modern take on learning deep learning & AI using open-source libraries. Moreover, I tried to be concise but also go beyond the typical topics covered in university classes

1/2
------
And what was also particularly important to me was to also include important topics that are super relevant in practice: learning rate schedulers, mixed-precision training, distributed multi-GPU training strategies, and more!

2/2
------
Was just reading about the latest Python packaging recommendations in 2023 (via https://drivendata.co/blog/python-packaging-2023…). 

Takeway: `setup. py` & `setup.cfg` are now deprecated in favor of a single `pyproject.toml` file

Has anyone done the "conversion" yet and? Any experiences, and/or gotchas?
------
I just heard the news that Falcon dropped the revenue reporting & sharing requirements, converting the license into a regular Apache 2.0 License. That’s great!
------
The new Falcon LLM, beating LLaMA in benchmarks, sounds exciting. Looking forward to reading the paper in detail. 

But a BIG WARNING if you want to use it for anything commercial: the license requires paying 10% of the revenue. Which requires disclosing financial company info !?  twitter.com/omarsar0/statu…
------
And that would be finetuning an LLM on a MacBook under an hour on the 1000-example "LIMA: Less Is More for Alignment" dataset (https://arxiv.org/abs/2305.11206). 

(PS: Eagerly waiting for the authors to share it :))
------
Thanks to our community you can now fine-tune LLMs on your laptop 

Lit-Parrot now supports fine-tuning on Apple’s metal accelerator (MPS) 

Fine-tune a large language model with Alpaca within 40 hours on your laptop now https://lightning.ai/pages/blog/how-to-finetune-gpt-like-large-language-models-on-a-custom-dataset/…

Kudos to agmo1993 for this… Show more
------
A new alternative to RLHF just dropped!
In Direct Preference Optimization: Your Language Model is Secretly a Reward Model (https://arxiv.org/abs/2305.18290), the researchers show that the cross-entropy loss for fitting the reward model in RLHF can be used directly to finetune the LLM.
------
We did a small ablations in SantaCoder (StarCoder's little cousin): https://arxiv.org/abs/2301.03988

TLDR: it hurts a bit (given everything else is the same) but if you need to generate long sequences you save a ton of GPU memory in the KV cache and don't want to ever go back
------
What happens if we train LLMs for multiple epochs?

The question I asked multiple times in the past finally got answered in this new preprint,
"To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis".

1/6
------
Caveat, the ablation study is missing two crucial entries!

5/6
------
However, the takeaway is that the key question was answered: LLMs are susceptible to overfitting when repeating the training data for multiple epochs, degrading the task performance.

Link to the paper: https://arxiv.org/abs/2305.13230

6/6
------
Most recent large transformer decoders use this trick of having multiple heads for the queries, but only one for the keys/values.

I always thought it’s a small not well documented trick of the trade. But no, there’s a nice paper about „multi query attention“, of course by Noam.
------
When all you need is one write-head, not a co-author or many citations
------
If you are using `watermark` for your Python scripts or Jupyter notebooks: 
Watermark version 2.4 now has a new `--gpu` flag to display available GPU hardware (requires cuda).

Via a kind contribution: https://github.com/rasbt/watermark/pull/90…
------
A new Ahead of AI issue is out, where I am covering the latest research highlights concerning LLM tuning and dataset efficiency:
------
By the way, if you are looking for research directions, I highlighted a few interesting things and open questions in bold:
------
One of the reasons why I joined 
@kaggle
 in the first place was to move away from subjective based peer reviews and participate in competitive competitions with objective metrics without human intervention.

Now Kaggle is hosting a competition where you have to write a research… Show more
------
> "The unfortunate thing is that the scientists as well as the policymakers, the people who are making decisions or creating these advances, are only being either positively or negatively excited by such advances, not being critical about it."

Well said!
------
More AI "doomer" pushback. Thanks to @kchonyc for talking to me for this one. https://venturebeat.com/ai/top-ai-researcher-dismisses-ai-extinction-fears-challenges-hero-scientist-narrative/…
------
Bored by decoder-only transformers? Let’s revisit encoder-style BERT models! 
Traditionally, BERT LLMs were pretrained with a 15% masking rate.  
Turns out varying the masking rate between 15-30% during pretraining improves accuracy & time to convergence: https://arxiv.org/abs/2305.15096
------
*Yes, decoder-only is a misnomer  (as I discuss in my Machine Learning Q and AI book)
------
Stop using positional encoding (PE) in Transformer decoders (e.g. GPTs). Our work shows 𝗡𝗼𝗣𝗘 (no positional encoding) outperforms all variants like absolute, relative, ALiBi, Rotary. A decoder can learn PE in its representation (see proof). Time for 𝗡𝗼𝗣𝗘 𝗟𝗟𝗠𝘀[1/n]
------
Excited to share Unit 10, the finale of my latest course!  

https://lightning.ai/pages/courses/deep-learning-fundamentals/10.0-overview-the-finale-our-next-steps-after-ai-model-training/…

My aim is to provide a modern take on learning deep learning & AI using open-source libraries. 
And I hope you enjoyed the journey from backpropagation to implementing LLMs in PyTorch!
------
My goal with this course was to provide a modern take on learning deep learning & AI using open-source libraries. Moreover, I tried to be concise but also go beyond the typical topics covered in university classes

1/2
------
And what was also particularly important to me was to also include important topics that are super relevant in practice: learning rate schedulers, mixed-precision training, distributed multi-GPU training strategies, and more!

2/2
------
Was just reading about the latest Python packaging recommendations in 2023 (via https://drivendata.co/blog/python-packaging-2023…). 

Takeway: `setup. py` & `setup.cfg` are now deprecated in favor of a single `pyproject.toml` file

Has anyone done the "conversion" yet and? Any experiences, and/or gotchas?
------
I just heard the news that Falcon dropped the revenue reporting & sharing requirements, converting the license into a regular Apache 2.0 License. That’s great!
------
The new Falcon LLM, beating LLaMA in benchmarks, sounds exciting. Looking forward to reading the paper in detail. 

But a BIG WARNING if you want to use it for anything commercial: the license requires paying 10% of the revenue. Which requires disclosing financial company info !?  twitter.com/omarsar0/statu…
------
And that would be finetuning an LLM on a MacBook under an hour on the 1000-example "LIMA: Less Is More for Alignment" dataset (https://arxiv.org/abs/2305.11206). 

(PS: Eagerly waiting for the authors to share it :))
------
Thanks to our community you can now fine-tune LLMs on your laptop 

Lit-Parrot now supports fine-tuning on Apple’s metal accelerator (MPS) 

Fine-tune a large language model with Alpaca within 40 hours on your laptop now https://lightning.ai/pages/blog/how-to-finetune-gpt-like-large-language-models-on-a-custom-dataset/…

Kudos to agmo1993 for this… Show more
------
A new alternative to RLHF just dropped!
In Direct Preference Optimization: Your Language Model is Secretly a Reward Model (https://arxiv.org/abs/2305.18290), the researchers show that the cross-entropy loss for fitting the reward model in RLHF can be used directly to finetune the LLM.
------
We did a small ablations in SantaCoder (StarCoder's little cousin): https://arxiv.org/abs/2301.03988

TLDR: it hurts a bit (given everything else is the same) but if you need to generate long sequences you save a ton of GPU memory in the KV cache and don't want to ever go back
------
What happens if we train LLMs for multiple epochs?

The question I asked multiple times in the past finally got answered in this new preprint,
"To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis".

1/6
------
Caveat, the ablation study is missing two crucial entries!

5/6
------
However, the takeaway is that the key question was answered: LLMs are susceptible to overfitting when repeating the training data for multiple epochs, degrading the task performance.

Link to the paper: https://arxiv.org/abs/2305.13230

6/6
------
Most recent large transformer decoders use this trick of having multiple heads for the queries, but only one for the keys/values.

I always thought it’s a small not well documented trick of the trade. But no, there’s a nice paper about „multi query attention“, of course by Noam.
------
When all you need is one write-head, not a co-author or many citations
------
If you are using `watermark` for your Python scripts or Jupyter notebooks: 
Watermark version 2.4 now has a new `--gpu` flag to display available GPU hardware (requires cuda).

Via a kind contribution: https://github.com/rasbt/watermark/pull/90…
------
Just looking at the most impactful LLM literature for this month, I am once more impressed by the importance of LLaMA for research: LIMA, Goat, QLoRA, ...

Clearly, LLaMA is the most valuable LLM model for innovation this year, and it underlines the importance of open source.
------
Whoa, another round number ... 

It's very motivating to know that so many people find my writings useful! 

https://magazine.sebastianraschka.com

(Stay tuned for the next issue, which will not only cover the latest LLM research but also transformers for computer vision for a change!)
------
Whoa, just crossed the 200k follower mark here on Twitter !

So grateful that you all find value in my content.
And I am looking forward to writing and sharing about AI research for many years to come!   

PS: Also pls feel free to ask me anything if you like!
------
"LIMA: Less Is More for Alignment" might be a gamechanger for researchers & tinkerers who want to develop capable LLMs. 

The researchers showed that a 65B LLaMA model finetuned on only 1000 examples (in supervised fashion) is not too far behind bigger models like ChatGPT!

1/6
------
This is an interesting paper. One baseline and ablation study I am really missing here though is how LIMA compared to a 65B LLaMA base model finetuned with RLHF instead of supervised learning. 

5/6
------
If you want to read more, here's a link to the paper: https://arxiv.org/abs/2305.11206

Unfortunately, a code repository does not seem to exist yet.

6/6
------
The new Falcon LLM, beating LLaMA in benchmarks, sounds exciting. Looking forward to reading the paper in detail. 

But a BIG WARNING if you want to use it for anything commercial: the license requires paying 10% of the revenue. Which requires disclosing financial company info !?
------
Introducing Falcon-40B. A new language model trained on 1000B tokens.

What's included:

- 7B and 40B models made available by TII
- surpasses LLaMA 65B and other models like MPT and RedPajama on the Open LLM Leaderboard
- architecture is optimized for inference, with… Show more
------
Thanks 
@unsorsodicorda
 for bringing this to attention
------
About the motivation behind self-attention!

A few short videos  for a long weekend 
https://lightning.ai/pages/courses/deep-learning-fundamentals/unit-8.0-natural-language-processing-and-large-language-models/8.4-from-rnns-to-the-transformer-architecture/…
------
LLMs are now almost everywhere

But did you ever wonder what motivated creating the transformer architecture and where the attention mechanism comes from? 

Check out @rasbt's 3 short (< 5 min) videos to learn about the evolution from RNNs to transformers… Show more
------
*Yes, decoder-only is a misnomer  (as I discuss in my Machine Learning Q and AI book)
------
Stop using positional encoding (PE) in Transformer decoders (e.g. GPTs). Our work shows 𝗡𝗼𝗣𝗘 (no positional encoding) outperforms all variants like absolute, relative, ALiBi, Rotary. A decoder can learn PE in its representation (see proof). Time for 𝗡𝗼𝗣𝗘 𝗟𝗟𝗠𝘀[1/n]
------
Excited to share Unit 10, the finale of my latest course!  

https://lightning.ai/pages/courses/deep-learning-fundamentals/10.0-overview-the-finale-our-next-steps-after-ai-model-training/…

My aim is to provide a modern take on learning deep learning & AI using open-source libraries. 
And I hope you enjoyed the journey from backpropagation to implementing LLMs in PyTorch!
------
My goal with this course was to provide a modern take on learning deep learning & AI using open-source libraries. Moreover, I tried to be concise but also go beyond the typical topics covered in university classes

1/2
------
And what was also particularly important to me was to also include important topics that are super relevant in practice: learning rate schedulers, mixed-precision training, distributed multi-GPU training strategies, and more!

2/2
------
Was just reading about the latest Python packaging recommendations in 2023 (via https://drivendata.co/blog/python-packaging-2023…). 

Takeway: `setup. py` & `setup.cfg` are now deprecated in favor of a single `pyproject.toml` file

Has anyone done the "conversion" yet and? Any experiences, and/or gotchas?
------
I just heard the news that Falcon dropped the revenue reporting & sharing requirements, converting the license into a regular Apache 2.0 License. That’s great!
------
The new Falcon LLM, beating LLaMA in benchmarks, sounds exciting. Looking forward to reading the paper in detail. 

But a BIG WARNING if you want to use it for anything commercial: the license requires paying 10% of the revenue. Which requires disclosing financial company info !?  twitter.com/omarsar0/statu…
------
And that would be finetuning an LLM on a MacBook under an hour on the 1000-example "LIMA: Less Is More for Alignment" dataset (https://arxiv.org/abs/2305.11206). 

(PS: Eagerly waiting for the authors to share it :))
------
Thanks to our community you can now fine-tune LLMs on your laptop 

Lit-Parrot now supports fine-tuning on Apple’s metal accelerator (MPS) 

Fine-tune a large language model with Alpaca within 40 hours on your laptop now https://lightning.ai/pages/blog/how-to-finetune-gpt-like-large-language-models-on-a-custom-dataset/…

Kudos to agmo1993 for this… Show more
------
A new alternative to RLHF just dropped!
In Direct Preference Optimization: Your Language Model is Secretly a Reward Model (https://arxiv.org/abs/2305.18290), the researchers show that the cross-entropy loss for fitting the reward model in RLHF can be used directly to finetune the LLM.
------
We did a small ablations in SantaCoder (StarCoder's little cousin): https://arxiv.org/abs/2301.03988

TLDR: it hurts a bit (given everything else is the same) but if you need to generate long sequences you save a ton of GPU memory in the KV cache and don't want to ever go back
------
What happens if we train LLMs for multiple epochs?

The question I asked multiple times in the past finally got answered in this new preprint,
"To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis".

1/6
------
Caveat, the ablation study is missing two crucial entries!

5/6
------
However, the takeaway is that the key question was answered: LLMs are susceptible to overfitting when repeating the training data for multiple epochs, degrading the task performance.

Link to the paper: https://arxiv.org/abs/2305.13230

6/6
------
Most recent large transformer decoders use this trick of having multiple heads for the queries, but only one for the keys/values.

I always thought it’s a small not well documented trick of the trade. But no, there’s a nice paper about „multi query attention“, of course by Noam.
------
When all you need is one write-head, not a co-author or many citations
------
If you are using `watermark` for your Python scripts or Jupyter notebooks: 
Watermark version 2.4 now has a new `--gpu` flag to display available GPU hardware (requires cuda).

Via a kind contribution: https://github.com/rasbt/watermark/pull/90…
------
Just looking at the most impactful LLM literature for this month, I am once more impressed by the importance of LLaMA for research: LIMA, Goat, QLoRA, ...

Clearly, LLaMA is the most valuable LLM model for innovation this year, and it underlines the importance of open source.
------
Whoa, another round number ... 

It's very motivating to know that so many people find my writings useful! 

https://magazine.sebastianraschka.com

(Stay tuned for the next issue, which will not only cover the latest LLM research but also transformers for computer vision for a change!)
------
Whoa, just crossed the 200k follower mark here on Twitter !

So grateful that you all find value in my content.
And I am looking forward to writing and sharing about AI research for many years to come!   

PS: Also pls feel free to ask me anything if you like!
------
"LIMA: Less Is More for Alignment" might be a gamechanger for researchers & tinkerers who want to develop capable LLMs. 

The researchers showed that a 65B LLaMA model finetuned on only 1000 examples (in supervised fashion) is not too far behind bigger models like ChatGPT!

1/6
------
This is an interesting paper. One baseline and ablation study I am really missing here though is how LIMA compared to a 65B LLaMA base model finetuned with RLHF instead of supervised learning. 

5/6
------
If you want to read more, here's a link to the paper: https://arxiv.org/abs/2305.11206

Unfortunately, a code repository does not seem to exist yet.

6/6
------
The new Falcon LLM, beating LLaMA in benchmarks, sounds exciting. Looking forward to reading the paper in detail. 

But a BIG WARNING if you want to use it for anything commercial: the license requires paying 10% of the revenue. Which requires disclosing financial company info !?
------
Introducing Falcon-40B. A new language model trained on 1000B tokens.

What's included:

- 7B and 40B models made available by TII
- surpasses LLaMA 65B and other models like MPT and RedPajama on the Open LLM Leaderboard
- architecture is optimized for inference, with… Show more
------
Thanks 
@unsorsodicorda
 for bringing this to attention
------
About the motivation behind self-attention!

A few short videos  for a long weekend 
https://lightning.ai/pages/courses/deep-learning-fundamentals/unit-8.0-natural-language-processing-and-large-language-models/8.4-from-rnns-to-the-transformer-architecture/…
------
LLMs are now almost everywhere

But did you ever wonder what motivated creating the transformer architecture and where the attention mechanism comes from? 

Check out @rasbt's 3 short (< 5 min) videos to learn about the evolution from RNNs to transformers… Show more
------
Here's an AI tongue-twister:

Dropout Drops Double Descent
https://arxiv.org/abs/2305.16179

And a brain teaser: 
previous deep learning models do not encounter double-descent scenarios—because we already apply a usual regularization approach like the dropout in our models.
------
Bonus II. On Transformers with linearized self-attention, BERT, RoBERTa models, Pre and Post Layer Normalization and more
------
Just put together a list of papers to highlight 4 interesting tidbits about transformers and large language models (LLMs).

Including a discussion on why the original transformer architecture figure is wrong, and a related approach published in 1991!

https://magazine.sebastianraschka.com/p/why-the-original-transformer-figure/…
------
@bryanklow
 Do you have an updated link for the GitHub repository? The one referenced in the paper doesn't work:  https://github.com/liutiedong/goat

9/9
------
PS: If you are interested in adopting a similar LLM for instruction-finetuning on a custom dataset, 
@aniketmaurya
 had a nice blog post walkthrough last week based on Lit-Parrot: https://lightning.ai/pages/blog/how-to-finetune-gpt-like-large-language-models-on-a-custom-dataset/…

The same works for the related Lit-LLaMA repo:
------
Was just reading about the latest Python packaging recommendations in 2023 (via https://drivendata.co/blog/python-packaging-2023…). 

Takeway: `setup. py` & `setup.cfg` are now deprecated in favor of a single `pyproject.toml` file

Has anyone done the "conversion" yet and? Any experiences, and/or gotchas?
------
I just heard the news that Falcon dropped the revenue reporting & sharing requirements, converting the license into a regular Apache 2.0 License. That’s great!
------
The new Falcon LLM, beating LLaMA in benchmarks, sounds exciting. Looking forward to reading the paper in detail. 

But a BIG WARNING if you want to use it for anything commercial: the license requires paying 10% of the revenue. Which requires disclosing financial company info !?  twitter.com/omarsar0/statu…
------
And that would be finetuning an LLM on a MacBook under an hour on the 1000-example "LIMA: Less Is More for Alignment" dataset (https://arxiv.org/abs/2305.11206). 

(PS: Eagerly waiting for the authors to share it :))
------
Thanks to our community you can now fine-tune LLMs on your laptop 

Lit-Parrot now supports fine-tuning on Apple’s metal accelerator (MPS) 

Fine-tune a large language model with Alpaca within 40 hours on your laptop now https://lightning.ai/pages/blog/how-to-finetune-gpt-like-large-language-models-on-a-custom-dataset/…

Kudos to agmo1993 for this… Show more
------
A new alternative to RLHF just dropped!
In Direct Preference Optimization: Your Language Model is Secretly a Reward Model (https://arxiv.org/abs/2305.18290), the researchers show that the cross-entropy loss for fitting the reward model in RLHF can be used directly to finetune the LLM.
------
We did a small ablations in SantaCoder (StarCoder's little cousin): https://arxiv.org/abs/2301.03988

TLDR: it hurts a bit (given everything else is the same) but if you need to generate long sequences you save a ton of GPU memory in the KV cache and don't want to ever go back
------
What happens if we train LLMs for multiple epochs?

The question I asked multiple times in the past finally got answered in this new preprint,
"To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis".

1/6
------
Caveat, the ablation study is missing two crucial entries!

5/6
------
However, the takeaway is that the key question was answered: LLMs are susceptible to overfitting when repeating the training data for multiple epochs, degrading the task performance.

Link to the paper: https://arxiv.org/abs/2305.13230

6/6
------
Most recent large transformer decoders use this trick of having multiple heads for the queries, but only one for the keys/values.

I always thought it’s a small not well documented trick of the trade. But no, there’s a nice paper about „multi query attention“, of course by Noam.
------
When all you need is one write-head, not a co-author or many citations
------
If you are using `watermark` for your Python scripts or Jupyter notebooks: 
Watermark version 2.4 now has a new `--gpu` flag to display available GPU hardware (requires cuda).

Via a kind contribution: https://github.com/rasbt/watermark/pull/90…
------
Just looking at the most impactful LLM literature for this month, I am once more impressed by the importance of LLaMA for research: LIMA, Goat, QLoRA, ...

Clearly, LLaMA is the most valuable LLM model for innovation this year, and it underlines the importance of open source.
------
Whoa, another round number ... 

It's very motivating to know that so many people find my writings useful! 

https://magazine.sebastianraschka.com

(Stay tuned for the next issue, which will not only cover the latest LLM research but also transformers for computer vision for a change!)
------
Whoa, just crossed the 200k follower mark here on Twitter !

So grateful that you all find value in my content.
And I am looking forward to writing and sharing about AI research for many years to come!   

PS: Also pls feel free to ask me anything if you like!
------
"LIMA: Less Is More for Alignment" might be a gamechanger for researchers & tinkerers who want to develop capable LLMs. 

The researchers showed that a 65B LLaMA model finetuned on only 1000 examples (in supervised fashion) is not too far behind bigger models like ChatGPT!

1/6
------
This is an interesting paper. One baseline and ablation study I am really missing here though is how LIMA compared to a 65B LLaMA base model finetuned with RLHF instead of supervised learning. 

5/6
------
If you want to read more, here's a link to the paper: https://arxiv.org/abs/2305.11206

Unfortunately, a code repository does not seem to exist yet.

6/6
------
The new Falcon LLM, beating LLaMA in benchmarks, sounds exciting. Looking forward to reading the paper in detail. 

But a BIG WARNING if you want to use it for anything commercial: the license requires paying 10% of the revenue. Which requires disclosing financial company info !?
------
Introducing Falcon-40B. A new language model trained on 1000B tokens.

What's included:

- 7B and 40B models made available by TII
- surpasses LLaMA 65B and other models like MPT and RedPajama on the Open LLM Leaderboard
- architecture is optimized for inference, with… Show more
------
Thanks 
@unsorsodicorda
 for bringing this to attention
------
About the motivation behind self-attention!

A few short videos  for a long weekend 
https://lightning.ai/pages/courses/deep-learning-fundamentals/unit-8.0-natural-language-processing-and-large-language-models/8.4-from-rnns-to-the-transformer-architecture/…
------
LLMs are now almost everywhere

But did you ever wonder what motivated creating the transformer architecture and where the attention mechanism comes from? 

Check out @rasbt's 3 short (< 5 min) videos to learn about the evolution from RNNs to transformers… Show more
------
Here's an AI tongue-twister:

Dropout Drops Double Descent
https://arxiv.org/abs/2305.16179

And a brain teaser: 
previous deep learning models do not encounter double-descent scenarios—because we already apply a usual regularization approach like the dropout in our models.
------
Bonus II. On Transformers with linearized self-attention, BERT, RoBERTa models, Pre and Post Layer Normalization and more
------
Just put together a list of papers to highlight 4 interesting tidbits about transformers and large language models (LLMs).

Including a discussion on why the original transformer architecture figure is wrong, and a related approach published in 1991!

https://magazine.sebastianraschka.com/p/why-the-original-transformer-figure/…
------
@bryanklow
 Do you have an updated link for the GitHub repository? The one referenced in the paper doesn't work:  https://github.com/liutiedong/goat

9/9
------
PS: If you are interested in adopting a similar LLM for instruction-finetuning on a custom dataset, 
@aniketmaurya
 had a nice blog post walkthrough last week based on Lit-Parrot: https://lightning.ai/pages/blog/how-to-finetune-gpt-like-large-language-models-on-a-custom-dataset/…

The same works for the related Lit-LLaMA repo:
------
Let's take a closer look at the Goat model, a finetuned 7B LLaMA model that outperforms GPT-4 on arithmetic tasks.

1/9
------
Here are the links if you want to learn more:

- Paper: https://arxiv.org/abs/2305.14201 
- GitHub repo: I am eagerly waiting for it!

8/9
------
I'd say arithmetic tasks make for a good testbed, because it's easy to generate synthetic training sets with the true labels. And the generated responses are easy to evaluate as well compared to other free-form text answers. 

7/9
------
Elephant in the room: why using LLM for simple arithmetic tasks where we have more capable and reliable tools like Wolfram Alpha or just regular calculators?

6/9
------
Point 1 above is obvious: A 7B LLaMA base-model is of course not as good as GPT-4. 

For point 2: It turns out that finetuning other LLMs tha utilize a different tokenization than LLaMA (e.g., OPT, GPT-J, GPT-NeoX, and Pythia) are not as good as Goat when finetuned either.

5/9
------
Why is it so good? The two main ingredients are

1. supervised finetuning on a target task (versus general pretraining or intruction finetuning)

2. LLaMA's tokenization (splits each digit into an individual token)

4/9
------
We did a small ablations in SantaCoder (StarCoder's little cousin): https://arxiv.org/abs/2301.03988

TLDR: it hurts a bit (given everything else is the same) but if you need to generate long sequences you save a ton of GPU memory in the KV cache and don't want to ever go back
------
What happens if we train LLMs for multiple epochs?

The question I asked multiple times in the past finally got answered in this new preprint,
"To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis".

1/6
------
Caveat, the ablation study is missing two crucial entries!

5/6
------
However, the takeaway is that the key question was answered: LLMs are susceptible to overfitting when repeating the training data for multiple epochs, degrading the task performance.

Link to the paper: https://arxiv.org/abs/2305.13230

6/6
------
Most recent large transformer decoders use this trick of having multiple heads for the queries, but only one for the keys/values.

I always thought it’s a small not well documented trick of the trade. But no, there’s a nice paper about „multi query attention“, of course by Noam.
------
When all you need is one write-head, not a co-author or many citations
------
If you are using `watermark` for your Python scripts or Jupyter notebooks: 
Watermark version 2.4 now has a new `--gpu` flag to display available GPU hardware (requires cuda).

Via a kind contribution: https://github.com/rasbt/watermark/pull/90…
------
Just looking at the most impactful LLM literature for this month, I am once more impressed by the importance of LLaMA for research: LIMA, Goat, QLoRA, ...

Clearly, LLaMA is the most valuable LLM model for innovation this year, and it underlines the importance of open source.
------
Whoa, another round number ... 

It's very motivating to know that so many people find my writings useful! 

https://magazine.sebastianraschka.com

(Stay tuned for the next issue, which will not only cover the latest LLM research but also transformers for computer vision for a change!)
------
Whoa, just crossed the 200k follower mark here on Twitter !

So grateful that you all find value in my content.
And I am looking forward to writing and sharing about AI research for many years to come!   

PS: Also pls feel free to ask me anything if you like!
------
"LIMA: Less Is More for Alignment" might be a gamechanger for researchers & tinkerers who want to develop capable LLMs. 

The researchers showed that a 65B LLaMA model finetuned on only 1000 examples (in supervised fashion) is not too far behind bigger models like ChatGPT!

1/6
------
This is an interesting paper. One baseline and ablation study I am really missing here though is how LIMA compared to a 65B LLaMA base model finetuned with RLHF instead of supervised learning. 

5/6
------
If you want to read more, here's a link to the paper: https://arxiv.org/abs/2305.11206

Unfortunately, a code repository does not seem to exist yet.

6/6
------
The new Falcon LLM, beating LLaMA in benchmarks, sounds exciting. Looking forward to reading the paper in detail. 

But a BIG WARNING if you want to use it for anything commercial: the license requires paying 10% of the revenue. Which requires disclosing financial company info !?
------
Introducing Falcon-40B. A new language model trained on 1000B tokens.

What's included:

- 7B and 40B models made available by TII
- surpasses LLaMA 65B and other models like MPT and RedPajama on the Open LLM Leaderboard
- architecture is optimized for inference, with… Show more
------
Thanks 
@unsorsodicorda
 for bringing this to attention
------
About the motivation behind self-attention!

A few short videos  for a long weekend 
https://lightning.ai/pages/courses/deep-learning-fundamentals/unit-8.0-natural-language-processing-and-large-language-models/8.4-from-rnns-to-the-transformer-architecture/…
------
LLMs are now almost everywhere

But did you ever wonder what motivated creating the transformer architecture and where the attention mechanism comes from? 

Check out @rasbt's 3 short (< 5 min) videos to learn about the evolution from RNNs to transformers… Show more
------
Here's an AI tongue-twister:

Dropout Drops Double Descent
https://arxiv.org/abs/2305.16179

And a brain teaser: 
previous deep learning models do not encounter double-descent scenarios—because we already apply a usual regularization approach like the dropout in our models.
------
Bonus II. On Transformers with linearized self-attention, BERT, RoBERTa models, Pre and Post Layer Normalization and more
------
Just put together a list of papers to highlight 4 interesting tidbits about transformers and large language models (LLMs).

Including a discussion on why the original transformer architecture figure is wrong, and a related approach published in 1991!

https://magazine.sebastianraschka.com/p/why-the-original-transformer-figure/…
------
@bryanklow
 Do you have an updated link for the GitHub repository? The one referenced in the paper doesn't work:  https://github.com/liutiedong/goat

9/9
------
PS: If you are interested in adopting a similar LLM for instruction-finetuning on a custom dataset, 
@aniketmaurya
 had a nice blog post walkthrough last week based on Lit-Parrot: https://lightning.ai/pages/blog/how-to-finetune-gpt-like-large-language-models-on-a-custom-dataset/…

The same works for the related Lit-LLaMA repo:
------
Let's take a closer look at the Goat model, a finetuned 7B LLaMA model that outperforms GPT-4 on arithmetic tasks.

1/9
------
Here are the links if you want to learn more:

- Paper: https://arxiv.org/abs/2305.14201 
- GitHub repo: I am eagerly waiting for it!

8/9
------
I'd say arithmetic tasks make for a good testbed, because it's easy to generate synthetic training sets with the true labels. And the generated responses are easy to evaluate as well compared to other free-form text answers. 

7/9
------
Elephant in the room: why using LLM for simple arithmetic tasks where we have more capable and reliable tools like Wolfram Alpha or just regular calculators?

6/9
------
Point 1 above is obvious: A 7B LLaMA base-model is of course not as good as GPT-4. 

For point 2: It turns out that finetuning other LLMs tha utilize a different tokenization than LLaMA (e.g., OPT, GPT-J, GPT-NeoX, and Pythia) are not as good as Goat when finetuned either.

5/9
------
Why is it so good? The two main ingredients are

1. supervised finetuning on a target task (versus general pretraining or intruction finetuning)

2. LLaMA's tokenization (splits each digit into an individual token)

4/9
------
Not only does the 7B Goat model outperform a ~75x larger 540B PaLM model and GPT-4 in zero-shot settings, but a zero-shot 7B Goat model also outperforms the larger models when the other models use 3-shot prompts.

2/9
------
Zero-shot means that the main query is provided without additional examples of the task. 3-shot means that 3 examples are provided in the input prompt.

3/9
------
“Regulate us in a way that will make it hard for others to compete”
------
 Deep Learning Fundamentals

A free course on deep learning using a modern open-source stack.

Taught by 
@rasbt
 ,best-selling author, professor & AI educator.

Arguably the best course on DL today!

Check this out
------
It’s crazy. And the GPT 1-3 papers are actually still worthwhile reading. Ideally back to back as they read like a good story. 

That was also the main reason why I was so bumped about the GPT-4 technical report. It’s like stopping the Song of Ice and Fire series after book 5.
------
Most of you probably haven’t read this. Below is OpenAI’s announcement for GPT-1 in Jun 2018.

5 years ago, the dedication to scaling up was already there. The level of foresight was unreal. Rome wasn’t built in a day.

Key quotes: 

- “We’re increasingly interested in… Show more
------
Most recent large transformer decoders use this trick of having multiple heads for the queries, but only one for the keys/values.

I always thought it’s a small not well documented trick of the trade. But no, there’s a nice paper about „multi query attention“, of course by Noam.
------
When all you need is one write-head, not a co-author or many citations
------
If you are using `watermark` for your Python scripts or Jupyter notebooks: 
Watermark version 2.4 now has a new `--gpu` flag to display available GPU hardware (requires cuda).

Via a kind contribution: https://github.com/rasbt/watermark/pull/90…
------
Just looking at the most impactful LLM literature for this month, I am once more impressed by the importance of LLaMA for research: LIMA, Goat, QLoRA, ...

Clearly, LLaMA is the most valuable LLM model for innovation this year, and it underlines the importance of open source.
------
Whoa, another round number ... 

It's very motivating to know that so many people find my writings useful! 

https://magazine.sebastianraschka.com

(Stay tuned for the next issue, which will not only cover the latest LLM research but also transformers for computer vision for a change!)
------
Whoa, just crossed the 200k follower mark here on Twitter !

So grateful that you all find value in my content.
And I am looking forward to writing and sharing about AI research for many years to come!   

PS: Also pls feel free to ask me anything if you like!
------
"LIMA: Less Is More for Alignment" might be a gamechanger for researchers & tinkerers who want to develop capable LLMs. 

The researchers showed that a 65B LLaMA model finetuned on only 1000 examples (in supervised fashion) is not too far behind bigger models like ChatGPT!

1/6
------
This is an interesting paper. One baseline and ablation study I am really missing here though is how LIMA compared to a 65B LLaMA base model finetuned with RLHF instead of supervised learning. 

5/6
------
If you want to read more, here's a link to the paper: https://arxiv.org/abs/2305.11206

Unfortunately, a code repository does not seem to exist yet.

6/6
------
The new Falcon LLM, beating LLaMA in benchmarks, sounds exciting. Looking forward to reading the paper in detail. 

But a BIG WARNING if you want to use it for anything commercial: the license requires paying 10% of the revenue. Which requires disclosing financial company info !?
------
Introducing Falcon-40B. A new language model trained on 1000B tokens.

What's included:

- 7B and 40B models made available by TII
- surpasses LLaMA 65B and other models like MPT and RedPajama on the Open LLM Leaderboard
- architecture is optimized for inference, with… Show more
------
Thanks 
@unsorsodicorda
 for bringing this to attention
------
About the motivation behind self-attention!

A few short videos  for a long weekend 
https://lightning.ai/pages/courses/deep-learning-fundamentals/unit-8.0-natural-language-processing-and-large-language-models/8.4-from-rnns-to-the-transformer-architecture/…
------
LLMs are now almost everywhere

But did you ever wonder what motivated creating the transformer architecture and where the attention mechanism comes from? 

Check out @rasbt's 3 short (< 5 min) videos to learn about the evolution from RNNs to transformers… Show more
------
Here's an AI tongue-twister:

Dropout Drops Double Descent
https://arxiv.org/abs/2305.16179

And a brain teaser: 
previous deep learning models do not encounter double-descent scenarios—because we already apply a usual regularization approach like the dropout in our models.
------
Bonus II. On Transformers with linearized self-attention, BERT, RoBERTa models, Pre and Post Layer Normalization and more
------
Just put together a list of papers to highlight 4 interesting tidbits about transformers and large language models (LLMs).

Including a discussion on why the original transformer architecture figure is wrong, and a related approach published in 1991!

https://magazine.sebastianraschka.com/p/why-the-original-transformer-figure/…
------
@bryanklow
 Do you have an updated link for the GitHub repository? The one referenced in the paper doesn't work:  https://github.com/liutiedong/goat

9/9
------
PS: If you are interested in adopting a similar LLM for instruction-finetuning on a custom dataset, 
@aniketmaurya
 had a nice blog post walkthrough last week based on Lit-Parrot: https://lightning.ai/pages/blog/how-to-finetune-gpt-like-large-language-models-on-a-custom-dataset/…

The same works for the related Lit-LLaMA repo:
------
Let's take a closer look at the Goat model, a finetuned 7B LLaMA model that outperforms GPT-4 on arithmetic tasks.

1/9
------
Here are the links if you want to learn more:

- Paper: https://arxiv.org/abs/2305.14201 
- GitHub repo: I am eagerly waiting for it!

8/9
------
I'd say arithmetic tasks make for a good testbed, because it's easy to generate synthetic training sets with the true labels. And the generated responses are easy to evaluate as well compared to other free-form text answers. 

7/9
------
Elephant in the room: why using LLM for simple arithmetic tasks where we have more capable and reliable tools like Wolfram Alpha or just regular calculators?

6/9
------
Point 1 above is obvious: A 7B LLaMA base-model is of course not as good as GPT-4. 

For point 2: It turns out that finetuning other LLMs tha utilize a different tokenization than LLaMA (e.g., OPT, GPT-J, GPT-NeoX, and Pythia) are not as good as Goat when finetuned either.

5/9
------
Why is it so good? The two main ingredients are

1. supervised finetuning on a target task (versus general pretraining or intruction finetuning)

2. LLaMA's tokenization (splits each digit into an individual token)

4/9
------
Not only does the 7B Goat model outperform a ~75x larger 540B PaLM model and GPT-4 in zero-shot settings, but a zero-shot 7B Goat model also outperforms the larger models when the other models use 3-shot prompts.

2/9
------
Zero-shot means that the main query is provided without additional examples of the task. 3-shot means that 3 examples are provided in the input prompt.

3/9
------
“Regulate us in a way that will make it hard for others to compete”
------
 Deep Learning Fundamentals

A free course on deep learning using a modern open-source stack.

Taught by 
@rasbt
 ,best-selling author, professor & AI educator.

Arguably the best course on DL today!

Check this out
------
It’s crazy. And the GPT 1-3 papers are actually still worthwhile reading. Ideally back to back as they read like a good story. 

That was also the main reason why I was so bumped about the GPT-4 technical report. It’s like stopping the Song of Ice and Fire series after book 5.
------
Most of you probably haven’t read this. Below is OpenAI’s announcement for GPT-1 in Jun 2018.

5 years ago, the dedication to scaling up was already there. The level of foresight was unreal. Rome wasn’t built in a day.

Key quotes: 

- “We’re increasingly interested in… Show more
------
Yes, thanks! I've been trying to convince people that finetuning LLMs is *the way* to get better modeling performance for months now!

Finally some more official evidence.
------
Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks

introduce Goat, a fine-tuned LLaMA model that significantly outperforms GPT-4 on a range of arithmetic tasks. Fine-tuned on a synthetically generated dataset, Goat achieves state-of-the-art performance on BIG-bench… Show more
------
OpenAI marketing may try to make you believe that GPT-4 is all you need. It's maybe sufficient for a lot of tasks, but it's unreasonable that in-context learning should do better than a well-finetuned model. (Also: you can combine finetuned models and prompting as well)
------
Just put together a list of papers to highlight 4 interesting tidbits about transformers and large language models (LLMs).

Including a discussion on why the original transformer architecture figure is wrong, and a related approach published in 1991!
------
The list is now featuring Fast Weight Programmers by 
@SchmidhuberAI
 of course!
------
Congrats to everyone who graduated this month! And also congrats to those who are currently enjoying a well-deserved semester break!

Now Looking for something to learn during summer? Check out 
@skalskip92
's curated "crème de la crème of AI courses" list:
https://github.com/SkalskiP/courses…
------
"LIMA: Less Is More for Alignment" might be a gamechanger for researchers & tinkerers who want to develop capable LLMs. 

The researchers showed that a 65B LLaMA model finetuned on only 1000 examples (in supervised fashion) is not too far behind bigger models like ChatGPT!

1/6
------
This is an interesting paper. One baseline and ablation study I am really missing here though is how LIMA compared to a 65B LLaMA base model finetuned with RLHF instead of supervised learning. 

5/6
------
If you want to read more, here's a link to the paper: https://arxiv.org/abs/2305.11206

Unfortunately, a code repository does not seem to exist yet.

6/6
------
The new Falcon LLM, beating LLaMA in benchmarks, sounds exciting. Looking forward to reading the paper in detail. 

But a BIG WARNING if you want to use it for anything commercial: the license requires paying 10% of the revenue. Which requires disclosing financial company info !?
------
Introducing Falcon-40B. A new language model trained on 1000B tokens.

What's included:

- 7B and 40B models made available by TII
- surpasses LLaMA 65B and other models like MPT and RedPajama on the Open LLM Leaderboard
- architecture is optimized for inference, with… Show more
------
Thanks 
@unsorsodicorda
 for bringing this to attention
------
About the motivation behind self-attention!

A few short videos  for a long weekend 
https://lightning.ai/pages/courses/deep-learning-fundamentals/unit-8.0-natural-language-processing-and-large-language-models/8.4-from-rnns-to-the-transformer-architecture/…
------
LLMs are now almost everywhere

But did you ever wonder what motivated creating the transformer architecture and where the attention mechanism comes from? 

Check out @rasbt's 3 short (< 5 min) videos to learn about the evolution from RNNs to transformers… Show more
------
Here's an AI tongue-twister:

Dropout Drops Double Descent
https://arxiv.org/abs/2305.16179

And a brain teaser: 
previous deep learning models do not encounter double-descent scenarios—because we already apply a usual regularization approach like the dropout in our models.
------
Bonus II. On Transformers with linearized self-attention, BERT, RoBERTa models, Pre and Post Layer Normalization and more
------
Just put together a list of papers to highlight 4 interesting tidbits about transformers and large language models (LLMs).

Including a discussion on why the original transformer architecture figure is wrong, and a related approach published in 1991!

https://magazine.sebastianraschka.com/p/why-the-original-transformer-figure/…
------
@bryanklow
 Do you have an updated link for the GitHub repository? The one referenced in the paper doesn't work:  https://github.com/liutiedong/goat

9/9
------
PS: If you are interested in adopting a similar LLM for instruction-finetuning on a custom dataset, 
@aniketmaurya
 had a nice blog post walkthrough last week based on Lit-Parrot: https://lightning.ai/pages/blog/how-to-finetune-gpt-like-large-language-models-on-a-custom-dataset/…

The same works for the related Lit-LLaMA repo:
------
Let's take a closer look at the Goat model, a finetuned 7B LLaMA model that outperforms GPT-4 on arithmetic tasks.

1/9
------
Here are the links if you want to learn more:

- Paper: https://arxiv.org/abs/2305.14201 
- GitHub repo: I am eagerly waiting for it!

8/9
------
I'd say arithmetic tasks make for a good testbed, because it's easy to generate synthetic training sets with the true labels. And the generated responses are easy to evaluate as well compared to other free-form text answers. 

7/9
------
Elephant in the room: why using LLM for simple arithmetic tasks where we have more capable and reliable tools like Wolfram Alpha or just regular calculators?

6/9
------
Point 1 above is obvious: A 7B LLaMA base-model is of course not as good as GPT-4. 

For point 2: It turns out that finetuning other LLMs tha utilize a different tokenization than LLaMA (e.g., OPT, GPT-J, GPT-NeoX, and Pythia) are not as good as Goat when finetuned either.

5/9
------
Why is it so good? The two main ingredients are

1. supervised finetuning on a target task (versus general pretraining or intruction finetuning)

2. LLaMA's tokenization (splits each digit into an individual token)

4/9
------
Not only does the 7B Goat model outperform a ~75x larger 540B PaLM model and GPT-4 in zero-shot settings, but a zero-shot 7B Goat model also outperforms the larger models when the other models use 3-shot prompts.

2/9
------
Zero-shot means that the main query is provided without additional examples of the task. 3-shot means that 3 examples are provided in the input prompt.

3/9
------
“Regulate us in a way that will make it hard for others to compete”
------
 Deep Learning Fundamentals

A free course on deep learning using a modern open-source stack.

Taught by 
@rasbt
 ,best-selling author, professor & AI educator.

Arguably the best course on DL today!

Check this out
------
It’s crazy. And the GPT 1-3 papers are actually still worthwhile reading. Ideally back to back as they read like a good story. 

That was also the main reason why I was so bumped about the GPT-4 technical report. It’s like stopping the Song of Ice and Fire series after book 5.
------
Most of you probably haven’t read this. Below is OpenAI’s announcement for GPT-1 in Jun 2018.

5 years ago, the dedication to scaling up was already there. The level of foresight was unreal. Rome wasn’t built in a day.

Key quotes: 

- “We’re increasingly interested in… Show more
------
Yes, thanks! I've been trying to convince people that finetuning LLMs is *the way* to get better modeling performance for months now!

Finally some more official evidence.
------
Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks

introduce Goat, a fine-tuned LLaMA model that significantly outperforms GPT-4 on a range of arithmetic tasks. Fine-tuned on a synthetically generated dataset, Goat achieves state-of-the-art performance on BIG-bench… Show more
------
OpenAI marketing may try to make you believe that GPT-4 is all you need. It's maybe sufficient for a lot of tasks, but it's unreasonable that in-context learning should do better than a well-finetuned model. (Also: you can combine finetuned models and prompting as well)
------
Just put together a list of papers to highlight 4 interesting tidbits about transformers and large language models (LLMs).

Including a discussion on why the original transformer architecture figure is wrong, and a related approach published in 1991!
------
The list is now featuring Fast Weight Programmers by 
@SchmidhuberAI
 of course!
------
Congrats to everyone who graduated this month! And also congrats to those who are currently enjoying a well-deserved semester break!

Now Looking for something to learn during summer? Check out 
@skalskip92
's curated "crème de la crème of AI courses" list:
https://github.com/SkalskiP/courses…
------
Have been heads down working on some LLM coding projects the last couple of days ... 

Long story short, among others, you can now finetune LLMs on instructions using the recent LLaMA-Adapter v2 method via Lit-LLAMA: https://github.com/Lightning-AI/lit-llama…

1/3
------
2/3 What is LLaMA-Adapter v2? It's a parameter-efficient finetuning method for LLMs. 

LLaMA-Adapter uses only 1.2M trainable parameters (for a 7B param model) by modifying each transformer block as I sketched below.  V2 adds another 3.2 M parameters.

2/3
------
Multimodal inputs aside, V2 makes the RMSNorm layers trainable + adds trainable bias & scale params.

Kudos to 
@lantiga
 
@adrianwaelchli
 
@carmocca
 for striking a great balance between sophisticated & hackable code via Lit-LLaMA so that I was able to implement it in no time!

3/3
------
About the motivation behind self-attention!

A few short videos  for a long weekend 
https://lightning.ai/pages/courses/deep-learning-fundamentals/unit-8.0-natural-language-processing-and-large-language-models/8.4-from-rnns-to-the-transformer-architecture/…
------
LLMs are now almost everywhere

But did you ever wonder what motivated creating the transformer architecture and where the attention mechanism comes from? 

Check out @rasbt's 3 short (< 5 min) videos to learn about the evolution from RNNs to transformers… Show more
------
Here's an AI tongue-twister:

Dropout Drops Double Descent
https://arxiv.org/abs/2305.16179

And a brain teaser: 
previous deep learning models do not encounter double-descent scenarios—because we already apply a usual regularization approach like the dropout in our models.
------
Bonus II. On Transformers with linearized self-attention, BERT, RoBERTa models, Pre and Post Layer Normalization and more
------
Just put together a list of papers to highlight 4 interesting tidbits about transformers and large language models (LLMs).

Including a discussion on why the original transformer architecture figure is wrong, and a related approach published in 1991!

https://magazine.sebastianraschka.com/p/why-the-original-transformer-figure/…
------
@bryanklow
 Do you have an updated link for the GitHub repository? The one referenced in the paper doesn't work:  https://github.com/liutiedong/goat

9/9
------
PS: If you are interested in adopting a similar LLM for instruction-finetuning on a custom dataset, 
@aniketmaurya
 had a nice blog post walkthrough last week based on Lit-Parrot: https://lightning.ai/pages/blog/how-to-finetune-gpt-like-large-language-models-on-a-custom-dataset/…

The same works for the related Lit-LLaMA repo:
------
Let's take a closer look at the Goat model, a finetuned 7B LLaMA model that outperforms GPT-4 on arithmetic tasks.

1/9
------
Here are the links if you want to learn more:

- Paper: https://arxiv.org/abs/2305.14201 
- GitHub repo: I am eagerly waiting for it!

8/9
------
I'd say arithmetic tasks make for a good testbed, because it's easy to generate synthetic training sets with the true labels. And the generated responses are easy to evaluate as well compared to other free-form text answers. 

7/9
------
Elephant in the room: why using LLM for simple arithmetic tasks where we have more capable and reliable tools like Wolfram Alpha or just regular calculators?

6/9
------
Point 1 above is obvious: A 7B LLaMA base-model is of course not as good as GPT-4. 

For point 2: It turns out that finetuning other LLMs tha utilize a different tokenization than LLaMA (e.g., OPT, GPT-J, GPT-NeoX, and Pythia) are not as good as Goat when finetuned either.

5/9
------
Why is it so good? The two main ingredients are

1. supervised finetuning on a target task (versus general pretraining or intruction finetuning)

2. LLaMA's tokenization (splits each digit into an individual token)

4/9
------
Not only does the 7B Goat model outperform a ~75x larger 540B PaLM model and GPT-4 in zero-shot settings, but a zero-shot 7B Goat model also outperforms the larger models when the other models use 3-shot prompts.

2/9
------
Zero-shot means that the main query is provided without additional examples of the task. 3-shot means that 3 examples are provided in the input prompt.

3/9
------
“Regulate us in a way that will make it hard for others to compete”
------
 Deep Learning Fundamentals

A free course on deep learning using a modern open-source stack.

Taught by 
@rasbt
 ,best-selling author, professor & AI educator.

Arguably the best course on DL today!

Check this out
------
It’s crazy. And the GPT 1-3 papers are actually still worthwhile reading. Ideally back to back as they read like a good story. 

That was also the main reason why I was so bumped about the GPT-4 technical report. It’s like stopping the Song of Ice and Fire series after book 5.
------
Most of you probably haven’t read this. Below is OpenAI’s announcement for GPT-1 in Jun 2018.

5 years ago, the dedication to scaling up was already there. The level of foresight was unreal. Rome wasn’t built in a day.

Key quotes: 

- “We’re increasingly interested in… Show more
------
Yes, thanks! I've been trying to convince people that finetuning LLMs is *the way* to get better modeling performance for months now!

Finally some more official evidence.
------
Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks

introduce Goat, a fine-tuned LLaMA model that significantly outperforms GPT-4 on a range of arithmetic tasks. Fine-tuned on a synthetically generated dataset, Goat achieves state-of-the-art performance on BIG-bench… Show more
------
OpenAI marketing may try to make you believe that GPT-4 is all you need. It's maybe sufficient for a lot of tasks, but it's unreasonable that in-context learning should do better than a well-finetuned model. (Also: you can combine finetuned models and prompting as well)
------
Just put together a list of papers to highlight 4 interesting tidbits about transformers and large language models (LLMs).

Including a discussion on why the original transformer architecture figure is wrong, and a related approach published in 1991!
------
The list is now featuring Fast Weight Programmers by 
@SchmidhuberAI
 of course!
------
Congrats to everyone who graduated this month! And also congrats to those who are currently enjoying a well-deserved semester break!

Now Looking for something to learn during summer? Check out 
@skalskip92
's curated "crème de la crème of AI courses" list:
https://github.com/SkalskiP/courses…
------
Have been heads down working on some LLM coding projects the last couple of days ... 

Long story short, among others, you can now finetune LLMs on instructions using the recent LLaMA-Adapter v2 method via Lit-LLAMA: https://github.com/Lightning-AI/lit-llama…

1/3
------
2/3 What is LLaMA-Adapter v2? It's a parameter-efficient finetuning method for LLMs. 

LLaMA-Adapter uses only 1.2M trainable parameters (for a 7B param model) by modifying each transformer block as I sketched below.  V2 adds another 3.2 M parameters.

2/3
------
Multimodal inputs aside, V2 makes the RMSNorm layers trainable + adds trainable bias & scale params.

Kudos to 
@lantiga
 
@adrianwaelchli
 
@carmocca
 for striking a great balance between sophisticated & hackable code via Lit-LLaMA so that I was able to implement it in no time!

3/3
------
This looks like it might be the first proof of concept of multiple plugins - in this case WebPilot and Zapier - being combined together to exfiltrate private data via a prompt injection attack

I wrote about this class of attack here: https://simonwillison.net/2023/Apr/14/worst-that-can-happen/#data-exfiltration…
------
 Let ChatGPT visit a website and have your email stolen.

Plugins, Prompt Injection and Cross Plug-in Request Forgery.

Not sharing “shell code” but… 

Why no human in the loop? @openai Would mitigate the CPRF at least

#OPENAI #ChatGPT #plugins #infosec #ai #humanintheloop
------
Are you in NYC & have no plans for tonight yet? Join us at the Open Source AI meetup tonight at 6 pm with 
@_willfalcon
, 
@EMostaque
, and many others! 

(So far, >1100 people signed up. It's going to be fun!) 

Signup link below:
------
That was fun! Was really nice chatting with everyone!!
------
Let's take a closer look at the Goat model, a finetuned 7B LLaMA model that outperforms GPT-4 on arithmetic tasks.

1/9
------
Here are the links if you want to learn more:

- Paper: https://arxiv.org/abs/2305.14201 
- GitHub repo: I am eagerly waiting for it!

8/9
------
I'd say arithmetic tasks make for a good testbed, because it's easy to generate synthetic training sets with the true labels. And the generated responses are easy to evaluate as well compared to other free-form text answers. 

7/9
------
Elephant in the room: why using LLM for simple arithmetic tasks where we have more capable and reliable tools like Wolfram Alpha or just regular calculators?

6/9
------
Point 1 above is obvious: A 7B LLaMA base-model is of course not as good as GPT-4. 

For point 2: It turns out that finetuning other LLMs tha utilize a different tokenization than LLaMA (e.g., OPT, GPT-J, GPT-NeoX, and Pythia) are not as good as Goat when finetuned either.

5/9
------
Why is it so good? The two main ingredients are

1. supervised finetuning on a target task (versus general pretraining or intruction finetuning)

2. LLaMA's tokenization (splits each digit into an individual token)

4/9
------
Not only does the 7B Goat model outperform a ~75x larger 540B PaLM model and GPT-4 in zero-shot settings, but a zero-shot 7B Goat model also outperforms the larger models when the other models use 3-shot prompts.

2/9
------
Zero-shot means that the main query is provided without additional examples of the task. 3-shot means that 3 examples are provided in the input prompt.

3/9
------
“Regulate us in a way that will make it hard for others to compete”
------
 Deep Learning Fundamentals

A free course on deep learning using a modern open-source stack.

Taught by 
@rasbt
 ,best-selling author, professor & AI educator.

Arguably the best course on DL today!

Check this out
------
It’s crazy. And the GPT 1-3 papers are actually still worthwhile reading. Ideally back to back as they read like a good story. 

That was also the main reason why I was so bumped about the GPT-4 technical report. It’s like stopping the Song of Ice and Fire series after book 5.
------
Most of you probably haven’t read this. Below is OpenAI’s announcement for GPT-1 in Jun 2018.

5 years ago, the dedication to scaling up was already there. The level of foresight was unreal. Rome wasn’t built in a day.

Key quotes: 

- “We’re increasingly interested in… Show more
------
Yes, thanks! I've been trying to convince people that finetuning LLMs is *the way* to get better modeling performance for months now!

Finally some more official evidence.
------
Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks

introduce Goat, a fine-tuned LLaMA model that significantly outperforms GPT-4 on a range of arithmetic tasks. Fine-tuned on a synthetically generated dataset, Goat achieves state-of-the-art performance on BIG-bench… Show more
------
OpenAI marketing may try to make you believe that GPT-4 is all you need. It's maybe sufficient for a lot of tasks, but it's unreasonable that in-context learning should do better than a well-finetuned model. (Also: you can combine finetuned models and prompting as well)
------
Just put together a list of papers to highlight 4 interesting tidbits about transformers and large language models (LLMs).

Including a discussion on why the original transformer architecture figure is wrong, and a related approach published in 1991!
------
The list is now featuring Fast Weight Programmers by 
@SchmidhuberAI
 of course!
------
Congrats to everyone who graduated this month! And also congrats to those who are currently enjoying a well-deserved semester break!

Now Looking for something to learn during summer? Check out 
@skalskip92
's curated "crème de la crème of AI courses" list:
https://github.com/SkalskiP/courses…
------
Have been heads down working on some LLM coding projects the last couple of days ... 

Long story short, among others, you can now finetune LLMs on instructions using the recent LLaMA-Adapter v2 method via Lit-LLAMA: https://github.com/Lightning-AI/lit-llama…

1/3
------
2/3 What is LLaMA-Adapter v2? It's a parameter-efficient finetuning method for LLMs. 

LLaMA-Adapter uses only 1.2M trainable parameters (for a 7B param model) by modifying each transformer block as I sketched below.  V2 adds another 3.2 M parameters.

2/3
------
Multimodal inputs aside, V2 makes the RMSNorm layers trainable + adds trainable bias & scale params.

Kudos to 
@lantiga
 
@adrianwaelchli
 
@carmocca
 for striking a great balance between sophisticated & hackable code via Lit-LLaMA so that I was able to implement it in no time!

3/3
------
This looks like it might be the first proof of concept of multiple plugins - in this case WebPilot and Zapier - being combined together to exfiltrate private data via a prompt injection attack

I wrote about this class of attack here: https://simonwillison.net/2023/Apr/14/worst-that-can-happen/#data-exfiltration…
------
 Let ChatGPT visit a website and have your email stolen.

Plugins, Prompt Injection and Cross Plug-in Request Forgery.

Not sharing “shell code” but… 

Why no human in the loop? @openai Would mitigate the CPRF at least

#OPENAI #ChatGPT #plugins #infosec #ai #humanintheloop
------
Are you in NYC & have no plans for tonight yet? Join us at the Open Source AI meetup tonight at 6 pm with 
@_willfalcon
, 
@EMostaque
, and many others! 

(So far, >1100 people signed up. It's going to be fun!) 

Signup link below:
------
That was fun! Was really nice chatting with everyone!!
------
There are lots of parameter-efficient finetuning techniques for LLM out there. But adapters is perhaps the simplest one to get started: insert additional fully-connected layers.

PS: Beyond transformers, it can be used for other architectures too!
------
How well does it work in practice? Performance-wise,  it falls somewhere in between only finetuning the output layer and finetuning all layers. But coz it's so simple to implement, it's a great intro & baseline before experimenting with other param.-efficient finetuning methods.
------
Location info here:
------
Which route will you take?

https://lightning.ai/pages/keep-ai-open-source/…
------
Zero-shot means that the main query is provided without additional examples of the task. 3-shot means that 3 examples are provided in the input prompt.

3/9
------
“Regulate us in a way that will make it hard for others to compete”
------
 Deep Learning Fundamentals

A free course on deep learning using a modern open-source stack.

Taught by 
@rasbt
 ,best-selling author, professor & AI educator.

Arguably the best course on DL today!

Check this out
------
It’s crazy. And the GPT 1-3 papers are actually still worthwhile reading. Ideally back to back as they read like a good story. 

That was also the main reason why I was so bumped about the GPT-4 technical report. It’s like stopping the Song of Ice and Fire series after book 5.
------
Most of you probably haven’t read this. Below is OpenAI’s announcement for GPT-1 in Jun 2018.

5 years ago, the dedication to scaling up was already there. The level of foresight was unreal. Rome wasn’t built in a day.

Key quotes: 

- “We’re increasingly interested in… Show more
------
Yes, thanks! I've been trying to convince people that finetuning LLMs is *the way* to get better modeling performance for months now!

Finally some more official evidence.
------
Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks

introduce Goat, a fine-tuned LLaMA model that significantly outperforms GPT-4 on a range of arithmetic tasks. Fine-tuned on a synthetically generated dataset, Goat achieves state-of-the-art performance on BIG-bench… Show more
------
OpenAI marketing may try to make you believe that GPT-4 is all you need. It's maybe sufficient for a lot of tasks, but it's unreasonable that in-context learning should do better than a well-finetuned model. (Also: you can combine finetuned models and prompting as well)
------
Just put together a list of papers to highlight 4 interesting tidbits about transformers and large language models (LLMs).

Including a discussion on why the original transformer architecture figure is wrong, and a related approach published in 1991!
------
The list is now featuring Fast Weight Programmers by 
@SchmidhuberAI
 of course!
------
Congrats to everyone who graduated this month! And also congrats to those who are currently enjoying a well-deserved semester break!

Now Looking for something to learn during summer? Check out 
@skalskip92
's curated "crème de la crème of AI courses" list:
https://github.com/SkalskiP/courses…
------
Have been heads down working on some LLM coding projects the last couple of days ... 

Long story short, among others, you can now finetune LLMs on instructions using the recent LLaMA-Adapter v2 method via Lit-LLAMA: https://github.com/Lightning-AI/lit-llama…

1/3
------
2/3 What is LLaMA-Adapter v2? It's a parameter-efficient finetuning method for LLMs. 

LLaMA-Adapter uses only 1.2M trainable parameters (for a 7B param model) by modifying each transformer block as I sketched below.  V2 adds another 3.2 M parameters.

2/3
------
Multimodal inputs aside, V2 makes the RMSNorm layers trainable + adds trainable bias & scale params.

Kudos to 
@lantiga
 
@adrianwaelchli
 
@carmocca
 for striking a great balance between sophisticated & hackable code via Lit-LLaMA so that I was able to implement it in no time!

3/3
------
This looks like it might be the first proof of concept of multiple plugins - in this case WebPilot and Zapier - being combined together to exfiltrate private data via a prompt injection attack

I wrote about this class of attack here: https://simonwillison.net/2023/Apr/14/worst-that-can-happen/#data-exfiltration…
------
 Let ChatGPT visit a website and have your email stolen.

Plugins, Prompt Injection and Cross Plug-in Request Forgery.

Not sharing “shell code” but… 

Why no human in the loop? @openai Would mitigate the CPRF at least

#OPENAI #ChatGPT #plugins #infosec #ai #humanintheloop
------
Are you in NYC & have no plans for tonight yet? Join us at the Open Source AI meetup tonight at 6 pm with 
@_willfalcon
, 
@EMostaque
, and many others! 

(So far, >1100 people signed up. It's going to be fun!) 

Signup link below:
------
That was fun! Was really nice chatting with everyone!!
------
There are lots of parameter-efficient finetuning techniques for LLM out there. But adapters is perhaps the simplest one to get started: insert additional fully-connected layers.

PS: Beyond transformers, it can be used for other architectures too!
------
How well does it work in practice? Performance-wise,  it falls somewhere in between only finetuning the output layer and finetuning all layers. But coz it's so simple to implement, it's a great intro & baseline before experimenting with other param.-efficient finetuning methods.
------
Location info here:
------
Which route will you take?

https://lightning.ai/pages/keep-ai-open-source/…
------
See, LLMs don’t magically get skills out of thin air, as some papers suggest.

This is a very nice paper taking a deep dive into one of them (translation skill) and it clearly comes from it being in the data.

I think that’s great and a good motivator for training on everything!
------
1.4% of PALM’s training instances are detected as bilingual, while 0.34% contain at least one translated sentence pair. We were able to mine such pairs across all languages studied; therefore, none of these languages is truly zero-shot in the context of translation.
------
Why can we use relatively vanilla gradient descent algos (e.g., w/o strong regularization) to tune billion-parameter LLMs on datasets with only thousands of labeled examples?

An interesting (older) paper that motivates the rationale behind LoRA etc: https://arxiv.org/abs/2012.13255

1/3
------
So, LLM pretraining implicitly minimizes intrinsic dimension of LLMs (the longer the lower the dim).

Then, adapting a small number of parameters (like optimizing only 200) a RoBERTa model achieves 90% of the full parameter performance.

2/3
------
More details in Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning: https://arxiv.org/abs/2012.13255

3/3
------
Excited to be launching a series of short videos (5 min or less) to optimize the training performance of your PyTorch models!

Covering
- mixed-precision training
- multi-GPU training strategies
- Compiling PyTorch models
- and finding good batch sizes

 https://lightning.ai/pages/courses/deep-learning-fundamentals/9.0-overview-techniques-for-speeding-up-model-training/…
------
It’s crazy. And the GPT 1-3 papers are actually still worthwhile reading. Ideally back to back as they read like a good story. 

That was also the main reason why I was so bumped about the GPT-4 technical report. It’s like stopping the Song of Ice and Fire series after book 5.
------
Most of you probably haven’t read this. Below is OpenAI’s announcement for GPT-1 in Jun 2018.

5 years ago, the dedication to scaling up was already there. The level of foresight was unreal. Rome wasn’t built in a day.

Key quotes: 

- “We’re increasingly interested in… Show more
------
Yes, thanks! I've been trying to convince people that finetuning LLMs is *the way* to get better modeling performance for months now!

Finally some more official evidence.
------
Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks

introduce Goat, a fine-tuned LLaMA model that significantly outperforms GPT-4 on a range of arithmetic tasks. Fine-tuned on a synthetically generated dataset, Goat achieves state-of-the-art performance on BIG-bench… Show more
------
OpenAI marketing may try to make you believe that GPT-4 is all you need. It's maybe sufficient for a lot of tasks, but it's unreasonable that in-context learning should do better than a well-finetuned model. (Also: you can combine finetuned models and prompting as well)
------
Just put together a list of papers to highlight 4 interesting tidbits about transformers and large language models (LLMs).

Including a discussion on why the original transformer architecture figure is wrong, and a related approach published in 1991!
------
The list is now featuring Fast Weight Programmers by 
@SchmidhuberAI
 of course!
------
Congrats to everyone who graduated this month! And also congrats to those who are currently enjoying a well-deserved semester break!

Now Looking for something to learn during summer? Check out 
@skalskip92
's curated "crème de la crème of AI courses" list:
https://github.com/SkalskiP/courses…
------
Have been heads down working on some LLM coding projects the last couple of days ... 

Long story short, among others, you can now finetune LLMs on instructions using the recent LLaMA-Adapter v2 method via Lit-LLAMA: https://github.com/Lightning-AI/lit-llama…

1/3
------
2/3 What is LLaMA-Adapter v2? It's a parameter-efficient finetuning method for LLMs. 

LLaMA-Adapter uses only 1.2M trainable parameters (for a 7B param model) by modifying each transformer block as I sketched below.  V2 adds another 3.2 M parameters.

2/3
------
Multimodal inputs aside, V2 makes the RMSNorm layers trainable + adds trainable bias & scale params.

Kudos to 
@lantiga
 
@adrianwaelchli
 
@carmocca
 for striking a great balance between sophisticated & hackable code via Lit-LLaMA so that I was able to implement it in no time!

3/3
------
This looks like it might be the first proof of concept of multiple plugins - in this case WebPilot and Zapier - being combined together to exfiltrate private data via a prompt injection attack

I wrote about this class of attack here: https://simonwillison.net/2023/Apr/14/worst-that-can-happen/#data-exfiltration…
------
 Let ChatGPT visit a website and have your email stolen.

Plugins, Prompt Injection and Cross Plug-in Request Forgery.

Not sharing “shell code” but… 

Why no human in the loop? @openai Would mitigate the CPRF at least

#OPENAI #ChatGPT #plugins #infosec #ai #humanintheloop
------
Are you in NYC & have no plans for tonight yet? Join us at the Open Source AI meetup tonight at 6 pm with 
@_willfalcon
, 
@EMostaque
, and many others! 

(So far, >1100 people signed up. It's going to be fun!) 

Signup link below:
------
That was fun! Was really nice chatting with everyone!!
------
There are lots of parameter-efficient finetuning techniques for LLM out there. But adapters is perhaps the simplest one to get started: insert additional fully-connected layers.

PS: Beyond transformers, it can be used for other architectures too!
------
How well does it work in practice? Performance-wise,  it falls somewhere in between only finetuning the output layer and finetuning all layers. But coz it's so simple to implement, it's a great intro & baseline before experimenting with other param.-efficient finetuning methods.
------
Location info here:
------
Which route will you take?

https://lightning.ai/pages/keep-ai-open-source/…
------
See, LLMs don’t magically get skills out of thin air, as some papers suggest.

This is a very nice paper taking a deep dive into one of them (translation skill) and it clearly comes from it being in the data.

I think that’s great and a good motivator for training on everything!
------
1.4% of PALM’s training instances are detected as bilingual, while 0.34% contain at least one translated sentence pair. We were able to mine such pairs across all languages studied; therefore, none of these languages is truly zero-shot in the context of translation.
------
Why can we use relatively vanilla gradient descent algos (e.g., w/o strong regularization) to tune billion-parameter LLMs on datasets with only thousands of labeled examples?

An interesting (older) paper that motivates the rationale behind LoRA etc: https://arxiv.org/abs/2012.13255

1/3
------
So, LLM pretraining implicitly minimizes intrinsic dimension of LLMs (the longer the lower the dim).

Then, adapting a small number of parameters (like optimizing only 200) a RoBERTa model achieves 90% of the full parameter performance.

2/3
------
More details in Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning: https://arxiv.org/abs/2012.13255

3/3
------
Excited to be launching a series of short videos (5 min or less) to optimize the training performance of your PyTorch models!

Covering
- mixed-precision training
- multi-GPU training strategies
- Compiling PyTorch models
- and finding good batch sizes

 https://lightning.ai/pages/courses/deep-learning-fundamentals/9.0-overview-techniques-for-speeding-up-model-training/…
------
That’s what’s https://magazine.sebastianraschka.com/p/ai-research-highlights-in-3-sentences… is for 
------
Good discussion of the current CNN (ConvNet) vs ViT landscape, including paper references and good practical advice from 
@giffmana
 & 
@wightmanr
------
What’s the status of vision transformers. Have these dethroned convents? Is that the trend? How do things look on the horizon?
------
Prof. Gilbert Strang gave his final lecture today!

End of an era. His linear algebra lectures where the best of the best .

His lectures are a treasure, and I hope they will remain accessible indefinitely: https://ocw.mit.edu/courses/18-06-linear-algebra-spring-2010/…
------
Prof. Gilbert Strang the original YouTuber, before there was a YouTube.  

Thank you for your many contributions!  twitter.com/CSProfKGD/stat…
------
Common ways to finetune LLMs are to 
a) update the output layers vs 
b) update more (or all) layers. 

Usually, the more parameters we update, the better for target-task performance (wrote about here: https://magazine.sebastianraschka.com/p/finetuning-large-language-models…). 

Small plot twist via https://arxiv.org/abs/2202.10054

1/3
------
Congrats to everyone who graduated this month! And also congrats to those who are currently enjoying a well-deserved semester break!

Now Looking for something to learn during summer? Check out 
@skalskip92
's curated "crème de la crème of AI courses" list:
https://github.com/SkalskiP/courses…
------
Have been heads down working on some LLM coding projects the last couple of days ... 

Long story short, among others, you can now finetune LLMs on instructions using the recent LLaMA-Adapter v2 method via Lit-LLAMA: https://github.com/Lightning-AI/lit-llama…

1/3
------
2/3 What is LLaMA-Adapter v2? It's a parameter-efficient finetuning method for LLMs. 

LLaMA-Adapter uses only 1.2M trainable parameters (for a 7B param model) by modifying each transformer block as I sketched below.  V2 adds another 3.2 M parameters.

2/3
------
Multimodal inputs aside, V2 makes the RMSNorm layers trainable + adds trainable bias & scale params.

Kudos to 
@lantiga
 
@adrianwaelchli
 
@carmocca
 for striking a great balance between sophisticated & hackable code via Lit-LLaMA so that I was able to implement it in no time!

3/3
------
This looks like it might be the first proof of concept of multiple plugins - in this case WebPilot and Zapier - being combined together to exfiltrate private data via a prompt injection attack

I wrote about this class of attack here: https://simonwillison.net/2023/Apr/14/worst-that-can-happen/#data-exfiltration…
------
 Let ChatGPT visit a website and have your email stolen.

Plugins, Prompt Injection and Cross Plug-in Request Forgery.

Not sharing “shell code” but… 

Why no human in the loop? @openai Would mitigate the CPRF at least

#OPENAI #ChatGPT #plugins #infosec #ai #humanintheloop
------
Are you in NYC & have no plans for tonight yet? Join us at the Open Source AI meetup tonight at 6 pm with 
@_willfalcon
, 
@EMostaque
, and many others! 

(So far, >1100 people signed up. It's going to be fun!) 

Signup link below:
------
That was fun! Was really nice chatting with everyone!!
------
There are lots of parameter-efficient finetuning techniques for LLM out there. But adapters is perhaps the simplest one to get started: insert additional fully-connected layers.

PS: Beyond transformers, it can be used for other architectures too!
------
How well does it work in practice? Performance-wise,  it falls somewhere in between only finetuning the output layer and finetuning all layers. But coz it's so simple to implement, it's a great intro & baseline before experimenting with other param.-efficient finetuning methods.
------
Location info here:
------
Which route will you take?

https://lightning.ai/pages/keep-ai-open-source/…
------
See, LLMs don’t magically get skills out of thin air, as some papers suggest.

This is a very nice paper taking a deep dive into one of them (translation skill) and it clearly comes from it being in the data.

I think that’s great and a good motivator for training on everything!
------
1.4% of PALM’s training instances are detected as bilingual, while 0.34% contain at least one translated sentence pair. We were able to mine such pairs across all languages studied; therefore, none of these languages is truly zero-shot in the context of translation.
------
Why can we use relatively vanilla gradient descent algos (e.g., w/o strong regularization) to tune billion-parameter LLMs on datasets with only thousands of labeled examples?

An interesting (older) paper that motivates the rationale behind LoRA etc: https://arxiv.org/abs/2012.13255

1/3
------
So, LLM pretraining implicitly minimizes intrinsic dimension of LLMs (the longer the lower the dim).

Then, adapting a small number of parameters (like optimizing only 200) a RoBERTa model achieves 90% of the full parameter performance.

2/3
------
More details in Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning: https://arxiv.org/abs/2012.13255

3/3
------
Excited to be launching a series of short videos (5 min or less) to optimize the training performance of your PyTorch models!

Covering
- mixed-precision training
- multi-GPU training strategies
- Compiling PyTorch models
- and finding good batch sizes

 https://lightning.ai/pages/courses/deep-learning-fundamentals/9.0-overview-techniques-for-speeding-up-model-training/…
------
That’s what’s https://magazine.sebastianraschka.com/p/ai-research-highlights-in-3-sentences… is for 
------
Good discussion of the current CNN (ConvNet) vs ViT landscape, including paper references and good practical advice from 
@giffmana
 & 
@wightmanr
------
What’s the status of vision transformers. Have these dethroned convents? Is that the trend? How do things look on the horizon?
------
Prof. Gilbert Strang gave his final lecture today!

End of an era. His linear algebra lectures where the best of the best .

His lectures are a treasure, and I hope they will remain accessible indefinitely: https://ocw.mit.edu/courses/18-06-linear-algebra-spring-2010/…
------
Prof. Gilbert Strang the original YouTuber, before there was a YouTube.  

Thank you for your many contributions!  twitter.com/CSProfKGD/stat…
------
Common ways to finetune LLMs are to 
a) update the output layers vs 
b) update more (or all) layers. 

Usually, the more parameters we update, the better for target-task performance (wrote about here: https://magazine.sebastianraschka.com/p/finetuning-large-language-models…). 

Small plot twist via https://arxiv.org/abs/2202.10054

1/3
------
It turns out that a), tuning only the output layer, can be better on out-of-distribution tasks. 

But this maybe expected due to the large number of parameters when finetuning the whole LLM.

2/3
------
A suggested technique to get the best of both worlds seems to be a 2-step process that 
1) trains the output layers first, and 
2) then finetunes the whole LLM.

Full paper here: https://arxiv.org/abs/2202.10054
------
I've just put together a roundup of 22 AI research paper highlights from April-May 2023! 

https://magazine.sebastianraschka.com/p/ai-research-highlights-in-3-sentences…

Lots of exciting things happening at an incredible pace!
------

------
This looks like it might be the first proof of concept of multiple plugins - in this case WebPilot and Zapier - being combined together to exfiltrate private data via a prompt injection attack

I wrote about this class of attack here: https://simonwillison.net/2023/Apr/14/worst-that-can-happen/#data-exfiltration…
------
 Let ChatGPT visit a website and have your email stolen.

Plugins, Prompt Injection and Cross Plug-in Request Forgery.

Not sharing “shell code” but… 

Why no human in the loop? @openai Would mitigate the CPRF at least

#OPENAI #ChatGPT #plugins #infosec #ai #humanintheloop
------
Are you in NYC & have no plans for tonight yet? Join us at the Open Source AI meetup tonight at 6 pm with 
@_willfalcon
, 
@EMostaque
, and many others! 

(So far, >1100 people signed up. It's going to be fun!) 

Signup link below:
------
That was fun! Was really nice chatting with everyone!!
------
There are lots of parameter-efficient finetuning techniques for LLM out there. But adapters is perhaps the simplest one to get started: insert additional fully-connected layers.

PS: Beyond transformers, it can be used for other architectures too!
------
How well does it work in practice? Performance-wise,  it falls somewhere in between only finetuning the output layer and finetuning all layers. But coz it's so simple to implement, it's a great intro & baseline before experimenting with other param.-efficient finetuning methods.
------
Location info here:
------
Which route will you take?

https://lightning.ai/pages/keep-ai-open-source/…
------
See, LLMs don’t magically get skills out of thin air, as some papers suggest.

This is a very nice paper taking a deep dive into one of them (translation skill) and it clearly comes from it being in the data.

I think that’s great and a good motivator for training on everything!
------
1.4% of PALM’s training instances are detected as bilingual, while 0.34% contain at least one translated sentence pair. We were able to mine such pairs across all languages studied; therefore, none of these languages is truly zero-shot in the context of translation.
------
Why can we use relatively vanilla gradient descent algos (e.g., w/o strong regularization) to tune billion-parameter LLMs on datasets with only thousands of labeled examples?

An interesting (older) paper that motivates the rationale behind LoRA etc: https://arxiv.org/abs/2012.13255

1/3
------
So, LLM pretraining implicitly minimizes intrinsic dimension of LLMs (the longer the lower the dim).

Then, adapting a small number of parameters (like optimizing only 200) a RoBERTa model achieves 90% of the full parameter performance.

2/3
------
More details in Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning: https://arxiv.org/abs/2012.13255

3/3
------
Excited to be launching a series of short videos (5 min or less) to optimize the training performance of your PyTorch models!

Covering
- mixed-precision training
- multi-GPU training strategies
- Compiling PyTorch models
- and finding good batch sizes

 https://lightning.ai/pages/courses/deep-learning-fundamentals/9.0-overview-techniques-for-speeding-up-model-training/…
------
That’s what’s https://magazine.sebastianraschka.com/p/ai-research-highlights-in-3-sentences… is for 
------
Good discussion of the current CNN (ConvNet) vs ViT landscape, including paper references and good practical advice from 
@giffmana
 & 
@wightmanr
------
What’s the status of vision transformers. Have these dethroned convents? Is that the trend? How do things look on the horizon?
------
Prof. Gilbert Strang gave his final lecture today!

End of an era. His linear algebra lectures where the best of the best .

His lectures are a treasure, and I hope they will remain accessible indefinitely: https://ocw.mit.edu/courses/18-06-linear-algebra-spring-2010/…
------
Prof. Gilbert Strang the original YouTuber, before there was a YouTube.  

Thank you for your many contributions!  twitter.com/CSProfKGD/stat…
------
Common ways to finetune LLMs are to 
a) update the output layers vs 
b) update more (or all) layers. 

Usually, the more parameters we update, the better for target-task performance (wrote about here: https://magazine.sebastianraschka.com/p/finetuning-large-language-models…). 

Small plot twist via https://arxiv.org/abs/2202.10054

1/3
------
It turns out that a), tuning only the output layer, can be better on out-of-distribution tasks. 

But this maybe expected due to the large number of parameters when finetuning the whole LLM.

2/3
------
A suggested technique to get the best of both worlds seems to be a 2-step process that 
1) trains the output layers first, and 
2) then finetunes the whole LLM.

Full paper here: https://arxiv.org/abs/2202.10054
------
I've just put together a roundup of 22 AI research paper highlights from April-May 2023! 

https://magazine.sebastianraschka.com/p/ai-research-highlights-in-3-sentences…

Lots of exciting things happening at an incredible pace!
------

------
For scale: The first Harry Potter book has 76,944 words, i.e ~100,000 tokens after tokenization.
------
Introducing 100K Context Windows! We’ve expanded Claude’s context window to 100,000 tokens of text, corresponding to around 75K words. Submit hundreds of pages of materials for Claude to digest and analyze. Conversations with Claude can go on for hours or days.
------
Mixed-precision training is one of the most effective, low-hanging fruits for getting more performance out of your LLMs. 

Wrote a new article covering the concepts behind float16, blfoat16, and mixed-precision for both finetuning LLMs & LLaMA inference: 
https://lightning.ai/pages/community/tutorial/accelerating-large-language-models-with-mixed-precision-techniques/…
------
If you are in NYC next week, feel free to stop by and and say Hi!
------
~800 strong now. 

May 19th. Come meet NYC’s community of people working on AI. 

https://partiful.com/e/AqsGTfRFmbCgIlMMnbEk…
------
Is this Google I/O or LLM Prompt Engineering 101?
------
Deep learning research 2012-2022: Our proposed method achieves SOTA accuracy on [image/text dataset].

Deep learning research 2023: "We review the cost associated with querying popular LLM APIs."

(Joking aside, it's an interesting paper, though: https://arxiv.org/abs/2305.05176)
------
There are lots of parameter-efficient finetuning techniques for LLM out there. But adapters is perhaps the simplest one to get started: insert additional fully-connected layers.

PS: Beyond transformers, it can be used for other architectures too!
------
How well does it work in practice? Performance-wise,  it falls somewhere in between only finetuning the output layer and finetuning all layers. But coz it's so simple to implement, it's a great intro & baseline before experimenting with other param.-efficient finetuning methods.
------
Location info here:
------
Which route will you take?

https://lightning.ai/pages/keep-ai-open-source/…
------
See, LLMs don’t magically get skills out of thin air, as some papers suggest.

This is a very nice paper taking a deep dive into one of them (translation skill) and it clearly comes from it being in the data.

I think that’s great and a good motivator for training on everything!
------
1.4% of PALM’s training instances are detected as bilingual, while 0.34% contain at least one translated sentence pair. We were able to mine such pairs across all languages studied; therefore, none of these languages is truly zero-shot in the context of translation.
------
Why can we use relatively vanilla gradient descent algos (e.g., w/o strong regularization) to tune billion-parameter LLMs on datasets with only thousands of labeled examples?

An interesting (older) paper that motivates the rationale behind LoRA etc: https://arxiv.org/abs/2012.13255

1/3
------
So, LLM pretraining implicitly minimizes intrinsic dimension of LLMs (the longer the lower the dim).

Then, adapting a small number of parameters (like optimizing only 200) a RoBERTa model achieves 90% of the full parameter performance.

2/3
------
More details in Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning: https://arxiv.org/abs/2012.13255

3/3
------
Excited to be launching a series of short videos (5 min or less) to optimize the training performance of your PyTorch models!

Covering
- mixed-precision training
- multi-GPU training strategies
- Compiling PyTorch models
- and finding good batch sizes

 https://lightning.ai/pages/courses/deep-learning-fundamentals/9.0-overview-techniques-for-speeding-up-model-training/…
------
That’s what’s https://magazine.sebastianraschka.com/p/ai-research-highlights-in-3-sentences… is for 
------
Good discussion of the current CNN (ConvNet) vs ViT landscape, including paper references and good practical advice from 
@giffmana
 & 
@wightmanr
------
What’s the status of vision transformers. Have these dethroned convents? Is that the trend? How do things look on the horizon?
------
Prof. Gilbert Strang gave his final lecture today!

End of an era. His linear algebra lectures where the best of the best .

His lectures are a treasure, and I hope they will remain accessible indefinitely: https://ocw.mit.edu/courses/18-06-linear-algebra-spring-2010/…
------
Prof. Gilbert Strang the original YouTuber, before there was a YouTube.  

Thank you for your many contributions!  twitter.com/CSProfKGD/stat…
------
Common ways to finetune LLMs are to 
a) update the output layers vs 
b) update more (or all) layers. 

Usually, the more parameters we update, the better for target-task performance (wrote about here: https://magazine.sebastianraschka.com/p/finetuning-large-language-models…). 

Small plot twist via https://arxiv.org/abs/2202.10054

1/3
------
It turns out that a), tuning only the output layer, can be better on out-of-distribution tasks. 

But this maybe expected due to the large number of parameters when finetuning the whole LLM.

2/3
------
A suggested technique to get the best of both worlds seems to be a 2-step process that 
1) trains the output layers first, and 
2) then finetunes the whole LLM.

Full paper here: https://arxiv.org/abs/2202.10054
------
I've just put together a roundup of 22 AI research paper highlights from April-May 2023! 

https://magazine.sebastianraschka.com/p/ai-research-highlights-in-3-sentences…

Lots of exciting things happening at an incredible pace!
------

------
For scale: The first Harry Potter book has 76,944 words, i.e ~100,000 tokens after tokenization.
------
Introducing 100K Context Windows! We’ve expanded Claude’s context window to 100,000 tokens of text, corresponding to around 75K words. Submit hundreds of pages of materials for Claude to digest and analyze. Conversations with Claude can go on for hours or days.
------
Mixed-precision training is one of the most effective, low-hanging fruits for getting more performance out of your LLMs. 

Wrote a new article covering the concepts behind float16, blfoat16, and mixed-precision for both finetuning LLMs & LLaMA inference: 
https://lightning.ai/pages/community/tutorial/accelerating-large-language-models-with-mixed-precision-techniques/…
------
If you are in NYC next week, feel free to stop by and and say Hi!
------
~800 strong now. 

May 19th. Come meet NYC’s community of people working on AI. 

https://partiful.com/e/AqsGTfRFmbCgIlMMnbEk…
------
Is this Google I/O or LLM Prompt Engineering 101?
------
Deep learning research 2012-2022: Our proposed method achieves SOTA accuracy on [image/text dataset].

Deep learning research 2023: "We review the cost associated with querying popular LLM APIs."

(Joking aside, it's an interesting paper, though: https://arxiv.org/abs/2305.05176)
------
Have been heads-down working on a lot of stuff since last summer, and I am looking forward to conferencing again this year!

Will be sharing my talk title for SDSC soon, although I guess you can already tell what it's gonna be 
------
 Get ready for a thrilling event on LLMs and machine learning in production! Join us in-person this year for three action-packed (Sept 5-7) days filled with cutting-edge technical talks , designed to empower you with the latest state-of-the-art knowledge and connect you with… Show more
------
Why can we use relatively vanilla gradient descent algos (e.g., w/o strong regularization) to tune billion-parameter LLMs on datasets with only thousands of labeled examples?

An interesting (older) paper that motivates the rationale behind LoRA etc: https://arxiv.org/abs/2012.13255

1/3
------
So, LLM pretraining implicitly minimizes intrinsic dimension of LLMs (the longer the lower the dim).

Then, adapting a small number of parameters (like optimizing only 200) a RoBERTa model achieves 90% of the full parameter performance.

2/3
------
More details in Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning: https://arxiv.org/abs/2012.13255

3/3
------
Excited to be launching a series of short videos (5 min or less) to optimize the training performance of your PyTorch models!

Covering
- mixed-precision training
- multi-GPU training strategies
- Compiling PyTorch models
- and finding good batch sizes

 https://lightning.ai/pages/courses/deep-learning-fundamentals/9.0-overview-techniques-for-speeding-up-model-training/…
------
That’s what’s https://magazine.sebastianraschka.com/p/ai-research-highlights-in-3-sentences… is for 
------
Good discussion of the current CNN (ConvNet) vs ViT landscape, including paper references and good practical advice from 
@giffmana
 & 
@wightmanr
------
What’s the status of vision transformers. Have these dethroned convents? Is that the trend? How do things look on the horizon?
------
Prof. Gilbert Strang gave his final lecture today!

End of an era. His linear algebra lectures where the best of the best .

His lectures are a treasure, and I hope they will remain accessible indefinitely: https://ocw.mit.edu/courses/18-06-linear-algebra-spring-2010/…
------
Prof. Gilbert Strang the original YouTuber, before there was a YouTube.  

Thank you for your many contributions!  twitter.com/CSProfKGD/stat…
------
Common ways to finetune LLMs are to 
a) update the output layers vs 
b) update more (or all) layers. 

Usually, the more parameters we update, the better for target-task performance (wrote about here: https://magazine.sebastianraschka.com/p/finetuning-large-language-models…). 

Small plot twist via https://arxiv.org/abs/2202.10054

1/3
------
It turns out that a), tuning only the output layer, can be better on out-of-distribution tasks. 

But this maybe expected due to the large number of parameters when finetuning the whole LLM.

2/3
------
A suggested technique to get the best of both worlds seems to be a 2-step process that 
1) trains the output layers first, and 
2) then finetunes the whole LLM.

Full paper here: https://arxiv.org/abs/2202.10054
------
I've just put together a roundup of 22 AI research paper highlights from April-May 2023! 

https://magazine.sebastianraschka.com/p/ai-research-highlights-in-3-sentences…

Lots of exciting things happening at an incredible pace!
------

------
For scale: The first Harry Potter book has 76,944 words, i.e ~100,000 tokens after tokenization.
------
Introducing 100K Context Windows! We’ve expanded Claude’s context window to 100,000 tokens of text, corresponding to around 75K words. Submit hundreds of pages of materials for Claude to digest and analyze. Conversations with Claude can go on for hours or days.
------
Mixed-precision training is one of the most effective, low-hanging fruits for getting more performance out of your LLMs. 

Wrote a new article covering the concepts behind float16, blfoat16, and mixed-precision for both finetuning LLMs & LLaMA inference: 
https://lightning.ai/pages/community/tutorial/accelerating-large-language-models-with-mixed-precision-techniques/…
------
If you are in NYC next week, feel free to stop by and and say Hi!
------
~800 strong now. 

May 19th. Come meet NYC’s community of people working on AI. 

https://partiful.com/e/AqsGTfRFmbCgIlMMnbEk…
------
Is this Google I/O or LLM Prompt Engineering 101?
------
Deep learning research 2012-2022: Our proposed method achieves SOTA accuracy on [image/text dataset].

Deep learning research 2023: "We review the cost associated with querying popular LLM APIs."

(Joking aside, it's an interesting paper, though: https://arxiv.org/abs/2305.05176)
------
Have been heads-down working on a lot of stuff since last summer, and I am looking forward to conferencing again this year!

Will be sharing my talk title for SDSC soon, although I guess you can already tell what it's gonna be 
------
 Get ready for a thrilling event on LLMs and machine learning in production! Join us in-person this year for three action-packed (Sept 5-7) days filled with cutting-edge technical talks , designed to empower you with the latest state-of-the-art knowledge and connect you with… Show more
------
Here we go again. CC BY-NC is NOT an open-source license, neither according to FSF[1,2] nor OSI[3]

See the very FIRST point of Open Source definition[4]

I like the paper tho

1 https://gnu.org/licenses/license-list.html#CC-BY-NC…
2 https://en.wikipedia.org/wiki/Comparison_of_free_and_open-source_software_licenses…
3 https://opensource.org/licenses/
4 https://en.wikipedia.org/wiki/The_Open_Source_Definition…
------
IMAGEBIND: One Embedding Space To Bind Them All.

Learns a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data.

An open source project by Meta-FAIR.

Paper: https://dl.fbaipublicfiles.com/imagebind/imagebind_final.pdf…
Demo: https://imagebind.metademolab.com
Code:… Show more
------
New repo to experiment with the GPT-NeoX family of models (Pythia, StableLM, and RedPajama-INCITE): 

https://github.com/Lightning-AI/lit-parrot…

via 
@lantiga
 
@adrianwaelchli
  & 
@carmocca


(If you liked Lit-LLaMA, this one is similarly structured and pretty accessible.)
------
This has already been shown in NormFormer with a less convoluted architecture (http://arxiv.org/abs/2110.09456), we have also confirmed their results recently (http://arxiv.org/abs/2303.09859). I'm surprised that the ResiDual paper doesn't mention NormFormer anywhere
------
Was just listening to the podcast this morning, and there are some interesting points by 
@lantiga
 regarding open-source LLMs and datasets:

- whether open-source datasets like Common Crawl are really open source is being reevaluated. 

1/3
------
As an entrepreneur or decision maker, understanding the impact of open source licensing is crucial. 

@_willfalcon and @lantiga break down licensing and commercial use for LLMs in their latest episode of The AI Buzz  https://youtube.com/watch?v=LXjddn2AvPA…

#LLMs #OpenSource #AI #ML
------
-  E.g., if you use RedPajama, The Pile, you need to do it at your own risk.

- All open-source models have been trained on the abovementioned (or similar) data, so the situation is not simple to navigate. There's a few big lawsuits out there right now 

2/3
------
- This might be the reason why LLaMA by Meta is licensed for research-only, which is probably less controversial than models compatible with commercial use.

- Meta's Segment Anything is fully open-source because Meta went ahead and licensed all these millions of images.

3/3
------
Thanks for the kind shoutout for my newsletter 
@HamelHusain
 ! I was also particularly flattered that it reached Bestseller status this weekend!

I'm grateful for the incredible encouragement from all of you! It's a fantastic motivation to continue creating & sharing!
------
The production quality and care that @rasbt has put into his courses and writing are next level.  His materials are becoming one of my favorite ML resources.  

His newsletter is especially great https://magazine.sebastianraschka.com
------
That’s what’s https://magazine.sebastianraschka.com/p/ai-research-highlights-in-3-sentences… is for 
------
Good discussion of the current CNN (ConvNet) vs ViT landscape, including paper references and good practical advice from 
@giffmana
 & 
@wightmanr
------
What’s the status of vision transformers. Have these dethroned convents? Is that the trend? How do things look on the horizon?
------
Prof. Gilbert Strang gave his final lecture today!

End of an era. His linear algebra lectures where the best of the best .

His lectures are a treasure, and I hope they will remain accessible indefinitely: https://ocw.mit.edu/courses/18-06-linear-algebra-spring-2010/…
------
Prof. Gilbert Strang the original YouTuber, before there was a YouTube.  

Thank you for your many contributions!  twitter.com/CSProfKGD/stat…
------
Common ways to finetune LLMs are to 
a) update the output layers vs 
b) update more (or all) layers. 

Usually, the more parameters we update, the better for target-task performance (wrote about here: https://magazine.sebastianraschka.com/p/finetuning-large-language-models…). 

Small plot twist via https://arxiv.org/abs/2202.10054

1/3
------
It turns out that a), tuning only the output layer, can be better on out-of-distribution tasks. 

But this maybe expected due to the large number of parameters when finetuning the whole LLM.

2/3
------
A suggested technique to get the best of both worlds seems to be a 2-step process that 
1) trains the output layers first, and 
2) then finetunes the whole LLM.

Full paper here: https://arxiv.org/abs/2202.10054
------
I've just put together a roundup of 22 AI research paper highlights from April-May 2023! 

https://magazine.sebastianraschka.com/p/ai-research-highlights-in-3-sentences…

Lots of exciting things happening at an incredible pace!
------

------
For scale: The first Harry Potter book has 76,944 words, i.e ~100,000 tokens after tokenization.
------
Introducing 100K Context Windows! We’ve expanded Claude’s context window to 100,000 tokens of text, corresponding to around 75K words. Submit hundreds of pages of materials for Claude to digest and analyze. Conversations with Claude can go on for hours or days.
------
Mixed-precision training is one of the most effective, low-hanging fruits for getting more performance out of your LLMs. 

Wrote a new article covering the concepts behind float16, blfoat16, and mixed-precision for both finetuning LLMs & LLaMA inference: 
https://lightning.ai/pages/community/tutorial/accelerating-large-language-models-with-mixed-precision-techniques/…
------
If you are in NYC next week, feel free to stop by and and say Hi!
------
~800 strong now. 

May 19th. Come meet NYC’s community of people working on AI. 

https://partiful.com/e/AqsGTfRFmbCgIlMMnbEk…
------
Is this Google I/O or LLM Prompt Engineering 101?
------
Deep learning research 2012-2022: Our proposed method achieves SOTA accuracy on [image/text dataset].

Deep learning research 2023: "We review the cost associated with querying popular LLM APIs."

(Joking aside, it's an interesting paper, though: https://arxiv.org/abs/2305.05176)
------
Have been heads-down working on a lot of stuff since last summer, and I am looking forward to conferencing again this year!

Will be sharing my talk title for SDSC soon, although I guess you can already tell what it's gonna be 
------
 Get ready for a thrilling event on LLMs and machine learning in production! Join us in-person this year for three action-packed (Sept 5-7) days filled with cutting-edge technical talks , designed to empower you with the latest state-of-the-art knowledge and connect you with… Show more
------
Here we go again. CC BY-NC is NOT an open-source license, neither according to FSF[1,2] nor OSI[3]

See the very FIRST point of Open Source definition[4]

I like the paper tho

1 https://gnu.org/licenses/license-list.html#CC-BY-NC…
2 https://en.wikipedia.org/wiki/Comparison_of_free_and_open-source_software_licenses…
3 https://opensource.org/licenses/
4 https://en.wikipedia.org/wiki/The_Open_Source_Definition…
------
IMAGEBIND: One Embedding Space To Bind Them All.

Learns a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data.

An open source project by Meta-FAIR.

Paper: https://dl.fbaipublicfiles.com/imagebind/imagebind_final.pdf…
Demo: https://imagebind.metademolab.com
Code:… Show more
------
New repo to experiment with the GPT-NeoX family of models (Pythia, StableLM, and RedPajama-INCITE): 

https://github.com/Lightning-AI/lit-parrot…

via 
@lantiga
 
@adrianwaelchli
  & 
@carmocca


(If you liked Lit-LLaMA, this one is similarly structured and pretty accessible.)
------
This has already been shown in NormFormer with a less convoluted architecture (http://arxiv.org/abs/2110.09456), we have also confirmed their results recently (http://arxiv.org/abs/2303.09859). I'm surprised that the ResiDual paper doesn't mention NormFormer anywhere
------
Was just listening to the podcast this morning, and there are some interesting points by 
@lantiga
 regarding open-source LLMs and datasets:

- whether open-source datasets like Common Crawl are really open source is being reevaluated. 

1/3
------
As an entrepreneur or decision maker, understanding the impact of open source licensing is crucial. 

@_willfalcon and @lantiga break down licensing and commercial use for LLMs in their latest episode of The AI Buzz  https://youtube.com/watch?v=LXjddn2AvPA…

#LLMs #OpenSource #AI #ML
------
-  E.g., if you use RedPajama, The Pile, you need to do it at your own risk.

- All open-source models have been trained on the abovementioned (or similar) data, so the situation is not simple to navigate. There's a few big lawsuits out there right now 

2/3
------
- This might be the reason why LLaMA by Meta is licensed for research-only, which is probably less controversial than models compatible with commercial use.

- Meta's Segment Anything is fully open-source because Meta went ahead and licensed all these millions of images.

3/3
------
Thanks for the kind shoutout for my newsletter 
@HamelHusain
 ! I was also particularly flattered that it reached Bestseller status this weekend!

I'm grateful for the incredible encouragement from all of you! It's a fantastic motivation to continue creating & sharing!
------
The production quality and care that @rasbt has put into his courses and writing are next level.  His materials are becoming one of my favorite ML resources.  

His newsletter is especially great https://magazine.sebastianraschka.com
------
Fun fact: did you know that the original Attention Is All Your Need transformer figure is wrong? 

It places the layer normalization between the residual blocks, which doesn't match the code: https://github.com/tensorflow/tensor2tensor/commit/f5c9b17e617ea9179b7d84d36b1e8162cb369f25#diff-76e2b94ef16871bdbf46bf04dfe7f1477bafb884748f08197c9cf1b10a4dd78e…

PS: This is known as Post-LN Transformer

1/3
------
The "Layer Normalization in the Transformer Architecture (https://arxiv.org/abs/2002.04745) paper  argued that Pre-LN works better, addressing gradient problems, as shown below.

This is what many/most(?) architectures adopted in practice, but it can result in representation collapse

2/3
------
So, while there's still an ongoing discussion regarding using Post-LN or Pre-LN, there's also a new paper that proposes taking advantage of both worlds: 
ResiDual: Transformer with Dual Residual Connections (https://arxiv.org/abs/2304.14802)

3/3
------
It turns out that a), tuning only the output layer, can be better on out-of-distribution tasks. 

But this maybe expected due to the large number of parameters when finetuning the whole LLM.

2/3
------
A suggested technique to get the best of both worlds seems to be a 2-step process that 
1) trains the output layers first, and 
2) then finetunes the whole LLM.

Full paper here: https://arxiv.org/abs/2202.10054
------
I've just put together a roundup of 22 AI research paper highlights from April-May 2023! 

https://magazine.sebastianraschka.com/p/ai-research-highlights-in-3-sentences…

Lots of exciting things happening at an incredible pace!
------

------
For scale: The first Harry Potter book has 76,944 words, i.e ~100,000 tokens after tokenization.
------
Introducing 100K Context Windows! We’ve expanded Claude’s context window to 100,000 tokens of text, corresponding to around 75K words. Submit hundreds of pages of materials for Claude to digest and analyze. Conversations with Claude can go on for hours or days.
------
Mixed-precision training is one of the most effective, low-hanging fruits for getting more performance out of your LLMs. 

Wrote a new article covering the concepts behind float16, blfoat16, and mixed-precision for both finetuning LLMs & LLaMA inference: 
https://lightning.ai/pages/community/tutorial/accelerating-large-language-models-with-mixed-precision-techniques/…
------
If you are in NYC next week, feel free to stop by and and say Hi!
------
~800 strong now. 

May 19th. Come meet NYC’s community of people working on AI. 

https://partiful.com/e/AqsGTfRFmbCgIlMMnbEk…
------
Is this Google I/O or LLM Prompt Engineering 101?
------
Deep learning research 2012-2022: Our proposed method achieves SOTA accuracy on [image/text dataset].

Deep learning research 2023: "We review the cost associated with querying popular LLM APIs."

(Joking aside, it's an interesting paper, though: https://arxiv.org/abs/2305.05176)
------
Have been heads-down working on a lot of stuff since last summer, and I am looking forward to conferencing again this year!

Will be sharing my talk title for SDSC soon, although I guess you can already tell what it's gonna be 
------
 Get ready for a thrilling event on LLMs and machine learning in production! Join us in-person this year for three action-packed (Sept 5-7) days filled with cutting-edge technical talks , designed to empower you with the latest state-of-the-art knowledge and connect you with… Show more
------
Here we go again. CC BY-NC is NOT an open-source license, neither according to FSF[1,2] nor OSI[3]

See the very FIRST point of Open Source definition[4]

I like the paper tho

1 https://gnu.org/licenses/license-list.html#CC-BY-NC…
2 https://en.wikipedia.org/wiki/Comparison_of_free_and_open-source_software_licenses…
3 https://opensource.org/licenses/
4 https://en.wikipedia.org/wiki/The_Open_Source_Definition…
------
IMAGEBIND: One Embedding Space To Bind Them All.

Learns a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data.

An open source project by Meta-FAIR.

Paper: https://dl.fbaipublicfiles.com/imagebind/imagebind_final.pdf…
Demo: https://imagebind.metademolab.com
Code:… Show more
------
New repo to experiment with the GPT-NeoX family of models (Pythia, StableLM, and RedPajama-INCITE): 

https://github.com/Lightning-AI/lit-parrot…

via 
@lantiga
 
@adrianwaelchli
  & 
@carmocca


(If you liked Lit-LLaMA, this one is similarly structured and pretty accessible.)
------
This has already been shown in NormFormer with a less convoluted architecture (http://arxiv.org/abs/2110.09456), we have also confirmed their results recently (http://arxiv.org/abs/2303.09859). I'm surprised that the ResiDual paper doesn't mention NormFormer anywhere
------
Was just listening to the podcast this morning, and there are some interesting points by 
@lantiga
 regarding open-source LLMs and datasets:

- whether open-source datasets like Common Crawl are really open source is being reevaluated. 

1/3
------
As an entrepreneur or decision maker, understanding the impact of open source licensing is crucial. 

@_willfalcon and @lantiga break down licensing and commercial use for LLMs in their latest episode of The AI Buzz  https://youtube.com/watch?v=LXjddn2AvPA…

#LLMs #OpenSource #AI #ML
------
-  E.g., if you use RedPajama, The Pile, you need to do it at your own risk.

- All open-source models have been trained on the abovementioned (or similar) data, so the situation is not simple to navigate. There's a few big lawsuits out there right now 

2/3
------
- This might be the reason why LLaMA by Meta is licensed for research-only, which is probably less controversial than models compatible with commercial use.

- Meta's Segment Anything is fully open-source because Meta went ahead and licensed all these millions of images.

3/3
------
Thanks for the kind shoutout for my newsletter 
@HamelHusain
 ! I was also particularly flattered that it reached Bestseller status this weekend!

I'm grateful for the incredible encouragement from all of you! It's a fantastic motivation to continue creating & sharing!
------
The production quality and care that @rasbt has put into his courses and writing are next level.  His materials are becoming one of my favorite ML resources.  

His newsletter is especially great https://magazine.sebastianraschka.com
------
Fun fact: did you know that the original Attention Is All Your Need transformer figure is wrong? 

It places the layer normalization between the residual blocks, which doesn't match the code: https://github.com/tensorflow/tensor2tensor/commit/f5c9b17e617ea9179b7d84d36b1e8162cb369f25#diff-76e2b94ef16871bdbf46bf04dfe7f1477bafb884748f08197c9cf1b10a4dd78e…

PS: This is known as Post-LN Transformer

1/3
------
The "Layer Normalization in the Transformer Architecture (https://arxiv.org/abs/2002.04745) paper  argued that Pre-LN works better, addressing gradient problems, as shown below.

This is what many/most(?) architectures adopted in practice, but it can result in representation collapse

2/3
------
So, while there's still an ongoing discussion regarding using Post-LN or Pre-LN, there's also a new paper that proposes taking advantage of both worlds: 
ResiDual: Transformer with Dual Residual Connections (https://arxiv.org/abs/2304.14802)

3/3
------
That's a nice & motivating start into the new week! 

Happy Monday!
------
@rasbt 
Your course is really amazing! I think it's the best introduction to dl I ever see. Would recommend it to a lot of friends definitely.
------
It was a great month for open source: So many LLMs came out that it's become quite overwhelming to keep track of it all.

So, in this month's Ahead of AI issue, I am sharing resources and research insights on the latest open-source LLMs & datasets!
------
I do think that finetuning and customizing LLMs is the way forward for many companies.

And that’s why open source is so useful and important.

A few more thoughts in the interview below.
------
Why Open Source Developers Are Using LLaMA, Meta’s AI Model https://bit.ly/3nsNQEJ @ricmac @rasbt #opensource #Developers #LLaMA #Meta #AI #MachineLearning
------
Do you have to finetune your daily writing assistant? Probably not.

Are you a bank already using ML to process loan applications? You probably have tons of data, and why wouldn’t you finetune your model on that vs using an off-the-shelf solution.
------
ChatGPT is basically the IBM Watson at this stage. Is absolutely useful for many use cases and companies. 
But there is a reason why so many companies and researchers build & use open source models based on PyTorch (and TensorFlow).
------
Launching a new unit on Large Language Modeling and Natural Language processing!

In this free 1.5 h lecture, I'll explain 
- the limitations of RNNs for long sequences
- self-attention from the ground up
- how LLMs work

https://lightning.ai/pages/courses/deep-learning-fundamentals/unit-8.0-natural-language-processing-and-large-language-models/…
------

------
For scale: The first Harry Potter book has 76,944 words, i.e ~100,000 tokens after tokenization.
------
Introducing 100K Context Windows! We’ve expanded Claude’s context window to 100,000 tokens of text, corresponding to around 75K words. Submit hundreds of pages of materials for Claude to digest and analyze. Conversations with Claude can go on for hours or days.
------
Mixed-precision training is one of the most effective, low-hanging fruits for getting more performance out of your LLMs. 

Wrote a new article covering the concepts behind float16, blfoat16, and mixed-precision for both finetuning LLMs & LLaMA inference: 
https://lightning.ai/pages/community/tutorial/accelerating-large-language-models-with-mixed-precision-techniques/…
------
If you are in NYC next week, feel free to stop by and and say Hi!
------
~800 strong now. 

May 19th. Come meet NYC’s community of people working on AI. 

https://partiful.com/e/AqsGTfRFmbCgIlMMnbEk…
------
Is this Google I/O or LLM Prompt Engineering 101?
------
Deep learning research 2012-2022: Our proposed method achieves SOTA accuracy on [image/text dataset].

Deep learning research 2023: "We review the cost associated with querying popular LLM APIs."

(Joking aside, it's an interesting paper, though: https://arxiv.org/abs/2305.05176)
------
Have been heads-down working on a lot of stuff since last summer, and I am looking forward to conferencing again this year!

Will be sharing my talk title for SDSC soon, although I guess you can already tell what it's gonna be 
------
 Get ready for a thrilling event on LLMs and machine learning in production! Join us in-person this year for three action-packed (Sept 5-7) days filled with cutting-edge technical talks , designed to empower you with the latest state-of-the-art knowledge and connect you with… Show more
------
Here we go again. CC BY-NC is NOT an open-source license, neither according to FSF[1,2] nor OSI[3]

See the very FIRST point of Open Source definition[4]

I like the paper tho

1 https://gnu.org/licenses/license-list.html#CC-BY-NC…
2 https://en.wikipedia.org/wiki/Comparison_of_free_and_open-source_software_licenses…
3 https://opensource.org/licenses/
4 https://en.wikipedia.org/wiki/The_Open_Source_Definition…
------
IMAGEBIND: One Embedding Space To Bind Them All.

Learns a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data.

An open source project by Meta-FAIR.

Paper: https://dl.fbaipublicfiles.com/imagebind/imagebind_final.pdf…
Demo: https://imagebind.metademolab.com
Code:… Show more
------
New repo to experiment with the GPT-NeoX family of models (Pythia, StableLM, and RedPajama-INCITE): 

https://github.com/Lightning-AI/lit-parrot…

via 
@lantiga
 
@adrianwaelchli
  & 
@carmocca


(If you liked Lit-LLaMA, this one is similarly structured and pretty accessible.)
------
This has already been shown in NormFormer with a less convoluted architecture (http://arxiv.org/abs/2110.09456), we have also confirmed their results recently (http://arxiv.org/abs/2303.09859). I'm surprised that the ResiDual paper doesn't mention NormFormer anywhere
------
Was just listening to the podcast this morning, and there are some interesting points by 
@lantiga
 regarding open-source LLMs and datasets:

- whether open-source datasets like Common Crawl are really open source is being reevaluated. 

1/3
------
As an entrepreneur or decision maker, understanding the impact of open source licensing is crucial. 

@_willfalcon and @lantiga break down licensing and commercial use for LLMs in their latest episode of The AI Buzz  https://youtube.com/watch?v=LXjddn2AvPA…

#LLMs #OpenSource #AI #ML
------
-  E.g., if you use RedPajama, The Pile, you need to do it at your own risk.

- All open-source models have been trained on the abovementioned (or similar) data, so the situation is not simple to navigate. There's a few big lawsuits out there right now 

2/3
------
- This might be the reason why LLaMA by Meta is licensed for research-only, which is probably less controversial than models compatible with commercial use.

- Meta's Segment Anything is fully open-source because Meta went ahead and licensed all these millions of images.

3/3
------
Thanks for the kind shoutout for my newsletter 
@HamelHusain
 ! I was also particularly flattered that it reached Bestseller status this weekend!

I'm grateful for the incredible encouragement from all of you! It's a fantastic motivation to continue creating & sharing!
------
The production quality and care that @rasbt has put into his courses and writing are next level.  His materials are becoming one of my favorite ML resources.  

His newsletter is especially great https://magazine.sebastianraschka.com
------
Fun fact: did you know that the original Attention Is All Your Need transformer figure is wrong? 

It places the layer normalization between the residual blocks, which doesn't match the code: https://github.com/tensorflow/tensor2tensor/commit/f5c9b17e617ea9179b7d84d36b1e8162cb369f25#diff-76e2b94ef16871bdbf46bf04dfe7f1477bafb884748f08197c9cf1b10a4dd78e…

PS: This is known as Post-LN Transformer

1/3
------
The "Layer Normalization in the Transformer Architecture (https://arxiv.org/abs/2002.04745) paper  argued that Pre-LN works better, addressing gradient problems, as shown below.

This is what many/most(?) architectures adopted in practice, but it can result in representation collapse

2/3
------
So, while there's still an ongoing discussion regarding using Post-LN or Pre-LN, there's also a new paper that proposes taking advantage of both worlds: 
ResiDual: Transformer with Dual Residual Connections (https://arxiv.org/abs/2304.14802)

3/3
------
That's a nice & motivating start into the new week! 

Happy Monday!
------
@rasbt 
Your course is really amazing! I think it's the best introduction to dl I ever see. Would recommend it to a lot of friends definitely.
------
It was a great month for open source: So many LLMs came out that it's become quite overwhelming to keep track of it all.

So, in this month's Ahead of AI issue, I am sharing resources and research insights on the latest open-source LLMs & datasets!
------
I do think that finetuning and customizing LLMs is the way forward for many companies.

And that’s why open source is so useful and important.

A few more thoughts in the interview below.
------
Why Open Source Developers Are Using LLaMA, Meta’s AI Model https://bit.ly/3nsNQEJ @ricmac @rasbt #opensource #Developers #LLaMA #Meta #AI #MachineLearning
------
Do you have to finetune your daily writing assistant? Probably not.

Are you a bank already using ML to process loan applications? You probably have tons of data, and why wouldn’t you finetune your model on that vs using an off-the-shelf solution.
------
ChatGPT is basically the IBM Watson at this stage. Is absolutely useful for many use cases and companies. 
But there is a reason why so many companies and researchers build & use open source models based on PyTorch (and TensorFlow).
------
Launching a new unit on Large Language Modeling and Natural Language processing!

In this free 1.5 h lecture, I'll explain 
- the limitations of RNNs for long sequences
- self-attention from the ground up
- how LLMs work

https://lightning.ai/pages/courses/deep-learning-fundamentals/unit-8.0-natural-language-processing-and-large-language-models/…
------
Regarding the 1.5 h, all videos are relatively short and bite-sized, of course. 
I.e., 20 short videos from embedding text to finetuning transformers.
------
Glad to share our new ACL paper. #ACL2023

A Simple but Useful Transformer Training System for Very Long Sequence 

Paper: https://arxiv.org/abs/2105.13120
Code: https://github.com/hpcaitech/ColossalAI…
Video: https://youtube.com/watch?v=HLLVKb7Cszs…

1/N
------
Looks like I have to amend my list and add number 5: Sequence parallelism as a new multi-GPU paradigm.
Sequence parallelism seems very useful for large-scale transformer training as it can take transformer inputs to new lengths (no pun intended).
https://arxiv.org/abs/2105.13120
[1/3]
------
"have you tried this new language? it's super slick and well designed and pleasant to use"

"when was it released?"

"late 2022"

"so chatgpt doesn't know about it?"

"no"

"so chatgpt can't help me with it?"

"I guess not"

"yeah, I'll stick with the old one"

Credit: 
@joelgrus
------
Nice demo of Mojo, a new Python-compatible language with a parallelizing compiler that can import Python libraries. twitter.com/jeremyphoward/…
------
Losing track of all the recent large language model (LLM) developments? 

Here's is another comprehensive LLM model zoo, available as table and dependency graph: https://crfm.stanford.edu/ecosystem-graphs/index.html?mode=table…
------
Have been heads-down working on a lot of stuff since last summer, and I am looking forward to conferencing again this year!

Will be sharing my talk title for SDSC soon, although I guess you can already tell what it's gonna be 
------
 Get ready for a thrilling event on LLMs and machine learning in production! Join us in-person this year for three action-packed (Sept 5-7) days filled with cutting-edge technical talks , designed to empower you with the latest state-of-the-art knowledge and connect you with… Show more
------
Here we go again. CC BY-NC is NOT an open-source license, neither according to FSF[1,2] nor OSI[3]

See the very FIRST point of Open Source definition[4]

I like the paper tho

1 https://gnu.org/licenses/license-list.html#CC-BY-NC…
2 https://en.wikipedia.org/wiki/Comparison_of_free_and_open-source_software_licenses…
3 https://opensource.org/licenses/
4 https://en.wikipedia.org/wiki/The_Open_Source_Definition…
------
IMAGEBIND: One Embedding Space To Bind Them All.

Learns a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data.

An open source project by Meta-FAIR.

Paper: https://dl.fbaipublicfiles.com/imagebind/imagebind_final.pdf…
Demo: https://imagebind.metademolab.com
Code:… Show more
------
New repo to experiment with the GPT-NeoX family of models (Pythia, StableLM, and RedPajama-INCITE): 

https://github.com/Lightning-AI/lit-parrot…

via 
@lantiga
 
@adrianwaelchli
  & 
@carmocca


(If you liked Lit-LLaMA, this one is similarly structured and pretty accessible.)
------
This has already been shown in NormFormer with a less convoluted architecture (http://arxiv.org/abs/2110.09456), we have also confirmed their results recently (http://arxiv.org/abs/2303.09859). I'm surprised that the ResiDual paper doesn't mention NormFormer anywhere
------
Was just listening to the podcast this morning, and there are some interesting points by 
@lantiga
 regarding open-source LLMs and datasets:

- whether open-source datasets like Common Crawl are really open source is being reevaluated. 

1/3
------
As an entrepreneur or decision maker, understanding the impact of open source licensing is crucial. 

@_willfalcon and @lantiga break down licensing and commercial use for LLMs in their latest episode of The AI Buzz  https://youtube.com/watch?v=LXjddn2AvPA…

#LLMs #OpenSource #AI #ML
------
-  E.g., if you use RedPajama, The Pile, you need to do it at your own risk.

- All open-source models have been trained on the abovementioned (or similar) data, so the situation is not simple to navigate. There's a few big lawsuits out there right now 

2/3
------
- This might be the reason why LLaMA by Meta is licensed for research-only, which is probably less controversial than models compatible with commercial use.

- Meta's Segment Anything is fully open-source because Meta went ahead and licensed all these millions of images.

3/3
------
Thanks for the kind shoutout for my newsletter 
@HamelHusain
 ! I was also particularly flattered that it reached Bestseller status this weekend!

I'm grateful for the incredible encouragement from all of you! It's a fantastic motivation to continue creating & sharing!
------
The production quality and care that @rasbt has put into his courses and writing are next level.  His materials are becoming one of my favorite ML resources.  

His newsletter is especially great https://magazine.sebastianraschka.com
------
Fun fact: did you know that the original Attention Is All Your Need transformer figure is wrong? 

It places the layer normalization between the residual blocks, which doesn't match the code: https://github.com/tensorflow/tensor2tensor/commit/f5c9b17e617ea9179b7d84d36b1e8162cb369f25#diff-76e2b94ef16871bdbf46bf04dfe7f1477bafb884748f08197c9cf1b10a4dd78e…

PS: This is known as Post-LN Transformer

1/3
------
The "Layer Normalization in the Transformer Architecture (https://arxiv.org/abs/2002.04745) paper  argued that Pre-LN works better, addressing gradient problems, as shown below.

This is what many/most(?) architectures adopted in practice, but it can result in representation collapse

2/3
------
So, while there's still an ongoing discussion regarding using Post-LN or Pre-LN, there's also a new paper that proposes taking advantage of both worlds: 
ResiDual: Transformer with Dual Residual Connections (https://arxiv.org/abs/2304.14802)

3/3
------
That's a nice & motivating start into the new week! 

Happy Monday!
------
@rasbt 
Your course is really amazing! I think it's the best introduction to dl I ever see. Would recommend it to a lot of friends definitely.
------
It was a great month for open source: So many LLMs came out that it's become quite overwhelming to keep track of it all.

So, in this month's Ahead of AI issue, I am sharing resources and research insights on the latest open-source LLMs & datasets!
------
I do think that finetuning and customizing LLMs is the way forward for many companies.

And that’s why open source is so useful and important.

A few more thoughts in the interview below.
------
Why Open Source Developers Are Using LLaMA, Meta’s AI Model https://bit.ly/3nsNQEJ @ricmac @rasbt #opensource #Developers #LLaMA #Meta #AI #MachineLearning
------
Do you have to finetune your daily writing assistant? Probably not.

Are you a bank already using ML to process loan applications? You probably have tons of data, and why wouldn’t you finetune your model on that vs using an off-the-shelf solution.
------
ChatGPT is basically the IBM Watson at this stage. Is absolutely useful for many use cases and companies. 
But there is a reason why so many companies and researchers build & use open source models based on PyTorch (and TensorFlow).
------
Launching a new unit on Large Language Modeling and Natural Language processing!

In this free 1.5 h lecture, I'll explain 
- the limitations of RNNs for long sequences
- self-attention from the ground up
- how LLMs work

https://lightning.ai/pages/courses/deep-learning-fundamentals/unit-8.0-natural-language-processing-and-large-language-models/…
------
Regarding the 1.5 h, all videos are relatively short and bite-sized, of course. 
I.e., 20 short videos from embedding text to finetuning transformers.
------
Glad to share our new ACL paper. #ACL2023

A Simple but Useful Transformer Training System for Very Long Sequence 

Paper: https://arxiv.org/abs/2105.13120
Code: https://github.com/hpcaitech/ColossalAI…
Video: https://youtube.com/watch?v=HLLVKb7Cszs…

1/N
------
Looks like I have to amend my list and add number 5: Sequence parallelism as a new multi-GPU paradigm.
Sequence parallelism seems very useful for large-scale transformer training as it can take transformer inputs to new lengths (no pun intended).
https://arxiv.org/abs/2105.13120
[1/3]
------
"have you tried this new language? it's super slick and well designed and pleasant to use"

"when was it released?"

"late 2022"

"so chatgpt doesn't know about it?"

"no"

"so chatgpt can't help me with it?"

"I guess not"

"yeah, I'll stick with the old one"

Credit: 
@joelgrus
------
Nice demo of Mojo, a new Python-compatible language with a parallelizing compiler that can import Python libraries. twitter.com/jeremyphoward/…
------
Losing track of all the recent large language model (LLM) developments? 

Here's is another comprehensive LLM model zoo, available as table and dependency graph: https://crfm.stanford.edu/ecosystem-graphs/index.html?mode=table…
------
This has already been shown in NormFormer with a less convoluted architecture (http://arxiv.org/abs/2110.09456), we have also confirmed their results recently (http://arxiv.org/abs/2303.09859). I'm surprised that the ResiDual paper doesn't mention NormFormer anywhere
------
Was just listening to the podcast this morning, and there are some interesting points by 
@lantiga
 regarding open-source LLMs and datasets:

- whether open-source datasets like Common Crawl are really open source is being reevaluated. 

1/3
------
As an entrepreneur or decision maker, understanding the impact of open source licensing is crucial. 

@_willfalcon and @lantiga break down licensing and commercial use for LLMs in their latest episode of The AI Buzz  https://youtube.com/watch?v=LXjddn2AvPA…

#LLMs #OpenSource #AI #ML
------
-  E.g., if you use RedPajama, The Pile, you need to do it at your own risk.

- All open-source models have been trained on the abovementioned (or similar) data, so the situation is not simple to navigate. There's a few big lawsuits out there right now 

2/3
------
- This might be the reason why LLaMA by Meta is licensed for research-only, which is probably less controversial than models compatible with commercial use.

- Meta's Segment Anything is fully open-source because Meta went ahead and licensed all these millions of images.

3/3
------
Thanks for the kind shoutout for my newsletter 
@HamelHusain
 ! I was also particularly flattered that it reached Bestseller status this weekend!

I'm grateful for the incredible encouragement from all of you! It's a fantastic motivation to continue creating & sharing!
------
The production quality and care that @rasbt has put into his courses and writing are next level.  His materials are becoming one of my favorite ML resources.  

His newsletter is especially great https://magazine.sebastianraschka.com
------
Fun fact: did you know that the original Attention Is All Your Need transformer figure is wrong? 

It places the layer normalization between the residual blocks, which doesn't match the code: https://github.com/tensorflow/tensor2tensor/commit/f5c9b17e617ea9179b7d84d36b1e8162cb369f25#diff-76e2b94ef16871bdbf46bf04dfe7f1477bafb884748f08197c9cf1b10a4dd78e…

PS: This is known as Post-LN Transformer

1/3
------
The "Layer Normalization in the Transformer Architecture (https://arxiv.org/abs/2002.04745) paper  argued that Pre-LN works better, addressing gradient problems, as shown below.

This is what many/most(?) architectures adopted in practice, but it can result in representation collapse

2/3
------
So, while there's still an ongoing discussion regarding using Post-LN or Pre-LN, there's also a new paper that proposes taking advantage of both worlds: 
ResiDual: Transformer with Dual Residual Connections (https://arxiv.org/abs/2304.14802)

3/3
------
That's a nice & motivating start into the new week! 

Happy Monday!
------
@rasbt 
Your course is really amazing! I think it's the best introduction to dl I ever see. Would recommend it to a lot of friends definitely.
------
It was a great month for open source: So many LLMs came out that it's become quite overwhelming to keep track of it all.

So, in this month's Ahead of AI issue, I am sharing resources and research insights on the latest open-source LLMs & datasets!
------
I do think that finetuning and customizing LLMs is the way forward for many companies.

And that’s why open source is so useful and important.

A few more thoughts in the interview below.
------
Why Open Source Developers Are Using LLaMA, Meta’s AI Model https://bit.ly/3nsNQEJ @ricmac @rasbt #opensource #Developers #LLaMA #Meta #AI #MachineLearning
------
Do you have to finetune your daily writing assistant? Probably not.

Are you a bank already using ML to process loan applications? You probably have tons of data, and why wouldn’t you finetune your model on that vs using an off-the-shelf solution.
------
ChatGPT is basically the IBM Watson at this stage. Is absolutely useful for many use cases and companies. 
But there is a reason why so many companies and researchers build & use open source models based on PyTorch (and TensorFlow).
------
Launching a new unit on Large Language Modeling and Natural Language processing!

In this free 1.5 h lecture, I'll explain 
- the limitations of RNNs for long sequences
- self-attention from the ground up
- how LLMs work

https://lightning.ai/pages/courses/deep-learning-fundamentals/unit-8.0-natural-language-processing-and-large-language-models/…
------
Regarding the 1.5 h, all videos are relatively short and bite-sized, of course. 
I.e., 20 short videos from embedding text to finetuning transformers.
------
Glad to share our new ACL paper. #ACL2023

A Simple but Useful Transformer Training System for Very Long Sequence 

Paper: https://arxiv.org/abs/2105.13120
Code: https://github.com/hpcaitech/ColossalAI…
Video: https://youtube.com/watch?v=HLLVKb7Cszs…

1/N
------
Looks like I have to amend my list and add number 5: Sequence parallelism as a new multi-GPU paradigm.
Sequence parallelism seems very useful for large-scale transformer training as it can take transformer inputs to new lengths (no pun intended).
https://arxiv.org/abs/2105.13120
[1/3]
------
"have you tried this new language? it's super slick and well designed and pleasant to use"

"when was it released?"

"late 2022"

"so chatgpt doesn't know about it?"

"no"

"so chatgpt can't help me with it?"

"I guess not"

"yeah, I'll stick with the old one"

Credit: 
@joelgrus
------
Nice demo of Mojo, a new Python-compatible language with a parallelizing compiler that can import Python libraries. twitter.com/jeremyphoward/…
------
Losing track of all the recent large language model (LLM) developments? 

Here's is another comprehensive LLM model zoo, available as table and dependency graph: https://crfm.stanford.edu/ecosystem-graphs/index.html?mode=table…
------
@DataCentric_AI
------
I think it has a pretty clear-cut definition already. There are of course many flavors of it, but in a nutshell it comes down to keeping the model training procedure fixed and changing the dataset to improve the model performance.

Figure from my ML Q and AI book:
------
Data-centric AI 
------
In the age of foundation models, designing the data pipeline is actually more important than tweaking the model architecture.

Here's a fun competition: curate an image-text dataset that yields high performance on downstream tasks, while keeping CLIP model & training *fixed*.
------
Remember LLaMA-Adapter as a nice parameter-efficient LLM finetuning last month?

Last month, I also predicted that we will be seing more multi-modal LLM models.

Here we go, let's look at the freshly released "LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model"!

1/10
------
PS: If you have any detailed questions about this paper, there's also a fireside chat with the authors tomorrow via 
@LightningAI
------
 LIVE TOMORROW!

We're chatting with the authors of LLaMA-Adapter and will also be demoing Lit LLaMA's reimplementation of their paper  Bring your questions!

RSVP to the Discord  https://discord.gg/7X5zCy5S?event=1101255415066538095…

LLaMA-Adapter V2 abs https://arxiv.org/abs/2304.15010

#LLMs #DeepLearning… Show more
------
And here are the references if you want to learn more.

LLaMA-Adapter V1: https://arxiv.org/abs/2303.16199

LLaMA-Adapter V2: https://arxiv.org/abs/2304.15010

10/10
------
CC 
@unsorsodicorda
------
Stop by and say Hi 
if you are in town and want to chat about open source & AI
------
Calling the AI community of NYC 

Come celebrate the power of open-source AI in our meetup with @StabilityAI!

May 19th | 6 PM
NYC | Secret Location

Show off your AI demos, be inspired and join the movement to democratize AI!

RSVP here https://partiful.com/e/AqsGTfRFmbCgIlMMnbEk…

#AI… Show more
------
Thanks for the kind shoutout for my newsletter 
@HamelHusain
 ! I was also particularly flattered that it reached Bestseller status this weekend!

I'm grateful for the incredible encouragement from all of you! It's a fantastic motivation to continue creating & sharing!
------
The production quality and care that @rasbt has put into his courses and writing are next level.  His materials are becoming one of my favorite ML resources.  

His newsletter is especially great https://magazine.sebastianraschka.com
------
Fun fact: did you know that the original Attention Is All Your Need transformer figure is wrong? 

It places the layer normalization between the residual blocks, which doesn't match the code: https://github.com/tensorflow/tensor2tensor/commit/f5c9b17e617ea9179b7d84d36b1e8162cb369f25#diff-76e2b94ef16871bdbf46bf04dfe7f1477bafb884748f08197c9cf1b10a4dd78e…

PS: This is known as Post-LN Transformer

1/3
------
The "Layer Normalization in the Transformer Architecture (https://arxiv.org/abs/2002.04745) paper  argued that Pre-LN works better, addressing gradient problems, as shown below.

This is what many/most(?) architectures adopted in practice, but it can result in representation collapse

2/3
------
So, while there's still an ongoing discussion regarding using Post-LN or Pre-LN, there's also a new paper that proposes taking advantage of both worlds: 
ResiDual: Transformer with Dual Residual Connections (https://arxiv.org/abs/2304.14802)

3/3
------
That's a nice & motivating start into the new week! 

Happy Monday!
------
@rasbt 
Your course is really amazing! I think it's the best introduction to dl I ever see. Would recommend it to a lot of friends definitely.
------
It was a great month for open source: So many LLMs came out that it's become quite overwhelming to keep track of it all.

So, in this month's Ahead of AI issue, I am sharing resources and research insights on the latest open-source LLMs & datasets!
------
I do think that finetuning and customizing LLMs is the way forward for many companies.

And that’s why open source is so useful and important.

A few more thoughts in the interview below.
------
Why Open Source Developers Are Using LLaMA, Meta’s AI Model https://bit.ly/3nsNQEJ @ricmac @rasbt #opensource #Developers #LLaMA #Meta #AI #MachineLearning
------
Do you have to finetune your daily writing assistant? Probably not.

Are you a bank already using ML to process loan applications? You probably have tons of data, and why wouldn’t you finetune your model on that vs using an off-the-shelf solution.
------
ChatGPT is basically the IBM Watson at this stage. Is absolutely useful for many use cases and companies. 
But there is a reason why so many companies and researchers build & use open source models based on PyTorch (and TensorFlow).
------
Launching a new unit on Large Language Modeling and Natural Language processing!

In this free 1.5 h lecture, I'll explain 
- the limitations of RNNs for long sequences
- self-attention from the ground up
- how LLMs work

https://lightning.ai/pages/courses/deep-learning-fundamentals/unit-8.0-natural-language-processing-and-large-language-models/…
------
Regarding the 1.5 h, all videos are relatively short and bite-sized, of course. 
I.e., 20 short videos from embedding text to finetuning transformers.
------
Glad to share our new ACL paper. #ACL2023

A Simple but Useful Transformer Training System for Very Long Sequence 

Paper: https://arxiv.org/abs/2105.13120
Code: https://github.com/hpcaitech/ColossalAI…
Video: https://youtube.com/watch?v=HLLVKb7Cszs…

1/N
------
Looks like I have to amend my list and add number 5: Sequence parallelism as a new multi-GPU paradigm.
Sequence parallelism seems very useful for large-scale transformer training as it can take transformer inputs to new lengths (no pun intended).
https://arxiv.org/abs/2105.13120
[1/3]
------
"have you tried this new language? it's super slick and well designed and pleasant to use"

"when was it released?"

"late 2022"

"so chatgpt doesn't know about it?"

"no"

"so chatgpt can't help me with it?"

"I guess not"

"yeah, I'll stick with the old one"

Credit: 
@joelgrus
------
Nice demo of Mojo, a new Python-compatible language with a parallelizing compiler that can import Python libraries. twitter.com/jeremyphoward/…
------
Losing track of all the recent large language model (LLM) developments? 

Here's is another comprehensive LLM model zoo, available as table and dependency graph: https://crfm.stanford.edu/ecosystem-graphs/index.html?mode=table…
------
@DataCentric_AI
------
I think it has a pretty clear-cut definition already. There are of course many flavors of it, but in a nutshell it comes down to keeping the model training procedure fixed and changing the dataset to improve the model performance.

Figure from my ML Q and AI book:
------
Data-centric AI 
------
In the age of foundation models, designing the data pipeline is actually more important than tweaking the model architecture.

Here's a fun competition: curate an image-text dataset that yields high performance on downstream tasks, while keeping CLIP model & training *fixed*.
------
Remember LLaMA-Adapter as a nice parameter-efficient LLM finetuning last month?

Last month, I also predicted that we will be seing more multi-modal LLM models.

Here we go, let's look at the freshly released "LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model"!

1/10
------
PS: If you have any detailed questions about this paper, there's also a fireside chat with the authors tomorrow via 
@LightningAI
------
 LIVE TOMORROW!

We're chatting with the authors of LLaMA-Adapter and will also be demoing Lit LLaMA's reimplementation of their paper  Bring your questions!

RSVP to the Discord  https://discord.gg/7X5zCy5S?event=1101255415066538095…

LLaMA-Adapter V2 abs https://arxiv.org/abs/2304.15010

#LLMs #DeepLearning… Show more
------
And here are the references if you want to learn more.

LLaMA-Adapter V1: https://arxiv.org/abs/2303.16199

LLaMA-Adapter V2: https://arxiv.org/abs/2304.15010

10/10
------
CC 
@unsorsodicorda
------
Stop by and say Hi 
if you are in town and want to chat about open source & AI
------
Calling the AI community of NYC 

Come celebrate the power of open-source AI in our meetup with @StabilityAI!

May 19th | 6 PM
NYC | Secret Location

Show off your AI demos, be inspired and join the movement to democratize AI!

RSVP here https://partiful.com/e/AqsGTfRFmbCgIlMMnbEk…

#AI… Show more
------
Yes, LLMs, are huge (no pun intended). 

Quoting a reviewer 

"The only thing I would change is to use LLM abbreviation. A lot of potential readers might skip [it] because of not knowing that large language models are LLMs."
------
When exactly became "LLM" such a trendy term?
------
In the NYT today, Cade Metz implies that I left Google so that I could criticize Google. Actually, I left so that I could talk about the dangers of AI without considering how this impacts Google. Google has acted very responsibly.
------
Generative AI is taking 2023 by storm. 
But generative AI has always been a thing since I was a little kid. 
Battles like the ones in Lord of the Rings movies 20 years ago have been generated by AI. 
It was just less talked about for some reason.

https://cnet.com/culture/entertainment/features/how-lord-of-the-rings-used-ai-to-change-big-screen-battles-forever/…
------
Also the fact that we can now have autonomous AI agents capable of planning etc.
Well, the video game AI in StarCraft 1998 was pretty autonomous to me even 25 years ago. And yes, it beat me somewhat consistently back then :P.
------
When discussing parameter-efficient LLM finetuning techniques, a question that often comes up is how prompt and prefix tuning are related. 

Here’s a short introduction and explanation.
------
So, while there's still an ongoing discussion regarding using Post-LN or Pre-LN, there's also a new paper that proposes taking advantage of both worlds: 
ResiDual: Transformer with Dual Residual Connections (https://arxiv.org/abs/2304.14802)

3/3
------
That's a nice & motivating start into the new week! 

Happy Monday!
------
@rasbt 
Your course is really amazing! I think it's the best introduction to dl I ever see. Would recommend it to a lot of friends definitely.
------
It was a great month for open source: So many LLMs came out that it's become quite overwhelming to keep track of it all.

So, in this month's Ahead of AI issue, I am sharing resources and research insights on the latest open-source LLMs & datasets!
------
I do think that finetuning and customizing LLMs is the way forward for many companies.

And that’s why open source is so useful and important.

A few more thoughts in the interview below.
------
Why Open Source Developers Are Using LLaMA, Meta’s AI Model https://bit.ly/3nsNQEJ @ricmac @rasbt #opensource #Developers #LLaMA #Meta #AI #MachineLearning
------
Do you have to finetune your daily writing assistant? Probably not.

Are you a bank already using ML to process loan applications? You probably have tons of data, and why wouldn’t you finetune your model on that vs using an off-the-shelf solution.
------
ChatGPT is basically the IBM Watson at this stage. Is absolutely useful for many use cases and companies. 
But there is a reason why so many companies and researchers build & use open source models based on PyTorch (and TensorFlow).
------
Launching a new unit on Large Language Modeling and Natural Language processing!

In this free 1.5 h lecture, I'll explain 
- the limitations of RNNs for long sequences
- self-attention from the ground up
- how LLMs work

https://lightning.ai/pages/courses/deep-learning-fundamentals/unit-8.0-natural-language-processing-and-large-language-models/…
------
Regarding the 1.5 h, all videos are relatively short and bite-sized, of course. 
I.e., 20 short videos from embedding text to finetuning transformers.
------
Glad to share our new ACL paper. #ACL2023

A Simple but Useful Transformer Training System for Very Long Sequence 

Paper: https://arxiv.org/abs/2105.13120
Code: https://github.com/hpcaitech/ColossalAI…
Video: https://youtube.com/watch?v=HLLVKb7Cszs…

1/N
------
Looks like I have to amend my list and add number 5: Sequence parallelism as a new multi-GPU paradigm.
Sequence parallelism seems very useful for large-scale transformer training as it can take transformer inputs to new lengths (no pun intended).
https://arxiv.org/abs/2105.13120
[1/3]
------
"have you tried this new language? it's super slick and well designed and pleasant to use"

"when was it released?"

"late 2022"

"so chatgpt doesn't know about it?"

"no"

"so chatgpt can't help me with it?"

"I guess not"

"yeah, I'll stick with the old one"

Credit: 
@joelgrus
------
Nice demo of Mojo, a new Python-compatible language with a parallelizing compiler that can import Python libraries. twitter.com/jeremyphoward/…
------
Losing track of all the recent large language model (LLM) developments? 

Here's is another comprehensive LLM model zoo, available as table and dependency graph: https://crfm.stanford.edu/ecosystem-graphs/index.html?mode=table…
------
@DataCentric_AI
------
I think it has a pretty clear-cut definition already. There are of course many flavors of it, but in a nutshell it comes down to keeping the model training procedure fixed and changing the dataset to improve the model performance.

Figure from my ML Q and AI book:
------
Data-centric AI 
------
In the age of foundation models, designing the data pipeline is actually more important than tweaking the model architecture.

Here's a fun competition: curate an image-text dataset that yields high performance on downstream tasks, while keeping CLIP model & training *fixed*.
------
Remember LLaMA-Adapter as a nice parameter-efficient LLM finetuning last month?

Last month, I also predicted that we will be seing more multi-modal LLM models.

Here we go, let's look at the freshly released "LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model"!

1/10
------
PS: If you have any detailed questions about this paper, there's also a fireside chat with the authors tomorrow via 
@LightningAI
------
 LIVE TOMORROW!

We're chatting with the authors of LLaMA-Adapter and will also be demoing Lit LLaMA's reimplementation of their paper  Bring your questions!

RSVP to the Discord  https://discord.gg/7X5zCy5S?event=1101255415066538095…

LLaMA-Adapter V2 abs https://arxiv.org/abs/2304.15010

#LLMs #DeepLearning… Show more
------
And here are the references if you want to learn more.

LLaMA-Adapter V1: https://arxiv.org/abs/2303.16199

LLaMA-Adapter V2: https://arxiv.org/abs/2304.15010

10/10
------
CC 
@unsorsodicorda
------
Stop by and say Hi 
if you are in town and want to chat about open source & AI
------
Calling the AI community of NYC 

Come celebrate the power of open-source AI in our meetup with @StabilityAI!

May 19th | 6 PM
NYC | Secret Location

Show off your AI demos, be inspired and join the movement to democratize AI!

RSVP here https://partiful.com/e/AqsGTfRFmbCgIlMMnbEk…

#AI… Show more
------
Yes, LLMs, are huge (no pun intended). 

Quoting a reviewer 

"The only thing I would change is to use LLM abbreviation. A lot of potential readers might skip [it] because of not knowing that large language models are LLMs."
------
When exactly became "LLM" such a trendy term?
------
In the NYT today, Cade Metz implies that I left Google so that I could criticize Google. Actually, I left so that I could talk about the dangers of AI without considering how this impacts Google. Google has acted very responsibly.
------
Generative AI is taking 2023 by storm. 
But generative AI has always been a thing since I was a little kid. 
Battles like the ones in Lord of the Rings movies 20 years ago have been generated by AI. 
It was just less talked about for some reason.

https://cnet.com/culture/entertainment/features/how-lord-of-the-rings-used-ai-to-change-big-screen-battles-forever/…
------
Also the fact that we can now have autonomous AI agents capable of planning etc.
Well, the video game AI in StarCraft 1998 was pretty autonomous to me even 25 years ago. And yes, it beat me somewhat consistently back then :P.
------
When discussing parameter-efficient LLM finetuning techniques, a question that often comes up is how prompt and prefix tuning are related. 

Here’s a short introduction and explanation.
------
So many LLMs so little time. 
A handy overview of the main ones and their respective licenses, courtesy of my colleague 
@DaniDapena
------
It's like we're on a quest for the Holy Grail, but instead of a cup, we're searching for the best LLM...

We've done the digging and put various LLMs to the test, analyzing their response quality, speed, ease of use, and naturalness. 

Check out what we discovered… Show more
------
CC 
@unsorsodicorda
------
Nvidia open-sourced a toolkit to address the hallucination issue (/capabilities) of LLMs called NeMo Guardrails. In a nutshell, how it works is that this method uses a database linking to hardcoded prompts, which have to be manually curated.

Repo: https://github.com/NVIDIA/NeMo-Guardrails…

1/4
------
So, if someone carefully tests the hardcoded prompts, one can ensure that the interaction does not diverge from permissible topics and so on.

3/4
------
It's an interesting but not groundbreaking approach as it does not give the LLM better or new capabilities. It merely restricts how far a user can interact with an LLM. Still, it might be a viable bandaid before researchers find alternative ways for hallucination mitigation. 
4/4
------
While everybody is making fun about sudden loss drop, let me tell you what it ACTUALLY means when that happens:

Your training run had a pre-emption and got restarted. Your input pipeline isn’t perfect and now partially going through examples you’ve already seen recently

No foom
------
Possible but hardly inevitable.  It becomes moderately more likely as people call it absurd and fail to take precautions against it, like checking for sudden drops in the loss function and suspending training.  Mostly, though, this is not a necessary postulate of a doom story. twitter.com/perrymetzger/s…
------
Launching a new unit on Large Language Modeling and Natural Language processing!

In this free 1.5 h lecture, I'll explain 
- the limitations of RNNs for long sequences
- self-attention from the ground up
- how LLMs work

https://lightning.ai/pages/courses/deep-learning-fundamentals/unit-8.0-natural-language-processing-and-large-language-models/…
------
Regarding the 1.5 h, all videos are relatively short and bite-sized, of course. 
I.e., 20 short videos from embedding text to finetuning transformers.
------
Glad to share our new ACL paper. #ACL2023

A Simple but Useful Transformer Training System for Very Long Sequence 

Paper: https://arxiv.org/abs/2105.13120
Code: https://github.com/hpcaitech/ColossalAI…
Video: https://youtube.com/watch?v=HLLVKb7Cszs…

1/N
------
Looks like I have to amend my list and add number 5: Sequence parallelism as a new multi-GPU paradigm.
Sequence parallelism seems very useful for large-scale transformer training as it can take transformer inputs to new lengths (no pun intended).
https://arxiv.org/abs/2105.13120
[1/3]
------
"have you tried this new language? it's super slick and well designed and pleasant to use"

"when was it released?"

"late 2022"

"so chatgpt doesn't know about it?"

"no"

"so chatgpt can't help me with it?"

"I guess not"

"yeah, I'll stick with the old one"

Credit: 
@joelgrus
------
Nice demo of Mojo, a new Python-compatible language with a parallelizing compiler that can import Python libraries. twitter.com/jeremyphoward/…
------
Losing track of all the recent large language model (LLM) developments? 

Here's is another comprehensive LLM model zoo, available as table and dependency graph: https://crfm.stanford.edu/ecosystem-graphs/index.html?mode=table…
------
@DataCentric_AI
------
I think it has a pretty clear-cut definition already. There are of course many flavors of it, but in a nutshell it comes down to keeping the model training procedure fixed and changing the dataset to improve the model performance.

Figure from my ML Q and AI book:
------
Data-centric AI 
------
In the age of foundation models, designing the data pipeline is actually more important than tweaking the model architecture.

Here's a fun competition: curate an image-text dataset that yields high performance on downstream tasks, while keeping CLIP model & training *fixed*.
------
Remember LLaMA-Adapter as a nice parameter-efficient LLM finetuning last month?

Last month, I also predicted that we will be seing more multi-modal LLM models.

Here we go, let's look at the freshly released "LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model"!

1/10
------
PS: If you have any detailed questions about this paper, there's also a fireside chat with the authors tomorrow via 
@LightningAI
------
 LIVE TOMORROW!

We're chatting with the authors of LLaMA-Adapter and will also be demoing Lit LLaMA's reimplementation of their paper  Bring your questions!

RSVP to the Discord  https://discord.gg/7X5zCy5S?event=1101255415066538095…

LLaMA-Adapter V2 abs https://arxiv.org/abs/2304.15010

#LLMs #DeepLearning… Show more
------
And here are the references if you want to learn more.

LLaMA-Adapter V1: https://arxiv.org/abs/2303.16199

LLaMA-Adapter V2: https://arxiv.org/abs/2304.15010

10/10
------
CC 
@unsorsodicorda
------
Stop by and say Hi 
if you are in town and want to chat about open source & AI
------
Calling the AI community of NYC 

Come celebrate the power of open-source AI in our meetup with @StabilityAI!

May 19th | 6 PM
NYC | Secret Location

Show off your AI demos, be inspired and join the movement to democratize AI!

RSVP here https://partiful.com/e/AqsGTfRFmbCgIlMMnbEk…

#AI… Show more
------
Yes, LLMs, are huge (no pun intended). 

Quoting a reviewer 

"The only thing I would change is to use LLM abbreviation. A lot of potential readers might skip [it] because of not knowing that large language models are LLMs."
------
When exactly became "LLM" such a trendy term?
------
In the NYT today, Cade Metz implies that I left Google so that I could criticize Google. Actually, I left so that I could talk about the dangers of AI without considering how this impacts Google. Google has acted very responsibly.
------
Generative AI is taking 2023 by storm. 
But generative AI has always been a thing since I was a little kid. 
Battles like the ones in Lord of the Rings movies 20 years ago have been generated by AI. 
It was just less talked about for some reason.

https://cnet.com/culture/entertainment/features/how-lord-of-the-rings-used-ai-to-change-big-screen-battles-forever/…
------
Also the fact that we can now have autonomous AI agents capable of planning etc.
Well, the video game AI in StarCraft 1998 was pretty autonomous to me even 25 years ago. And yes, it beat me somewhat consistently back then :P.
------
When discussing parameter-efficient LLM finetuning techniques, a question that often comes up is how prompt and prefix tuning are related. 

Here’s a short introduction and explanation.
------
So many LLMs so little time. 
A handy overview of the main ones and their respective licenses, courtesy of my colleague 
@DaniDapena
------
It's like we're on a quest for the Holy Grail, but instead of a cup, we're searching for the best LLM...

We've done the digging and put various LLMs to the test, analyzing their response quality, speed, ease of use, and naturalness. 

Check out what we discovered… Show more
------
CC 
@unsorsodicorda
------
Nvidia open-sourced a toolkit to address the hallucination issue (/capabilities) of LLMs called NeMo Guardrails. In a nutshell, how it works is that this method uses a database linking to hardcoded prompts, which have to be manually curated.

Repo: https://github.com/NVIDIA/NeMo-Guardrails…

1/4
------
So, if someone carefully tests the hardcoded prompts, one can ensure that the interaction does not diverge from permissible topics and so on.

3/4
------
It's an interesting but not groundbreaking approach as it does not give the LLM better or new capabilities. It merely restricts how far a user can interact with an LLM. Still, it might be a viable bandaid before researchers find alternative ways for hallucination mitigation. 
4/4
------
While everybody is making fun about sudden loss drop, let me tell you what it ACTUALLY means when that happens:

Your training run had a pre-emption and got restarted. Your input pipeline isn’t perfect and now partially going through examples you’ve already seen recently

No foom
------
Possible but hardly inevitable.  It becomes moderately more likely as people call it absurd and fail to take precautions against it, like checking for sudden drops in the loss function and suspending training.  Mostly, though, this is not a necessary postulate of a doom story. twitter.com/perrymetzger/s…
------
I think finetuning LLMs for custom targets and tasks will be the way forward. 

But the quality of the finetuned model hinges upon the quality of the pretrained base model.

We are working on federated training strategies to make such pretrained LLMs available as open source.
------
Help train AI Models for everyone, not just gatekeepers   

We're calling on individuals and institutions with spare compute capacity who want a world where AI is open and used for the benefit of all. 
Join us  https://lightning.ai/pages/lit/

#AI #OpenSource
------
Thanks to parameter-efficient finetuning techniques, you can finetune a 7B LLM on a single GPU in 1-2 h using techniques like low-rank adaptation (LoRA).

Just wrote a new article explaining how LoRA works & how to finetune a pretrained LLM like LLaMA:
------
Instruction finetuning is how we get from GPT-3-like pretrained base models to more capable LLMs like ChatGPT. This requires human-generated instruction data like databricks-dolly-15k.
So, how do we scale this? One way is bootstrapping an LLM off its own generations.

1/7
------
Losing track of all the recent large language model (LLM) developments? 

Here's is another comprehensive LLM model zoo, available as table and dependency graph: https://crfm.stanford.edu/ecosystem-graphs/index.html?mode=table…
------
@DataCentric_AI
------
I think it has a pretty clear-cut definition already. There are of course many flavors of it, but in a nutshell it comes down to keeping the model training procedure fixed and changing the dataset to improve the model performance.

Figure from my ML Q and AI book:
------
Data-centric AI 
------
In the age of foundation models, designing the data pipeline is actually more important than tweaking the model architecture.

Here's a fun competition: curate an image-text dataset that yields high performance on downstream tasks, while keeping CLIP model & training *fixed*.
------
Remember LLaMA-Adapter as a nice parameter-efficient LLM finetuning last month?

Last month, I also predicted that we will be seing more multi-modal LLM models.

Here we go, let's look at the freshly released "LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model"!

1/10
------
PS: If you have any detailed questions about this paper, there's also a fireside chat with the authors tomorrow via 
@LightningAI
------
 LIVE TOMORROW!

We're chatting with the authors of LLaMA-Adapter and will also be demoing Lit LLaMA's reimplementation of their paper  Bring your questions!

RSVP to the Discord  https://discord.gg/7X5zCy5S?event=1101255415066538095…

LLaMA-Adapter V2 abs https://arxiv.org/abs/2304.15010

#LLMs #DeepLearning… Show more
------
And here are the references if you want to learn more.

LLaMA-Adapter V1: https://arxiv.org/abs/2303.16199

LLaMA-Adapter V2: https://arxiv.org/abs/2304.15010

10/10
------
CC 
@unsorsodicorda
------
Stop by and say Hi 
if you are in town and want to chat about open source & AI
------
Calling the AI community of NYC 

Come celebrate the power of open-source AI in our meetup with @StabilityAI!

May 19th | 6 PM
NYC | Secret Location

Show off your AI demos, be inspired and join the movement to democratize AI!

RSVP here https://partiful.com/e/AqsGTfRFmbCgIlMMnbEk…

#AI… Show more
------
Yes, LLMs, are huge (no pun intended). 

Quoting a reviewer 

"The only thing I would change is to use LLM abbreviation. A lot of potential readers might skip [it] because of not knowing that large language models are LLMs."
------
When exactly became "LLM" such a trendy term?
------
In the NYT today, Cade Metz implies that I left Google so that I could criticize Google. Actually, I left so that I could talk about the dangers of AI without considering how this impacts Google. Google has acted very responsibly.
------
Generative AI is taking 2023 by storm. 
But generative AI has always been a thing since I was a little kid. 
Battles like the ones in Lord of the Rings movies 20 years ago have been generated by AI. 
It was just less talked about for some reason.

https://cnet.com/culture/entertainment/features/how-lord-of-the-rings-used-ai-to-change-big-screen-battles-forever/…
------
Also the fact that we can now have autonomous AI agents capable of planning etc.
Well, the video game AI in StarCraft 1998 was pretty autonomous to me even 25 years ago. And yes, it beat me somewhat consistently back then :P.
------
When discussing parameter-efficient LLM finetuning techniques, a question that often comes up is how prompt and prefix tuning are related. 

Here’s a short introduction and explanation.
------
So many LLMs so little time. 
A handy overview of the main ones and their respective licenses, courtesy of my colleague 
@DaniDapena
------
It's like we're on a quest for the Holy Grail, but instead of a cup, we're searching for the best LLM...

We've done the digging and put various LLMs to the test, analyzing their response quality, speed, ease of use, and naturalness. 

Check out what we discovered… Show more
------
CC 
@unsorsodicorda
------
Nvidia open-sourced a toolkit to address the hallucination issue (/capabilities) of LLMs called NeMo Guardrails. In a nutshell, how it works is that this method uses a database linking to hardcoded prompts, which have to be manually curated.

Repo: https://github.com/NVIDIA/NeMo-Guardrails…

1/4
------
So, if someone carefully tests the hardcoded prompts, one can ensure that the interaction does not diverge from permissible topics and so on.

3/4
------
It's an interesting but not groundbreaking approach as it does not give the LLM better or new capabilities. It merely restricts how far a user can interact with an LLM. Still, it might be a viable bandaid before researchers find alternative ways for hallucination mitigation. 
4/4
------
While everybody is making fun about sudden loss drop, let me tell you what it ACTUALLY means when that happens:

Your training run had a pre-emption and got restarted. Your input pipeline isn’t perfect and now partially going through examples you’ve already seen recently

No foom
------
Possible but hardly inevitable.  It becomes moderately more likely as people call it absurd and fail to take precautions against it, like checking for sudden drops in the loss function and suspending training.  Mostly, though, this is not a necessary postulate of a doom story. twitter.com/perrymetzger/s…
------
I think finetuning LLMs for custom targets and tasks will be the way forward. 

But the quality of the finetuned model hinges upon the quality of the pretrained base model.

We are working on federated training strategies to make such pretrained LLMs available as open source.
------
Help train AI Models for everyone, not just gatekeepers   

We're calling on individuals and institutions with spare compute capacity who want a world where AI is open and used for the benefit of all. 
Join us  https://lightning.ai/pages/lit/

#AI #OpenSource
------
Thanks to parameter-efficient finetuning techniques, you can finetune a 7B LLM on a single GPU in 1-2 h using techniques like low-rank adaptation (LoRA).

Just wrote a new article explaining how LoRA works & how to finetune a pretrained LLM like LLaMA:
------
Instruction finetuning is how we get from GPT-3-like pretrained base models to more capable LLMs like ChatGPT. This requires human-generated instruction data like databricks-dolly-15k.
So, how do we scale this? One way is bootstrapping an LLM off its own generations.

1/7
------
Which is more promising, human-generated instruction datasets or self-instruct-datasets? I vote for both. Why not start with a human-generated instruction dataset like the 15k instructions from databricks-dolly-15k and then scale this with self-instruct?

6/7
------
Here's a link to the SelfInstruct paper if you are interested in additional details: https://arxiv.org/abs/2212.10560

7/7
------
MLOps is dead, long live LMOps
------
In my recent article on LLM finetuning, I discussed the two extreme scenarios: (1) finetuning the output layers vs (2) finetuning all layers.

But what about the in-between? Added some additional results to the article. Turns out, tuning a few transformer blocks can be sufficient
------
Want to adopt the latest LLMs for a specific target domain (e.g., finance data) or target task like document classification?

Finetuning becomes more feasible as we see more and more pretrained LLMs become available under open-source licenses.

https://magazine.sebastianraschka.com/p/finetuning-large-language-models…

Here's an… Show more
------
Want to adopt the latest LLMs for a specific target domain (e.g., finance data) or target task like document classification?

Finetuning becomes more feasible as we see more and more pretrained LLMs become available under open-source licenses.

https://magazine.sebastianraschka.com/p/finetuning-large-language-models…

Here's an… Show more
------
Remember LLaMA-Adapter as a nice parameter-efficient LLM finetuning last month?

Last month, I also predicted that we will be seing more multi-modal LLM models.

Here we go, let's look at the freshly released "LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model"!

1/10
------
PS: If you have any detailed questions about this paper, there's also a fireside chat with the authors tomorrow via 
@LightningAI
------
 LIVE TOMORROW!

We're chatting with the authors of LLaMA-Adapter and will also be demoing Lit LLaMA's reimplementation of their paper  Bring your questions!

RSVP to the Discord  https://discord.gg/7X5zCy5S?event=1101255415066538095…

LLaMA-Adapter V2 abs https://arxiv.org/abs/2304.15010

#LLMs #DeepLearning… Show more
------
And here are the references if you want to learn more.

LLaMA-Adapter V1: https://arxiv.org/abs/2303.16199

LLaMA-Adapter V2: https://arxiv.org/abs/2304.15010

10/10
------
CC 
@unsorsodicorda
------
Stop by and say Hi 
if you are in town and want to chat about open source & AI
------
Calling the AI community of NYC 

Come celebrate the power of open-source AI in our meetup with @StabilityAI!

May 19th | 6 PM
NYC | Secret Location

Show off your AI demos, be inspired and join the movement to democratize AI!

RSVP here https://partiful.com/e/AqsGTfRFmbCgIlMMnbEk…

#AI… Show more
------
Yes, LLMs, are huge (no pun intended). 

Quoting a reviewer 

"The only thing I would change is to use LLM abbreviation. A lot of potential readers might skip [it] because of not knowing that large language models are LLMs."
------
When exactly became "LLM" such a trendy term?
------
In the NYT today, Cade Metz implies that I left Google so that I could criticize Google. Actually, I left so that I could talk about the dangers of AI without considering how this impacts Google. Google has acted very responsibly.
------
Generative AI is taking 2023 by storm. 
But generative AI has always been a thing since I was a little kid. 
Battles like the ones in Lord of the Rings movies 20 years ago have been generated by AI. 
It was just less talked about for some reason.

https://cnet.com/culture/entertainment/features/how-lord-of-the-rings-used-ai-to-change-big-screen-battles-forever/…
------
Also the fact that we can now have autonomous AI agents capable of planning etc.
Well, the video game AI in StarCraft 1998 was pretty autonomous to me even 25 years ago. And yes, it beat me somewhat consistently back then :P.
------
When discussing parameter-efficient LLM finetuning techniques, a question that often comes up is how prompt and prefix tuning are related. 

Here’s a short introduction and explanation.
------
So many LLMs so little time. 
A handy overview of the main ones and their respective licenses, courtesy of my colleague 
@DaniDapena
------
It's like we're on a quest for the Holy Grail, but instead of a cup, we're searching for the best LLM...

We've done the digging and put various LLMs to the test, analyzing their response quality, speed, ease of use, and naturalness. 

Check out what we discovered… Show more
------
CC 
@unsorsodicorda
------
Nvidia open-sourced a toolkit to address the hallucination issue (/capabilities) of LLMs called NeMo Guardrails. In a nutshell, how it works is that this method uses a database linking to hardcoded prompts, which have to be manually curated.

Repo: https://github.com/NVIDIA/NeMo-Guardrails…

1/4
------
So, if someone carefully tests the hardcoded prompts, one can ensure that the interaction does not diverge from permissible topics and so on.

3/4
------
It's an interesting but not groundbreaking approach as it does not give the LLM better or new capabilities. It merely restricts how far a user can interact with an LLM. Still, it might be a viable bandaid before researchers find alternative ways for hallucination mitigation. 
4/4
------
While everybody is making fun about sudden loss drop, let me tell you what it ACTUALLY means when that happens:

Your training run had a pre-emption and got restarted. Your input pipeline isn’t perfect and now partially going through examples you’ve already seen recently

No foom
------
Possible but hardly inevitable.  It becomes moderately more likely as people call it absurd and fail to take precautions against it, like checking for sudden drops in the loss function and suspending training.  Mostly, though, this is not a necessary postulate of a doom story. twitter.com/perrymetzger/s…
------
I think finetuning LLMs for custom targets and tasks will be the way forward. 

But the quality of the finetuned model hinges upon the quality of the pretrained base model.

We are working on federated training strategies to make such pretrained LLMs available as open source.
------
Help train AI Models for everyone, not just gatekeepers   

We're calling on individuals and institutions with spare compute capacity who want a world where AI is open and used for the benefit of all. 
Join us  https://lightning.ai/pages/lit/

#AI #OpenSource
------
Thanks to parameter-efficient finetuning techniques, you can finetune a 7B LLM on a single GPU in 1-2 h using techniques like low-rank adaptation (LoRA).

Just wrote a new article explaining how LoRA works & how to finetune a pretrained LLM like LLaMA:
------
Instruction finetuning is how we get from GPT-3-like pretrained base models to more capable LLMs like ChatGPT. This requires human-generated instruction data like databricks-dolly-15k.
So, how do we scale this? One way is bootstrapping an LLM off its own generations.

1/7
------
Which is more promising, human-generated instruction datasets or self-instruct-datasets? I vote for both. Why not start with a human-generated instruction dataset like the 15k instructions from databricks-dolly-15k and then scale this with self-instruct?

6/7
------
Here's a link to the SelfInstruct paper if you are interested in additional details: https://arxiv.org/abs/2212.10560

7/7
------
MLOps is dead, long live LMOps
------
In my recent article on LLM finetuning, I discussed the two extreme scenarios: (1) finetuning the output layers vs (2) finetuning all layers.

But what about the in-between? Added some additional results to the article. Turns out, tuning a few transformer blocks can be sufficient
------
Want to adopt the latest LLMs for a specific target domain (e.g., finance data) or target task like document classification?

Finetuning becomes more feasible as we see more and more pretrained LLMs become available under open-source licenses.

https://magazine.sebastianraschka.com/p/finetuning-large-language-models…

Here's an… Show more
------
Want to adopt the latest LLMs for a specific target domain (e.g., finance data) or target task like document classification?

Finetuning becomes more feasible as we see more and more pretrained LLMs become available under open-source licenses.

https://magazine.sebastianraschka.com/p/finetuning-large-language-models…

Here's an… Show more
------
Stop by and say Hi 
if you are in town and want to chat about open source & AI
------
Calling the AI community of NYC 

Come celebrate the power of open-source AI in our meetup with @StabilityAI!

May 19th | 6 PM
NYC | Secret Location

Show off your AI demos, be inspired and join the movement to democratize AI!

RSVP here https://partiful.com/e/AqsGTfRFmbCgIlMMnbEk…

#AI… Show more
------
Yes, LLMs, are huge (no pun intended). 

Quoting a reviewer 

"The only thing I would change is to use LLM abbreviation. A lot of potential readers might skip [it] because of not knowing that large language models are LLMs."
------
When exactly became "LLM" such a trendy term?
------
In the NYT today, Cade Metz implies that I left Google so that I could criticize Google. Actually, I left so that I could talk about the dangers of AI without considering how this impacts Google. Google has acted very responsibly.
------
Generative AI is taking 2023 by storm. 
But generative AI has always been a thing since I was a little kid. 
Battles like the ones in Lord of the Rings movies 20 years ago have been generated by AI. 
It was just less talked about for some reason.

https://cnet.com/culture/entertainment/features/how-lord-of-the-rings-used-ai-to-change-big-screen-battles-forever/…
------
Also the fact that we can now have autonomous AI agents capable of planning etc.
Well, the video game AI in StarCraft 1998 was pretty autonomous to me even 25 years ago. And yes, it beat me somewhat consistently back then :P.
------
When discussing parameter-efficient LLM finetuning techniques, a question that often comes up is how prompt and prefix tuning are related. 

Here’s a short introduction and explanation.
------
So many LLMs so little time. 
A handy overview of the main ones and their respective licenses, courtesy of my colleague 
@DaniDapena
------
It's like we're on a quest for the Holy Grail, but instead of a cup, we're searching for the best LLM...

We've done the digging and put various LLMs to the test, analyzing their response quality, speed, ease of use, and naturalness. 

Check out what we discovered… Show more
------
CC 
@unsorsodicorda
------
Nvidia open-sourced a toolkit to address the hallucination issue (/capabilities) of LLMs called NeMo Guardrails. In a nutshell, how it works is that this method uses a database linking to hardcoded prompts, which have to be manually curated.

Repo: https://github.com/NVIDIA/NeMo-Guardrails…

1/4
------
So, if someone carefully tests the hardcoded prompts, one can ensure that the interaction does not diverge from permissible topics and so on.

3/4
------
It's an interesting but not groundbreaking approach as it does not give the LLM better or new capabilities. It merely restricts how far a user can interact with an LLM. Still, it might be a viable bandaid before researchers find alternative ways for hallucination mitigation. 
4/4
------
While everybody is making fun about sudden loss drop, let me tell you what it ACTUALLY means when that happens:

Your training run had a pre-emption and got restarted. Your input pipeline isn’t perfect and now partially going through examples you’ve already seen recently

No foom
------
Possible but hardly inevitable.  It becomes moderately more likely as people call it absurd and fail to take precautions against it, like checking for sudden drops in the loss function and suspending training.  Mostly, though, this is not a necessary postulate of a doom story. twitter.com/perrymetzger/s…
------
I think finetuning LLMs for custom targets and tasks will be the way forward. 

But the quality of the finetuned model hinges upon the quality of the pretrained base model.

We are working on federated training strategies to make such pretrained LLMs available as open source.
------
Help train AI Models for everyone, not just gatekeepers   

We're calling on individuals and institutions with spare compute capacity who want a world where AI is open and used for the benefit of all. 
Join us  https://lightning.ai/pages/lit/

#AI #OpenSource
------
Thanks to parameter-efficient finetuning techniques, you can finetune a 7B LLM on a single GPU in 1-2 h using techniques like low-rank adaptation (LoRA).

Just wrote a new article explaining how LoRA works & how to finetune a pretrained LLM like LLaMA:
------
Instruction finetuning is how we get from GPT-3-like pretrained base models to more capable LLMs like ChatGPT. This requires human-generated instruction data like databricks-dolly-15k.
So, how do we scale this? One way is bootstrapping an LLM off its own generations.

1/7
------
Which is more promising, human-generated instruction datasets or self-instruct-datasets? I vote for both. Why not start with a human-generated instruction dataset like the 15k instructions from databricks-dolly-15k and then scale this with self-instruct?

6/7
------
Here's a link to the SelfInstruct paper if you are interested in additional details: https://arxiv.org/abs/2212.10560

7/7
------
MLOps is dead, long live LMOps
------
In my recent article on LLM finetuning, I discussed the two extreme scenarios: (1) finetuning the output layers vs (2) finetuning all layers.

But what about the in-between? Added some additional results to the article. Turns out, tuning a few transformer blocks can be sufficient
------
Want to adopt the latest LLMs for a specific target domain (e.g., finance data) or target task like document classification?

Finetuning becomes more feasible as we see more and more pretrained LLMs become available under open-source licenses.

https://magazine.sebastianraschka.com/p/finetuning-large-language-models…

Here's an… Show more
------
Want to adopt the latest LLMs for a specific target domain (e.g., finance data) or target task like document classification?

Finetuning becomes more feasible as we see more and more pretrained LLMs become available under open-source licenses.

https://magazine.sebastianraschka.com/p/finetuning-large-language-models…

Here's an… Show more
------
This! When people do back-of-the-envelope calculation regarding how much it costs to train an LLM, they often forget about hparam tuning.

The cost of developing a model is >> than the cost of replicating a model.
------
The great team @MetaAI research used 2,048 80gb A100s for 5 months to train the LLaMa suite of language models

This is 7.4m A100 hours

The 4 final models took 1.8m A100 hours to train

This is 1/4 of the total time spent building this, there is a lot of testing, trial & error
------
Just noticed that GitHub now has a "sync fork" feature. 

This is going to be a big deal for onboarding new contributors! So much convenient than the old way.
------
This will easily shave of 15*number_of_participants minutes of frustration in future sprints :)
------
A little correction since I totally mixed up the colors in this chart yesterday.

Training LLMs on duplicated data (= for multiple epochs) doesn't make a difference.
------
The open source Pythia suite of LLM models is an interesting alternative to other decoder-style (aka GPT-like) models. 

The recent Dolly v2 model used Pythia as the base (foundation) model, and I expect more models building on this in the future.

So, let's take a look!

1/6
------
The open source Pythia suite of LLM models is an interesting alternative to other decoder-style (aka GPT-like) models. 

The recent Dolly v2 model used Pythia as the base (foundation) model, and I expect more models building on this in the future.

So, let's take a look!

1/6
------
1) They mention that one of the interesting insights is that training on duplicated data (i.e. training for >1 epoch) does not benefit or hurt performance -- but the plot below shows otherwise .

4/6
------
It’s actually a great paper . 
This was the only little point (/plot) that didn’t make sense to me because it was contradicting the statement in the paper.
In case I am misunderstanding this, would be happy about some clarification 
@AiEleuther
------
Solid advice here: eventually the window closes and the barrier to entry becomes much higher the longer you wait.
------
If you are interested in it, here why I think you should get into AI right now.

Disclaimer: If you are looking for some AI hype thread, go somewhere else.
------
2) Does training order influence memorization? Unfortunately, it turns out that it does not. "Unfortunately" because if this was true, we could mitigate undesirable verbatim memorization issues by reordering the training data.

5/6
------
3) Does pretrained term frequency influence task performance? Yes, few-shot accuracy tends to be higher for terms that are occurring more frequently.

(Authors, please annotate your plot axes! )

6/6
------
So many LLMs so little time. 
A handy overview of the main ones and their respective licenses, courtesy of my colleague 
@DaniDapena
------
It's like we're on a quest for the Holy Grail, but instead of a cup, we're searching for the best LLM...

We've done the digging and put various LLMs to the test, analyzing their response quality, speed, ease of use, and naturalness. 

Check out what we discovered… Show more
------
CC 
@unsorsodicorda
------
Nvidia open-sourced a toolkit to address the hallucination issue (/capabilities) of LLMs called NeMo Guardrails. In a nutshell, how it works is that this method uses a database linking to hardcoded prompts, which have to be manually curated.

Repo: https://github.com/NVIDIA/NeMo-Guardrails…

1/4
------
So, if someone carefully tests the hardcoded prompts, one can ensure that the interaction does not diverge from permissible topics and so on.

3/4
------
It's an interesting but not groundbreaking approach as it does not give the LLM better or new capabilities. It merely restricts how far a user can interact with an LLM. Still, it might be a viable bandaid before researchers find alternative ways for hallucination mitigation. 
4/4
------
While everybody is making fun about sudden loss drop, let me tell you what it ACTUALLY means when that happens:

Your training run had a pre-emption and got restarted. Your input pipeline isn’t perfect and now partially going through examples you’ve already seen recently

No foom
------
Possible but hardly inevitable.  It becomes moderately more likely as people call it absurd and fail to take precautions against it, like checking for sudden drops in the loss function and suspending training.  Mostly, though, this is not a necessary postulate of a doom story. twitter.com/perrymetzger/s…
------
I think finetuning LLMs for custom targets and tasks will be the way forward. 

But the quality of the finetuned model hinges upon the quality of the pretrained base model.

We are working on federated training strategies to make such pretrained LLMs available as open source.
------
Help train AI Models for everyone, not just gatekeepers   

We're calling on individuals and institutions with spare compute capacity who want a world where AI is open and used for the benefit of all. 
Join us  https://lightning.ai/pages/lit/

#AI #OpenSource
------
Thanks to parameter-efficient finetuning techniques, you can finetune a 7B LLM on a single GPU in 1-2 h using techniques like low-rank adaptation (LoRA).

Just wrote a new article explaining how LoRA works & how to finetune a pretrained LLM like LLaMA:
------
Instruction finetuning is how we get from GPT-3-like pretrained base models to more capable LLMs like ChatGPT. This requires human-generated instruction data like databricks-dolly-15k.
So, how do we scale this? One way is bootstrapping an LLM off its own generations.

1/7
------
Which is more promising, human-generated instruction datasets or self-instruct-datasets? I vote for both. Why not start with a human-generated instruction dataset like the 15k instructions from databricks-dolly-15k and then scale this with self-instruct?

6/7
------
Here's a link to the SelfInstruct paper if you are interested in additional details: https://arxiv.org/abs/2212.10560

7/7
------
MLOps is dead, long live LMOps
------
In my recent article on LLM finetuning, I discussed the two extreme scenarios: (1) finetuning the output layers vs (2) finetuning all layers.

But what about the in-between? Added some additional results to the article. Turns out, tuning a few transformer blocks can be sufficient
------
Want to adopt the latest LLMs for a specific target domain (e.g., finance data) or target task like document classification?

Finetuning becomes more feasible as we see more and more pretrained LLMs become available under open-source licenses.

https://magazine.sebastianraschka.com/p/finetuning-large-language-models…

Here's an… Show more
------
Want to adopt the latest LLMs for a specific target domain (e.g., finance data) or target task like document classification?

Finetuning becomes more feasible as we see more and more pretrained LLMs become available under open-source licenses.

https://magazine.sebastianraschka.com/p/finetuning-large-language-models…

Here's an… Show more
------
This! When people do back-of-the-envelope calculation regarding how much it costs to train an LLM, they often forget about hparam tuning.

The cost of developing a model is >> than the cost of replicating a model.
------
The great team @MetaAI research used 2,048 80gb A100s for 5 months to train the LLaMa suite of language models

This is 7.4m A100 hours

The 4 final models took 1.8m A100 hours to train

This is 1/4 of the total time spent building this, there is a lot of testing, trial & error
------
Just noticed that GitHub now has a "sync fork" feature. 

This is going to be a big deal for onboarding new contributors! So much convenient than the old way.
------
This will easily shave of 15*number_of_participants minutes of frustration in future sprints :)
------
A little correction since I totally mixed up the colors in this chart yesterday.

Training LLMs on duplicated data (= for multiple epochs) doesn't make a difference.
------
The open source Pythia suite of LLM models is an interesting alternative to other decoder-style (aka GPT-like) models. 

The recent Dolly v2 model used Pythia as the base (foundation) model, and I expect more models building on this in the future.

So, let's take a look!

1/6
------
The open source Pythia suite of LLM models is an interesting alternative to other decoder-style (aka GPT-like) models. 

The recent Dolly v2 model used Pythia as the base (foundation) model, and I expect more models building on this in the future.

So, let's take a look!

1/6
------
1) They mention that one of the interesting insights is that training on duplicated data (i.e. training for >1 epoch) does not benefit or hurt performance -- but the plot below shows otherwise .

4/6
------
It’s actually a great paper . 
This was the only little point (/plot) that didn’t make sense to me because it was contradicting the statement in the paper.
In case I am misunderstanding this, would be happy about some clarification 
@AiEleuther
------
Solid advice here: eventually the window closes and the barrier to entry becomes much higher the longer you wait.
------
If you are interested in it, here why I think you should get into AI right now.

Disclaimer: If you are looking for some AI hype thread, go somewhere else.
------
2) Does training order influence memorization? Unfortunately, it turns out that it does not. "Unfortunately" because if this was true, we could mitigate undesirable verbatim memorization issues by reordering the training data.

5/6
------
3) Does pretrained term frequency influence task performance? Yes, few-shot accuracy tends to be higher for terms that are occurring more frequently.

(Authors, please annotate your plot axes! )

6/6
------
Bored by large language modeling? 
Good news, Unit 7: Getting Started with Computer Vision is out!

https://lightning.ai/pages/courses/deep-learning-fundamentals/unit-7-overview-getting-started-with-computer-vision/…

In this unit, I am covering 

- working with image data
- convolutional layers
- image augmentation
- and self-supervised learning
------
Unit 7 of @rasbt's Deep Learning Fundamentals course just dropped!  

Start unit now: https://lightning.ai/pages/courses/deep-learning-fundamentals/unit-7-overview-getting-started-with-computer-vision/…

In this unit, he covers:   
Tabular vs. Image Datasets 
Convolutional Neural Networks in PyTorch 
Image Classification 
Transfer Learning 
Finetuning with… Show more
------
The world needs high performance open source LLM.
The main obstacle today is the legal status of the training data.
------
OK, here it is: a line in the sand (in @Nature).  I am very wary about scientists---including political scientists---embracing/pushing proprietary LLMs. Let's try an open science approach.  Hope this take is a useful one. 
https://nature.com/articles/d41586-023-01295-4…
------
Whoa, super kind feedback about my course 

"I binge watched the course and god it is so good and clearly explained. It even cleared some of my conceptual doubts and also learnt new techniques like choosing the right activation function."
------
2/2

Code examples here: https://github.com/Lightning-AI/dl-fundamentals/tree/main/unit06-dl-tips…

And the full videos here: https://lightning.ai/pages/courses/deep-learning-fundamentals/unit-6-overview-essential-deep-learning-tips-tricks/…
------
"Understanding Large Language Models", the latest article from 
@rasbt
 is a must-read!
------
I think finetuning LLMs for custom targets and tasks will be the way forward. 

But the quality of the finetuned model hinges upon the quality of the pretrained base model.

We are working on federated training strategies to make such pretrained LLMs available as open source.
------
Help train AI Models for everyone, not just gatekeepers   

We're calling on individuals and institutions with spare compute capacity who want a world where AI is open and used for the benefit of all. 
Join us  https://lightning.ai/pages/lit/

#AI #OpenSource
------
Thanks to parameter-efficient finetuning techniques, you can finetune a 7B LLM on a single GPU in 1-2 h using techniques like low-rank adaptation (LoRA).

Just wrote a new article explaining how LoRA works & how to finetune a pretrained LLM like LLaMA:
------
Instruction finetuning is how we get from GPT-3-like pretrained base models to more capable LLMs like ChatGPT. This requires human-generated instruction data like databricks-dolly-15k.
So, how do we scale this? One way is bootstrapping an LLM off its own generations.

1/7
------
Which is more promising, human-generated instruction datasets or self-instruct-datasets? I vote for both. Why not start with a human-generated instruction dataset like the 15k instructions from databricks-dolly-15k and then scale this with self-instruct?

6/7
------
Here's a link to the SelfInstruct paper if you are interested in additional details: https://arxiv.org/abs/2212.10560

7/7
------
MLOps is dead, long live LMOps
------
In my recent article on LLM finetuning, I discussed the two extreme scenarios: (1) finetuning the output layers vs (2) finetuning all layers.

But what about the in-between? Added some additional results to the article. Turns out, tuning a few transformer blocks can be sufficient
------
Want to adopt the latest LLMs for a specific target domain (e.g., finance data) or target task like document classification?

Finetuning becomes more feasible as we see more and more pretrained LLMs become available under open-source licenses.

https://magazine.sebastianraschka.com/p/finetuning-large-language-models…

Here's an… Show more
------
Want to adopt the latest LLMs for a specific target domain (e.g., finance data) or target task like document classification?

Finetuning becomes more feasible as we see more and more pretrained LLMs become available under open-source licenses.

https://magazine.sebastianraschka.com/p/finetuning-large-language-models…

Here's an… Show more
------
This! When people do back-of-the-envelope calculation regarding how much it costs to train an LLM, they often forget about hparam tuning.

The cost of developing a model is >> than the cost of replicating a model.
------
The great team @MetaAI research used 2,048 80gb A100s for 5 months to train the LLaMa suite of language models

This is 7.4m A100 hours

The 4 final models took 1.8m A100 hours to train

This is 1/4 of the total time spent building this, there is a lot of testing, trial & error
------
Just noticed that GitHub now has a "sync fork" feature. 

This is going to be a big deal for onboarding new contributors! So much convenient than the old way.
------
This will easily shave of 15*number_of_participants minutes of frustration in future sprints :)
------
A little correction since I totally mixed up the colors in this chart yesterday.

Training LLMs on duplicated data (= for multiple epochs) doesn't make a difference.
------
The open source Pythia suite of LLM models is an interesting alternative to other decoder-style (aka GPT-like) models. 

The recent Dolly v2 model used Pythia as the base (foundation) model, and I expect more models building on this in the future.

So, let's take a look!

1/6
------
The open source Pythia suite of LLM models is an interesting alternative to other decoder-style (aka GPT-like) models. 

The recent Dolly v2 model used Pythia as the base (foundation) model, and I expect more models building on this in the future.

So, let's take a look!

1/6
------
1) They mention that one of the interesting insights is that training on duplicated data (i.e. training for >1 epoch) does not benefit or hurt performance -- but the plot below shows otherwise .

4/6
------
It’s actually a great paper . 
This was the only little point (/plot) that didn’t make sense to me because it was contradicting the statement in the paper.
In case I am misunderstanding this, would be happy about some clarification 
@AiEleuther
------
Solid advice here: eventually the window closes and the barrier to entry becomes much higher the longer you wait.
------
If you are interested in it, here why I think you should get into AI right now.

Disclaimer: If you are looking for some AI hype thread, go somewhere else.
------
2) Does training order influence memorization? Unfortunately, it turns out that it does not. "Unfortunately" because if this was true, we could mitigate undesirable verbatim memorization issues by reordering the training data.

5/6
------
3) Does pretrained term frequency influence task performance? Yes, few-shot accuracy tends to be higher for terms that are occurring more frequently.

(Authors, please annotate your plot axes! )

6/6
------
Bored by large language modeling? 
Good news, Unit 7: Getting Started with Computer Vision is out!

https://lightning.ai/pages/courses/deep-learning-fundamentals/unit-7-overview-getting-started-with-computer-vision/…

In this unit, I am covering 

- working with image data
- convolutional layers
- image augmentation
- and self-supervised learning
------
Unit 7 of @rasbt's Deep Learning Fundamentals course just dropped!  

Start unit now: https://lightning.ai/pages/courses/deep-learning-fundamentals/unit-7-overview-getting-started-with-computer-vision/…

In this unit, he covers:   
Tabular vs. Image Datasets 
Convolutional Neural Networks in PyTorch 
Image Classification 
Transfer Learning 
Finetuning with… Show more
------
The world needs high performance open source LLM.
The main obstacle today is the legal status of the training data.
------
OK, here it is: a line in the sand (in @Nature).  I am very wary about scientists---including political scientists---embracing/pushing proprietary LLMs. Let's try an open science approach.  Hope this take is a useful one. 
https://nature.com/articles/d41586-023-01295-4…
------
Whoa, super kind feedback about my course 

"I binge watched the course and god it is so good and clearly explained. It even cleared some of my conceptual doubts and also learnt new techniques like choosing the right activation function."
------
2/2

Code examples here: https://github.com/Lightning-AI/dl-fundamentals/tree/main/unit06-dl-tips…

And the full videos here: https://lightning.ai/pages/courses/deep-learning-fundamentals/unit-6-overview-essential-deep-learning-tips-tricks/…
------
"Understanding Large Language Models", the latest article from 
@rasbt
 is a must-read!
------
Last month was particularly exciting for open-source AI, featuring several open source implementations of large language models (LLMs).

Now we are seeing a big wave of open-source datasets! We had Databricks Dolly 15k and OpenAssistant Conversations for instruction finetuning.… Show more
------
Also, putting 1 and 1 together, this means that we will probably see a fresh wave of truly open-source LLMs in the coming weeks and months!
------
Which is more promising, human-generated instruction datasets or self-instruct-datasets? I vote for both. Why not start with a human-generated instruction dataset like the 15k instructions from databricks-dolly-15k and then scale this with self-instruct?

6/7
------
Here's a link to the SelfInstruct paper if you are interested in additional details: https://arxiv.org/abs/2212.10560

7/7
------
MLOps is dead, long live LMOps
------
In my recent article on LLM finetuning, I discussed the two extreme scenarios: (1) finetuning the output layers vs (2) finetuning all layers.

But what about the in-between? Added some additional results to the article. Turns out, tuning a few transformer blocks can be sufficient
------
Want to adopt the latest LLMs for a specific target domain (e.g., finance data) or target task like document classification?

Finetuning becomes more feasible as we see more and more pretrained LLMs become available under open-source licenses.

https://magazine.sebastianraschka.com/p/finetuning-large-language-models…

Here's an… Show more
------
Want to adopt the latest LLMs for a specific target domain (e.g., finance data) or target task like document classification?

Finetuning becomes more feasible as we see more and more pretrained LLMs become available under open-source licenses.

https://magazine.sebastianraschka.com/p/finetuning-large-language-models…

Here's an… Show more
------
This! When people do back-of-the-envelope calculation regarding how much it costs to train an LLM, they often forget about hparam tuning.

The cost of developing a model is >> than the cost of replicating a model.
------
The great team @MetaAI research used 2,048 80gb A100s for 5 months to train the LLaMa suite of language models

This is 7.4m A100 hours

The 4 final models took 1.8m A100 hours to train

This is 1/4 of the total time spent building this, there is a lot of testing, trial & error
------
Just noticed that GitHub now has a "sync fork" feature. 

This is going to be a big deal for onboarding new contributors! So much convenient than the old way.
------
This will easily shave of 15*number_of_participants minutes of frustration in future sprints :)
------
A little correction since I totally mixed up the colors in this chart yesterday.

Training LLMs on duplicated data (= for multiple epochs) doesn't make a difference.
------
The open source Pythia suite of LLM models is an interesting alternative to other decoder-style (aka GPT-like) models. 

The recent Dolly v2 model used Pythia as the base (foundation) model, and I expect more models building on this in the future.

So, let's take a look!

1/6
------
The open source Pythia suite of LLM models is an interesting alternative to other decoder-style (aka GPT-like) models. 

The recent Dolly v2 model used Pythia as the base (foundation) model, and I expect more models building on this in the future.

So, let's take a look!

1/6
------
1) They mention that one of the interesting insights is that training on duplicated data (i.e. training for >1 epoch) does not benefit or hurt performance -- but the plot below shows otherwise .

4/6
------
It’s actually a great paper . 
This was the only little point (/plot) that didn’t make sense to me because it was contradicting the statement in the paper.
In case I am misunderstanding this, would be happy about some clarification 
@AiEleuther
------
Solid advice here: eventually the window closes and the barrier to entry becomes much higher the longer you wait.
------
If you are interested in it, here why I think you should get into AI right now.

Disclaimer: If you are looking for some AI hype thread, go somewhere else.
------
2) Does training order influence memorization? Unfortunately, it turns out that it does not. "Unfortunately" because if this was true, we could mitigate undesirable verbatim memorization issues by reordering the training data.

5/6
------
3) Does pretrained term frequency influence task performance? Yes, few-shot accuracy tends to be higher for terms that are occurring more frequently.

(Authors, please annotate your plot axes! )

6/6
------
Bored by large language modeling? 
Good news, Unit 7: Getting Started with Computer Vision is out!

https://lightning.ai/pages/courses/deep-learning-fundamentals/unit-7-overview-getting-started-with-computer-vision/…

In this unit, I am covering 

- working with image data
- convolutional layers
- image augmentation
- and self-supervised learning
------
Unit 7 of @rasbt's Deep Learning Fundamentals course just dropped!  

Start unit now: https://lightning.ai/pages/courses/deep-learning-fundamentals/unit-7-overview-getting-started-with-computer-vision/…

In this unit, he covers:   
Tabular vs. Image Datasets 
Convolutional Neural Networks in PyTorch 
Image Classification 
Transfer Learning 
Finetuning with… Show more
------
The world needs high performance open source LLM.
The main obstacle today is the legal status of the training data.
------
OK, here it is: a line in the sand (in @Nature).  I am very wary about scientists---including political scientists---embracing/pushing proprietary LLMs. Let's try an open science approach.  Hope this take is a useful one. 
https://nature.com/articles/d41586-023-01295-4…
------
Whoa, super kind feedback about my course 

"I binge watched the course and god it is so good and clearly explained. It even cleared some of my conceptual doubts and also learnt new techniques like choosing the right activation function."
------
2/2

Code examples here: https://github.com/Lightning-AI/dl-fundamentals/tree/main/unit06-dl-tips…

And the full videos here: https://lightning.ai/pages/courses/deep-learning-fundamentals/unit-6-overview-essential-deep-learning-tips-tricks/…
------
"Understanding Large Language Models", the latest article from 
@rasbt
 is a must-read!
------
Last month was particularly exciting for open-source AI, featuring several open source implementations of large language models (LLMs).

Now we are seeing a big wave of open-source datasets! We had Databricks Dolly 15k and OpenAssistant Conversations for instruction finetuning.… Show more
------
Also, putting 1 and 1 together, this means that we will probably see a fresh wave of truly open-source LLMs in the coming weeks and months!
------
I like this description. But calculators are precise. 

I like to think of LLMs as thesauruses (/thesauri) for sentences. https://twitter.com/chrisalbon/status/1648053420845318144…
------
My favorite deep learning tips for the start of the new week: 

1) Rewinding to the best epoch with model checkpointing
2) Debugging by overfitting to a minibatch
3) Looking at model architecture summaries
4) Using learning rate schedulers

1/2
------
2/2

Code examples here: https://github.com/Lightning-AI/dl-fundamentals/tree/main/unit06-dl-tips…

And the full videos here:
------
> I’m so tired of AI hype tweets.

Feel exactly the same way. 

Actually, when a reviewer (of some in-progress work) recently commended me for taking the "mysteriousness and magic out of" LLMs, that made me extremely happy to hear! One of the kindest compliments.
------
I’m so tired of AI hype tweets. They’re all over my Twitter feed these days. twitter.com/fchollet/statu…
------
Sparse Pre-training and Dense Fine-tuning for LLMs -- a 2.5x reduction in pre-training FLOPs, without a significant loss in accuracy on the downstream tasks relative to the dense baseline.
https://arxiv.org/abs/2303.10464
------
Nice! We studied GPT-3 pre-training and fine-tuning in our recent work, where we show how you can get 60% FLOPs savings during pre-training using weight sparsity and still preserve downstream accuracy https://arxiv.org/abs/2303.10464
------
"Dear Sebastian Raschka,
We are happy to announce that your proposal 'Modern Deep Learning with PyTorch' has been selected for SciPy 2023 as a Tutorial!"

Looking forward to see you in Austin this summer  
------
Only 10 days left to snatch up #earlybird tickets for #scipy2023, the must-attend  #Python conference of the year! Don't miss out on the discount and secure your spot now 
https://scipy2023.scipy.org/tickets

#discount #data #mlops #devops #pythoncomputing #scientificpython #AI
------
Looks like generative computer vision models have been on vacation since December. When do you think we'll see the next wave of fresh approaches with better capabilities?

(*Multimodal LLMs will subsume them or steal the show)
------
Want to adopt the latest LLMs for a specific target domain (e.g., finance data) or target task like document classification?

Finetuning becomes more feasible as we see more and more pretrained LLMs become available under open-source licenses.

https://magazine.sebastianraschka.com/p/finetuning-large-language-models…

Here's an… Show more
------
This! When people do back-of-the-envelope calculation regarding how much it costs to train an LLM, they often forget about hparam tuning.

The cost of developing a model is >> than the cost of replicating a model.
------
The great team @MetaAI research used 2,048 80gb A100s for 5 months to train the LLaMa suite of language models

This is 7.4m A100 hours

The 4 final models took 1.8m A100 hours to train

This is 1/4 of the total time spent building this, there is a lot of testing, trial & error
------
Just noticed that GitHub now has a "sync fork" feature. 

This is going to be a big deal for onboarding new contributors! So much convenient than the old way.
------
This will easily shave of 15*number_of_participants minutes of frustration in future sprints :)
------
A little correction since I totally mixed up the colors in this chart yesterday.

Training LLMs on duplicated data (= for multiple epochs) doesn't make a difference.
------
The open source Pythia suite of LLM models is an interesting alternative to other decoder-style (aka GPT-like) models. 

The recent Dolly v2 model used Pythia as the base (foundation) model, and I expect more models building on this in the future.

So, let's take a look!

1/6
------
The open source Pythia suite of LLM models is an interesting alternative to other decoder-style (aka GPT-like) models. 

The recent Dolly v2 model used Pythia as the base (foundation) model, and I expect more models building on this in the future.

So, let's take a look!

1/6
------
1) They mention that one of the interesting insights is that training on duplicated data (i.e. training for >1 epoch) does not benefit or hurt performance -- but the plot below shows otherwise .

4/6
------
It’s actually a great paper . 
This was the only little point (/plot) that didn’t make sense to me because it was contradicting the statement in the paper.
In case I am misunderstanding this, would be happy about some clarification 
@AiEleuther
------
Solid advice here: eventually the window closes and the barrier to entry becomes much higher the longer you wait.
------
If you are interested in it, here why I think you should get into AI right now.

Disclaimer: If you are looking for some AI hype thread, go somewhere else.
------
2) Does training order influence memorization? Unfortunately, it turns out that it does not. "Unfortunately" because if this was true, we could mitigate undesirable verbatim memorization issues by reordering the training data.

5/6
------
3) Does pretrained term frequency influence task performance? Yes, few-shot accuracy tends to be higher for terms that are occurring more frequently.

(Authors, please annotate your plot axes! )

6/6
------
Bored by large language modeling? 
Good news, Unit 7: Getting Started with Computer Vision is out!

https://lightning.ai/pages/courses/deep-learning-fundamentals/unit-7-overview-getting-started-with-computer-vision/…

In this unit, I am covering 

- working with image data
- convolutional layers
- image augmentation
- and self-supervised learning
------
Unit 7 of @rasbt's Deep Learning Fundamentals course just dropped!  

Start unit now: https://lightning.ai/pages/courses/deep-learning-fundamentals/unit-7-overview-getting-started-with-computer-vision/…

In this unit, he covers:   
Tabular vs. Image Datasets 
Convolutional Neural Networks in PyTorch 
Image Classification 
Transfer Learning 
Finetuning with… Show more
------
The world needs high performance open source LLM.
The main obstacle today is the legal status of the training data.
------
OK, here it is: a line in the sand (in @Nature).  I am very wary about scientists---including political scientists---embracing/pushing proprietary LLMs. Let's try an open science approach.  Hope this take is a useful one. 
https://nature.com/articles/d41586-023-01295-4…
------
Whoa, super kind feedback about my course 

"I binge watched the course and god it is so good and clearly explained. It even cleared some of my conceptual doubts and also learnt new techniques like choosing the right activation function."
------
2/2

Code examples here: https://github.com/Lightning-AI/dl-fundamentals/tree/main/unit06-dl-tips…

And the full videos here: https://lightning.ai/pages/courses/deep-learning-fundamentals/unit-6-overview-essential-deep-learning-tips-tricks/…
------
"Understanding Large Language Models", the latest article from 
@rasbt
 is a must-read!
------
Last month was particularly exciting for open-source AI, featuring several open source implementations of large language models (LLMs).

Now we are seeing a big wave of open-source datasets! We had Databricks Dolly 15k and OpenAssistant Conversations for instruction finetuning.… Show more
------
Also, putting 1 and 1 together, this means that we will probably see a fresh wave of truly open-source LLMs in the coming weeks and months!
------
I like this description. But calculators are precise. 

I like to think of LLMs as thesauruses (/thesauri) for sentences. https://twitter.com/chrisalbon/status/1648053420845318144…
------
My favorite deep learning tips for the start of the new week: 

1) Rewinding to the best epoch with model checkpointing
2) Debugging by overfitting to a minibatch
3) Looking at model architecture summaries
4) Using learning rate schedulers

1/2
------
2/2

Code examples here: https://github.com/Lightning-AI/dl-fundamentals/tree/main/unit06-dl-tips…

And the full videos here:
------
> I’m so tired of AI hype tweets.

Feel exactly the same way. 

Actually, when a reviewer (of some in-progress work) recently commended me for taking the "mysteriousness and magic out of" LLMs, that made me extremely happy to hear! One of the kindest compliments.
------
I’m so tired of AI hype tweets. They’re all over my Twitter feed these days. twitter.com/fchollet/statu…
------
Sparse Pre-training and Dense Fine-tuning for LLMs -- a 2.5x reduction in pre-training FLOPs, without a significant loss in accuracy on the downstream tasks relative to the dense baseline.
https://arxiv.org/abs/2303.10464
------
Nice! We studied GPT-3 pre-training and fine-tuning in our recent work, where we show how you can get 60% FLOPs savings during pre-training using weight sparsity and still preserve downstream accuracy https://arxiv.org/abs/2303.10464
------
"Dear Sebastian Raschka,
We are happy to announce that your proposal 'Modern Deep Learning with PyTorch' has been selected for SciPy 2023 as a Tutorial!"

Looking forward to see you in Austin this summer  
------
Only 10 days left to snatch up #earlybird tickets for #scipy2023, the must-attend  #Python conference of the year! Don't miss out on the discount and secure your spot now 
https://scipy2023.scipy.org/tickets

#discount #data #mlops #devops #pythoncomputing #scientificpython #AI
------
Looks like generative computer vision models have been on vacation since December. When do you think we'll see the next wave of fresh approaches with better capabilities?

(*Multimodal LLMs will subsume them or steal the show)
------
On that note, I probably wouldn't consider Meta's SAM a generative model, but yeah, in a sense it's generating segmentation masks. 
I was more thinking about stable diffusion-like models aka "generative AI" for images and videos  (diffusion models, GANs, etc.).
------
Finetuning vs prompting?
Here's a nice empirical insight from https://arxiv.org/abs/2104.08691 illustrating that finetuning  outperforms prompting.

(Caveat: I wish they had done a per-dataset analysis to learn what how much labeled data is needed to outperform prompting)
------
Does that mean we don't need generalist LLMs? Here's the thing: we can also see in the plot above that the larger the finetuned LLM, the better. So, it would make sense to further develop large generalist LLMs, which can then be finetuned to become specialist LLMs.
------
BREAKING: AutoGPT to replace a significant amount of the workforce.

The first job threatened by AutoGPT is the "busy fool".

- Appear busy all day long
- Tell everyone you're busy
- Produce nothing

If this applies to your job, you should be worried.
------
Speaking of finetuning ... just saw Databricks open-sourced their databricks-dolly-15k dataset! 

https://github.com/databrickslabs/dolly/tree/master/data…

That's 15,000 high-quality human-generated prompt / response pairs specifically designed for ChatGPT-like instruction tuning.

Something fun to play with!!
------
Want to adapt large language models (LLMs) to your target domain? Parameter-efficient finetuning is and will remain the way forward!

I just distilled the main concepts, from prefix tuning to LLaMA-Adapters, in a new blog article here:

  https://lightning.ai/pages/community/article/understanding-llama-adapters/…
------
The open source Pythia suite of LLM models is an interesting alternative to other decoder-style (aka GPT-like) models. 

The recent Dolly v2 model used Pythia as the base (foundation) model, and I expect more models building on this in the future.

So, let's take a look!

1/6
------
1) They mention that one of the interesting insights is that training on duplicated data (i.e. training for >1 epoch) does not benefit or hurt performance -- but the plot below shows otherwise .

4/6
------
It’s actually a great paper . 
This was the only little point (/plot) that didn’t make sense to me because it was contradicting the statement in the paper.
In case I am misunderstanding this, would be happy about some clarification 
@AiEleuther
------
Solid advice here: eventually the window closes and the barrier to entry becomes much higher the longer you wait.
------
If you are interested in it, here why I think you should get into AI right now.

Disclaimer: If you are looking for some AI hype thread, go somewhere else.
------
2) Does training order influence memorization? Unfortunately, it turns out that it does not. "Unfortunately" because if this was true, we could mitigate undesirable verbatim memorization issues by reordering the training data.

5/6
------
3) Does pretrained term frequency influence task performance? Yes, few-shot accuracy tends to be higher for terms that are occurring more frequently.

(Authors, please annotate your plot axes! )

6/6
------
Bored by large language modeling? 
Good news, Unit 7: Getting Started with Computer Vision is out!

https://lightning.ai/pages/courses/deep-learning-fundamentals/unit-7-overview-getting-started-with-computer-vision/…

In this unit, I am covering 

- working with image data
- convolutional layers
- image augmentation
- and self-supervised learning
------
Unit 7 of @rasbt's Deep Learning Fundamentals course just dropped!  

Start unit now: https://lightning.ai/pages/courses/deep-learning-fundamentals/unit-7-overview-getting-started-with-computer-vision/…

In this unit, he covers:   
Tabular vs. Image Datasets 
Convolutional Neural Networks in PyTorch 
Image Classification 
Transfer Learning 
Finetuning with… Show more
------
The world needs high performance open source LLM.
The main obstacle today is the legal status of the training data.
------
OK, here it is: a line in the sand (in @Nature).  I am very wary about scientists---including political scientists---embracing/pushing proprietary LLMs. Let's try an open science approach.  Hope this take is a useful one. 
https://nature.com/articles/d41586-023-01295-4…
------
Whoa, super kind feedback about my course 

"I binge watched the course and god it is so good and clearly explained. It even cleared some of my conceptual doubts and also learnt new techniques like choosing the right activation function."
------
2/2

Code examples here: https://github.com/Lightning-AI/dl-fundamentals/tree/main/unit06-dl-tips…

And the full videos here: https://lightning.ai/pages/courses/deep-learning-fundamentals/unit-6-overview-essential-deep-learning-tips-tricks/…
------
"Understanding Large Language Models", the latest article from 
@rasbt
 is a must-read!
------
Last month was particularly exciting for open-source AI, featuring several open source implementations of large language models (LLMs).

Now we are seeing a big wave of open-source datasets! We had Databricks Dolly 15k and OpenAssistant Conversations for instruction finetuning.… Show more
------
Also, putting 1 and 1 together, this means that we will probably see a fresh wave of truly open-source LLMs in the coming weeks and months!
------
I like this description. But calculators are precise. 

I like to think of LLMs as thesauruses (/thesauri) for sentences. https://twitter.com/chrisalbon/status/1648053420845318144…
------
My favorite deep learning tips for the start of the new week: 

1) Rewinding to the best epoch with model checkpointing
2) Debugging by overfitting to a minibatch
3) Looking at model architecture summaries
4) Using learning rate schedulers

1/2
------
2/2

Code examples here: https://github.com/Lightning-AI/dl-fundamentals/tree/main/unit06-dl-tips…

And the full videos here:
------
> I’m so tired of AI hype tweets.

Feel exactly the same way. 

Actually, when a reviewer (of some in-progress work) recently commended me for taking the "mysteriousness and magic out of" LLMs, that made me extremely happy to hear! One of the kindest compliments.
------
I’m so tired of AI hype tweets. They’re all over my Twitter feed these days. twitter.com/fchollet/statu…
------
Sparse Pre-training and Dense Fine-tuning for LLMs -- a 2.5x reduction in pre-training FLOPs, without a significant loss in accuracy on the downstream tasks relative to the dense baseline.
https://arxiv.org/abs/2303.10464
------
Nice! We studied GPT-3 pre-training and fine-tuning in our recent work, where we show how you can get 60% FLOPs savings during pre-training using weight sparsity and still preserve downstream accuracy https://arxiv.org/abs/2303.10464
------
"Dear Sebastian Raschka,
We are happy to announce that your proposal 'Modern Deep Learning with PyTorch' has been selected for SciPy 2023 as a Tutorial!"

Looking forward to see you in Austin this summer  
------
Only 10 days left to snatch up #earlybird tickets for #scipy2023, the must-attend  #Python conference of the year! Don't miss out on the discount and secure your spot now 
https://scipy2023.scipy.org/tickets

#discount #data #mlops #devops #pythoncomputing #scientificpython #AI
------
Looks like generative computer vision models have been on vacation since December. When do you think we'll see the next wave of fresh approaches with better capabilities?

(*Multimodal LLMs will subsume them or steal the show)
------
On that note, I probably wouldn't consider Meta's SAM a generative model, but yeah, in a sense it's generating segmentation masks. 
I was more thinking about stable diffusion-like models aka "generative AI" for images and videos  (diffusion models, GANs, etc.).
------
Finetuning vs prompting?
Here's a nice empirical insight from https://arxiv.org/abs/2104.08691 illustrating that finetuning  outperforms prompting.

(Caveat: I wish they had done a per-dataset analysis to learn what how much labeled data is needed to outperform prompting)
------
Does that mean we don't need generalist LLMs? Here's the thing: we can also see in the plot above that the larger the finetuned LLM, the better. So, it would make sense to further develop large generalist LLMs, which can then be finetuned to become specialist LLMs.
------
BREAKING: AutoGPT to replace a significant amount of the workforce.

The first job threatened by AutoGPT is the "busy fool".

- Appear busy all day long
- Tell everyone you're busy
- Produce nothing

If this applies to your job, you should be worried.
------
Speaking of finetuning ... just saw Databricks open-sourced their databricks-dolly-15k dataset! 

https://github.com/databrickslabs/dolly/tree/master/data…

That's 15,000 high-quality human-generated prompt / response pairs specifically designed for ChatGPT-like instruction tuning.

Something fun to play with!!
------
Want to adapt large language models (LLMs) to your target domain? Parameter-efficient finetuning is and will remain the way forward!

I just distilled the main concepts, from prefix tuning to LLaMA-Adapters, in a new blog article here:

  https://lightning.ai/pages/community/article/understanding-llama-adapters/…
------
Want to adapt large language models (LLMs) to your target domain? Parameter-efficient finetuning is and will remain the way forward!

I just distilled the main concepts, from prefix tuning to LLaMA-Adapters, in a new blog article here:

  https://lightning.ai/pages/community/article/understanding-llama-adapters/…
------
Open-source efforts empower us to harness cutting-edge tech.  

And new tools like parameter-efficient fine-tuning techniques let us tailor LLMs for our needs  

Curious about LLaMA-Adapters?  Learn how they work in our latest blog post by @rasbt https://bit.ly/41ivvs6
------
It’s almost trivial nowadays to download & use a pretrained LLM for SOTA language translation but users will still flock to Google Translate because of the UI.
------
As we approach the one-year anniversary since the launch of PyTorch DataPipes, I still find myself using the conventional Dataset class .

Interested to hear if you have adopted DataPipes, and why / why not?

By the way, I blogged about DataPipes here:
------
In recent days, I shared various resources on finetuning large language models. Multiple people reached out asking what I think about in-context learning.

1/6
------
Bored by large language modeling? 
Good news, Unit 7: Getting Started with Computer Vision is out!

https://lightning.ai/pages/courses/deep-learning-fundamentals/unit-7-overview-getting-started-with-computer-vision/…

In this unit, I am covering 

- working with image data
- convolutional layers
- image augmentation
- and self-supervised learning
------
Unit 7 of @rasbt's Deep Learning Fundamentals course just dropped!  

Start unit now: https://lightning.ai/pages/courses/deep-learning-fundamentals/unit-7-overview-getting-started-with-computer-vision/…

In this unit, he covers:   
Tabular vs. Image Datasets 
Convolutional Neural Networks in PyTorch 
Image Classification 
Transfer Learning 
Finetuning with… Show more
------
The world needs high performance open source LLM.
The main obstacle today is the legal status of the training data.
------
OK, here it is: a line in the sand (in @Nature).  I am very wary about scientists---including political scientists---embracing/pushing proprietary LLMs. Let's try an open science approach.  Hope this take is a useful one. 
https://nature.com/articles/d41586-023-01295-4…
------
Whoa, super kind feedback about my course 

"I binge watched the course and god it is so good and clearly explained. It even cleared some of my conceptual doubts and also learnt new techniques like choosing the right activation function."
------
2/2

Code examples here: https://github.com/Lightning-AI/dl-fundamentals/tree/main/unit06-dl-tips…

And the full videos here: https://lightning.ai/pages/courses/deep-learning-fundamentals/unit-6-overview-essential-deep-learning-tips-tricks/…
------
"Understanding Large Language Models", the latest article from 
@rasbt
 is a must-read!
------
Last month was particularly exciting for open-source AI, featuring several open source implementations of large language models (LLMs).

Now we are seeing a big wave of open-source datasets! We had Databricks Dolly 15k and OpenAssistant Conversations for instruction finetuning.… Show more
------
Also, putting 1 and 1 together, this means that we will probably see a fresh wave of truly open-source LLMs in the coming weeks and months!
------
I like this description. But calculators are precise. 

I like to think of LLMs as thesauruses (/thesauri) for sentences. https://twitter.com/chrisalbon/status/1648053420845318144…
------
My favorite deep learning tips for the start of the new week: 

1) Rewinding to the best epoch with model checkpointing
2) Debugging by overfitting to a minibatch
3) Looking at model architecture summaries
4) Using learning rate schedulers

1/2
------
2/2

Code examples here: https://github.com/Lightning-AI/dl-fundamentals/tree/main/unit06-dl-tips…

And the full videos here:
------
> I’m so tired of AI hype tweets.

Feel exactly the same way. 

Actually, when a reviewer (of some in-progress work) recently commended me for taking the "mysteriousness and magic out of" LLMs, that made me extremely happy to hear! One of the kindest compliments.
------
I’m so tired of AI hype tweets. They’re all over my Twitter feed these days. twitter.com/fchollet/statu…
------
Sparse Pre-training and Dense Fine-tuning for LLMs -- a 2.5x reduction in pre-training FLOPs, without a significant loss in accuracy on the downstream tasks relative to the dense baseline.
https://arxiv.org/abs/2303.10464
------
Nice! We studied GPT-3 pre-training and fine-tuning in our recent work, where we show how you can get 60% FLOPs savings during pre-training using weight sparsity and still preserve downstream accuracy https://arxiv.org/abs/2303.10464
------
"Dear Sebastian Raschka,
We are happy to announce that your proposal 'Modern Deep Learning with PyTorch' has been selected for SciPy 2023 as a Tutorial!"

Looking forward to see you in Austin this summer  
------
Only 10 days left to snatch up #earlybird tickets for #scipy2023, the must-attend  #Python conference of the year! Don't miss out on the discount and secure your spot now 
https://scipy2023.scipy.org/tickets

#discount #data #mlops #devops #pythoncomputing #scientificpython #AI
------
Looks like generative computer vision models have been on vacation since December. When do you think we'll see the next wave of fresh approaches with better capabilities?

(*Multimodal LLMs will subsume them or steal the show)
------
On that note, I probably wouldn't consider Meta's SAM a generative model, but yeah, in a sense it's generating segmentation masks. 
I was more thinking about stable diffusion-like models aka "generative AI" for images and videos  (diffusion models, GANs, etc.).
------
Finetuning vs prompting?
Here's a nice empirical insight from https://arxiv.org/abs/2104.08691 illustrating that finetuning  outperforms prompting.

(Caveat: I wish they had done a per-dataset analysis to learn what how much labeled data is needed to outperform prompting)
------
Does that mean we don't need generalist LLMs? Here's the thing: we can also see in the plot above that the larger the finetuned LLM, the better. So, it would make sense to further develop large generalist LLMs, which can then be finetuned to become specialist LLMs.
------
BREAKING: AutoGPT to replace a significant amount of the workforce.

The first job threatened by AutoGPT is the "busy fool".

- Appear busy all day long
- Tell everyone you're busy
- Produce nothing

If this applies to your job, you should be worried.
------
Speaking of finetuning ... just saw Databricks open-sourced their databricks-dolly-15k dataset! 

https://github.com/databrickslabs/dolly/tree/master/data…

That's 15,000 high-quality human-generated prompt / response pairs specifically designed for ChatGPT-like instruction tuning.

Something fun to play with!!
------
Want to adapt large language models (LLMs) to your target domain? Parameter-efficient finetuning is and will remain the way forward!

I just distilled the main concepts, from prefix tuning to LLaMA-Adapters, in a new blog article here:

  https://lightning.ai/pages/community/article/understanding-llama-adapters/…
------
Want to adapt large language models (LLMs) to your target domain? Parameter-efficient finetuning is and will remain the way forward!

I just distilled the main concepts, from prefix tuning to LLaMA-Adapters, in a new blog article here:

  https://lightning.ai/pages/community/article/understanding-llama-adapters/…
------
Open-source efforts empower us to harness cutting-edge tech.  

And new tools like parameter-efficient fine-tuning techniques let us tailor LLMs for our needs  

Curious about LLaMA-Adapters?  Learn how they work in our latest blog post by @rasbt https://bit.ly/41ivvs6
------
It’s almost trivial nowadays to download & use a pretrained LLM for SOTA language translation but users will still flock to Google Translate because of the UI.
------
As we approach the one-year anniversary since the launch of PyTorch DataPipes, I still find myself using the conventional Dataset class .

Interested to hear if you have adopted DataPipes, and why / why not?

By the way, I blogged about DataPipes here:
------
In recent days, I shared various resources on finetuning large language models. Multiple people reached out asking what I think about in-context learning.

1/6
------
For LLMs, indexing with in-context learning transforms them into information retrieval systems, extracting data from external sources and websites. The indexing module dissects docs or websites into smaller pieces, converting them into vectors for storage in a database.

5/6
------
Upon receiving a user query, the indexing module calculates vector similarity betw the embedded query & stored vectors. It then retrieves the top k most similar embeddings to compose the response. A popular implementation of this is  LLaMA-Index: https://github.com/jerryjliu/llama_index…

6/6
------
Last month was particularly exciting for open-source AI, featuring several open source implementations of large language models (LLMs).

Now we are seeing a big wave of open-source datasets! We had Databricks Dolly 15k and OpenAssistant Conversations for instruction finetuning.… Show more
------
Also, putting 1 and 1 together, this means that we will probably see a fresh wave of truly open-source LLMs in the coming weeks and months!
------
I like this description. But calculators are precise. 

I like to think of LLMs as thesauruses (/thesauri) for sentences. https://twitter.com/chrisalbon/status/1648053420845318144…
------
My favorite deep learning tips for the start of the new week: 

1) Rewinding to the best epoch with model checkpointing
2) Debugging by overfitting to a minibatch
3) Looking at model architecture summaries
4) Using learning rate schedulers

1/2
------
2/2

Code examples here: https://github.com/Lightning-AI/dl-fundamentals/tree/main/unit06-dl-tips…

And the full videos here:
------
> I’m so tired of AI hype tweets.

Feel exactly the same way. 

Actually, when a reviewer (of some in-progress work) recently commended me for taking the "mysteriousness and magic out of" LLMs, that made me extremely happy to hear! One of the kindest compliments.
------
I’m so tired of AI hype tweets. They’re all over my Twitter feed these days. twitter.com/fchollet/statu…
------
Sparse Pre-training and Dense Fine-tuning for LLMs -- a 2.5x reduction in pre-training FLOPs, without a significant loss in accuracy on the downstream tasks relative to the dense baseline.
https://arxiv.org/abs/2303.10464
------
Nice! We studied GPT-3 pre-training and fine-tuning in our recent work, where we show how you can get 60% FLOPs savings during pre-training using weight sparsity and still preserve downstream accuracy https://arxiv.org/abs/2303.10464
------
"Dear Sebastian Raschka,
We are happy to announce that your proposal 'Modern Deep Learning with PyTorch' has been selected for SciPy 2023 as a Tutorial!"

Looking forward to see you in Austin this summer  
------
Only 10 days left to snatch up #earlybird tickets for #scipy2023, the must-attend  #Python conference of the year! Don't miss out on the discount and secure your spot now 
https://scipy2023.scipy.org/tickets

#discount #data #mlops #devops #pythoncomputing #scientificpython #AI
------
Looks like generative computer vision models have been on vacation since December. When do you think we'll see the next wave of fresh approaches with better capabilities?

(*Multimodal LLMs will subsume them or steal the show)
------
On that note, I probably wouldn't consider Meta's SAM a generative model, but yeah, in a sense it's generating segmentation masks. 
I was more thinking about stable diffusion-like models aka "generative AI" for images and videos  (diffusion models, GANs, etc.).
------
Finetuning vs prompting?
Here's a nice empirical insight from https://arxiv.org/abs/2104.08691 illustrating that finetuning  outperforms prompting.

(Caveat: I wish they had done a per-dataset analysis to learn what how much labeled data is needed to outperform prompting)
------
Does that mean we don't need generalist LLMs? Here's the thing: we can also see in the plot above that the larger the finetuned LLM, the better. So, it would make sense to further develop large generalist LLMs, which can then be finetuned to become specialist LLMs.
------
BREAKING: AutoGPT to replace a significant amount of the workforce.

The first job threatened by AutoGPT is the "busy fool".

- Appear busy all day long
- Tell everyone you're busy
- Produce nothing

If this applies to your job, you should be worried.
------
Speaking of finetuning ... just saw Databricks open-sourced their databricks-dolly-15k dataset! 

https://github.com/databrickslabs/dolly/tree/master/data…

That's 15,000 high-quality human-generated prompt / response pairs specifically designed for ChatGPT-like instruction tuning.

Something fun to play with!!
------
Want to adapt large language models (LLMs) to your target domain? Parameter-efficient finetuning is and will remain the way forward!

I just distilled the main concepts, from prefix tuning to LLaMA-Adapters, in a new blog article here:

  https://lightning.ai/pages/community/article/understanding-llama-adapters/…
------
Want to adapt large language models (LLMs) to your target domain? Parameter-efficient finetuning is and will remain the way forward!

I just distilled the main concepts, from prefix tuning to LLaMA-Adapters, in a new blog article here:

  https://lightning.ai/pages/community/article/understanding-llama-adapters/…
------
Open-source efforts empower us to harness cutting-edge tech.  

And new tools like parameter-efficient fine-tuning techniques let us tailor LLMs for our needs  

Curious about LLaMA-Adapters?  Learn how they work in our latest blog post by @rasbt https://bit.ly/41ivvs6
------
It’s almost trivial nowadays to download & use a pretrained LLM for SOTA language translation but users will still flock to Google Translate because of the UI.
------
As we approach the one-year anniversary since the launch of PyTorch DataPipes, I still find myself using the conventional Dataset class .

Interested to hear if you have adopted DataPipes, and why / why not?

By the way, I blogged about DataPipes here:
------
In recent days, I shared various resources on finetuning large language models. Multiple people reached out asking what I think about in-context learning.

1/6
------
For LLMs, indexing with in-context learning transforms them into information retrieval systems, extracting data from external sources and websites. The indexing module dissects docs or websites into smaller pieces, converting them into vectors for storage in a database.

5/6
------
Upon receiving a user query, the indexing module calculates vector similarity betw the embedded query & stored vectors. It then retrieves the top k most similar embeddings to compose the response. A popular implementation of this is  LLaMA-Index: https://github.com/jerryjliu/llama_index…

6/6
------
In-context learning is quite impressive & capable, though (as we have already seen even back then with older models such as GPT-2 & GPT-3). Researchers have recently begun developing interesting exploits allowing LLMs to access  external databases & documents w/o finetuning.

4/6
------
Similar to "pretraining" can we please switch to "finetuning" instead of "fine-tuning"? 

The latter looks like mini-batch and on-line gradient descent training for fully-connected layers  
------
LLMs just hit a major milestone with the release of the new "Generative agents" paper.

By using LLMs, generative agents were able to simulate human-like behavior in an interactive sandbox inspired by The Sims.

The agent architecture extends Language Models to store a complete… Show more
------
Yesterday, I talked about 2 of the 3 most popular parameter-efficient techniques to finetune large language models (LLMs). 

The 3rd method is Low-Rank Adaptation (LoRA) of course!

1/9
------
Prefix tuning and adapters are the 2 out of the 3 most widely used parameter-efficient finetuning methods for large language models (LLMs).

If you look closely, the recent LLaMA-Adapter method that made big waves is actually a prefix tuning method, not an adapter method.

1/2  twitter.com/rasbt/status/1…
------
For LoRA, during production, we can precompute and save the updated weights so there is zero overhead: 
W_updated = W_pretrained + ∆W 
= W_pretrained + A x B

8/9
------
Here's the full LoRA paper if you want to check it out: https://arxiv.org/abs/2106.09685

PS: We also added LoRA to Lit-LLaMA last week: https://github.com/Lightning-AI/lit-llama/blob/main/finetune_lora.py…

9/9
------
> I’m so tired of AI hype tweets.

Feel exactly the same way. 

Actually, when a reviewer (of some in-progress work) recently commended me for taking the "mysteriousness and magic out of" LLMs, that made me extremely happy to hear! One of the kindest compliments.
------
I’m so tired of AI hype tweets. They’re all over my Twitter feed these days. twitter.com/fchollet/statu…
------
Sparse Pre-training and Dense Fine-tuning for LLMs -- a 2.5x reduction in pre-training FLOPs, without a significant loss in accuracy on the downstream tasks relative to the dense baseline.
https://arxiv.org/abs/2303.10464
------
Nice! We studied GPT-3 pre-training and fine-tuning in our recent work, where we show how you can get 60% FLOPs savings during pre-training using weight sparsity and still preserve downstream accuracy https://arxiv.org/abs/2303.10464
------
"Dear Sebastian Raschka,
We are happy to announce that your proposal 'Modern Deep Learning with PyTorch' has been selected for SciPy 2023 as a Tutorial!"

Looking forward to see you in Austin this summer  
------
Only 10 days left to snatch up #earlybird tickets for #scipy2023, the must-attend  #Python conference of the year! Don't miss out on the discount and secure your spot now 
https://scipy2023.scipy.org/tickets

#discount #data #mlops #devops #pythoncomputing #scientificpython #AI
------
Looks like generative computer vision models have been on vacation since December. When do you think we'll see the next wave of fresh approaches with better capabilities?

(*Multimodal LLMs will subsume them or steal the show)
------
On that note, I probably wouldn't consider Meta's SAM a generative model, but yeah, in a sense it's generating segmentation masks. 
I was more thinking about stable diffusion-like models aka "generative AI" for images and videos  (diffusion models, GANs, etc.).
------
Finetuning vs prompting?
Here's a nice empirical insight from https://arxiv.org/abs/2104.08691 illustrating that finetuning  outperforms prompting.

(Caveat: I wish they had done a per-dataset analysis to learn what how much labeled data is needed to outperform prompting)
------
Does that mean we don't need generalist LLMs? Here's the thing: we can also see in the plot above that the larger the finetuned LLM, the better. So, it would make sense to further develop large generalist LLMs, which can then be finetuned to become specialist LLMs.
------
BREAKING: AutoGPT to replace a significant amount of the workforce.

The first job threatened by AutoGPT is the "busy fool".

- Appear busy all day long
- Tell everyone you're busy
- Produce nothing

If this applies to your job, you should be worried.
------
Speaking of finetuning ... just saw Databricks open-sourced their databricks-dolly-15k dataset! 

https://github.com/databrickslabs/dolly/tree/master/data…

That's 15,000 high-quality human-generated prompt / response pairs specifically designed for ChatGPT-like instruction tuning.

Something fun to play with!!
------
Want to adapt large language models (LLMs) to your target domain? Parameter-efficient finetuning is and will remain the way forward!

I just distilled the main concepts, from prefix tuning to LLaMA-Adapters, in a new blog article here:

  https://lightning.ai/pages/community/article/understanding-llama-adapters/…
------
Want to adapt large language models (LLMs) to your target domain? Parameter-efficient finetuning is and will remain the way forward!

I just distilled the main concepts, from prefix tuning to LLaMA-Adapters, in a new blog article here:

  https://lightning.ai/pages/community/article/understanding-llama-adapters/…
------
Open-source efforts empower us to harness cutting-edge tech.  

And new tools like parameter-efficient fine-tuning techniques let us tailor LLMs for our needs  

Curious about LLaMA-Adapters?  Learn how they work in our latest blog post by @rasbt https://bit.ly/41ivvs6
------
It’s almost trivial nowadays to download & use a pretrained LLM for SOTA language translation but users will still flock to Google Translate because of the UI.
------
As we approach the one-year anniversary since the launch of PyTorch DataPipes, I still find myself using the conventional Dataset class .

Interested to hear if you have adopted DataPipes, and why / why not?

By the way, I blogged about DataPipes here:
------
In recent days, I shared various resources on finetuning large language models. Multiple people reached out asking what I think about in-context learning.

1/6
------
For LLMs, indexing with in-context learning transforms them into information retrieval systems, extracting data from external sources and websites. The indexing module dissects docs or websites into smaller pieces, converting them into vectors for storage in a database.

5/6
------
Upon receiving a user query, the indexing module calculates vector similarity betw the embedded query & stored vectors. It then retrieves the top k most similar embeddings to compose the response. A popular implementation of this is  LLaMA-Index: https://github.com/jerryjliu/llama_index…

6/6
------
In-context learning is quite impressive & capable, though (as we have already seen even back then with older models such as GPT-2 & GPT-3). Researchers have recently begun developing interesting exploits allowing LLMs to access  external databases & documents w/o finetuning.

4/6
------
Similar to "pretraining" can we please switch to "finetuning" instead of "fine-tuning"? 

The latter looks like mini-batch and on-line gradient descent training for fully-connected layers  
------
LLMs just hit a major milestone with the release of the new "Generative agents" paper.

By using LLMs, generative agents were able to simulate human-like behavior in an interactive sandbox inspired by The Sims.

The agent architecture extends Language Models to store a complete… Show more
------
Yesterday, I talked about 2 of the 3 most popular parameter-efficient techniques to finetune large language models (LLMs). 

The 3rd method is Low-Rank Adaptation (LoRA) of course!

1/9
------
Prefix tuning and adapters are the 2 out of the 3 most widely used parameter-efficient finetuning methods for large language models (LLMs).

If you look closely, the recent LLaMA-Adapter method that made big waves is actually a prefix tuning method, not an adapter method.

1/2  twitter.com/rasbt/status/1…
------
For LoRA, during production, we can precompute and save the updated weights so there is zero overhead: 
W_updated = W_pretrained + ∆W 
= W_pretrained + A x B

8/9
------
Here's the full LoRA paper if you want to check it out: https://arxiv.org/abs/2106.09685

PS: We also added LoRA to Lit-LLaMA last week: https://github.com/Lightning-AI/lit-llama/blob/main/finetune_lora.py…

9/9
------
Got asked whether I collect these types of threads for easier reference somewhere.

I actually do! 

Or rather, I have more polished write-ups of lots of interesting topics like these in "Machine Learning Q and AI"
 https://leanpub.com/machine-learning-q-and-ai/…

Multi-GPU training, evaluating LLMs,… Show more
------
Prefix tuning and adapters are the 2 out of the 3 most widely used parameter-efficient finetuning methods for large language models (LLMs).

If you look closely, the recent LLaMA-Adapter method that made big waves is actually a prefix tuning method, not an adapter method.

1/2  twitter.com/rasbt/status/1…
------
Prefix tuning and adapters are the 2 out of the 3 most widely used parameter-efficient finetuning methods for large language models (LLMs).

If you look closely, the recent LLaMA-Adapter method that made big waves is actually a prefix tuning method, not an adapter method.

1/2
------
LLaMA-Adapter: finetuning large language models (LLMs) like LLaMA and matching Alpaca's modeling performance with greater finetuning efficiency

Let's have a look at this new paper (https://arxiv.org/abs/2303.16199) that proposes an adapter method for LLaMA instruction finetuning

1/5
------
In prefix tuning, we prepend trainable tensors (soft prompts) to each transformer block.

In the original adapter, fully connected layers were added after the multihead self-attention & existing fully connected layers in each transformer block.

Easier to see in pseudocode:

2/2
------
On that note, I probably wouldn't consider Meta's SAM a generative model, but yeah, in a sense it's generating segmentation masks. 
I was more thinking about stable diffusion-like models aka "generative AI" for images and videos  (diffusion models, GANs, etc.).
------
Finetuning vs prompting?
Here's a nice empirical insight from https://arxiv.org/abs/2104.08691 illustrating that finetuning  outperforms prompting.

(Caveat: I wish they had done a per-dataset analysis to learn what how much labeled data is needed to outperform prompting)
------
Does that mean we don't need generalist LLMs? Here's the thing: we can also see in the plot above that the larger the finetuned LLM, the better. So, it would make sense to further develop large generalist LLMs, which can then be finetuned to become specialist LLMs.
------
BREAKING: AutoGPT to replace a significant amount of the workforce.

The first job threatened by AutoGPT is the "busy fool".

- Appear busy all day long
- Tell everyone you're busy
- Produce nothing

If this applies to your job, you should be worried.
------
Speaking of finetuning ... just saw Databricks open-sourced their databricks-dolly-15k dataset! 

https://github.com/databrickslabs/dolly/tree/master/data…

That's 15,000 high-quality human-generated prompt / response pairs specifically designed for ChatGPT-like instruction tuning.

Something fun to play with!!
------
Want to adapt large language models (LLMs) to your target domain? Parameter-efficient finetuning is and will remain the way forward!

I just distilled the main concepts, from prefix tuning to LLaMA-Adapters, in a new blog article here:

  https://lightning.ai/pages/community/article/understanding-llama-adapters/…
------
Want to adapt large language models (LLMs) to your target domain? Parameter-efficient finetuning is and will remain the way forward!

I just distilled the main concepts, from prefix tuning to LLaMA-Adapters, in a new blog article here:

  https://lightning.ai/pages/community/article/understanding-llama-adapters/…
------
Open-source efforts empower us to harness cutting-edge tech.  

And new tools like parameter-efficient fine-tuning techniques let us tailor LLMs for our needs  

Curious about LLaMA-Adapters?  Learn how they work in our latest blog post by @rasbt https://bit.ly/41ivvs6
------
It’s almost trivial nowadays to download & use a pretrained LLM for SOTA language translation but users will still flock to Google Translate because of the UI.
------
As we approach the one-year anniversary since the launch of PyTorch DataPipes, I still find myself using the conventional Dataset class .

Interested to hear if you have adopted DataPipes, and why / why not?

By the way, I blogged about DataPipes here:
------
In recent days, I shared various resources on finetuning large language models. Multiple people reached out asking what I think about in-context learning.

1/6
------
For LLMs, indexing with in-context learning transforms them into information retrieval systems, extracting data from external sources and websites. The indexing module dissects docs or websites into smaller pieces, converting them into vectors for storage in a database.

5/6
------
Upon receiving a user query, the indexing module calculates vector similarity betw the embedded query & stored vectors. It then retrieves the top k most similar embeddings to compose the response. A popular implementation of this is  LLaMA-Index: https://github.com/jerryjliu/llama_index…

6/6
------
In-context learning is quite impressive & capable, though (as we have already seen even back then with older models such as GPT-2 & GPT-3). Researchers have recently begun developing interesting exploits allowing LLMs to access  external databases & documents w/o finetuning.

4/6
------
Similar to "pretraining" can we please switch to "finetuning" instead of "fine-tuning"? 

The latter looks like mini-batch and on-line gradient descent training for fully-connected layers  
------
LLMs just hit a major milestone with the release of the new "Generative agents" paper.

By using LLMs, generative agents were able to simulate human-like behavior in an interactive sandbox inspired by The Sims.

The agent architecture extends Language Models to store a complete… Show more
------
Yesterday, I talked about 2 of the 3 most popular parameter-efficient techniques to finetune large language models (LLMs). 

The 3rd method is Low-Rank Adaptation (LoRA) of course!

1/9
------
Prefix tuning and adapters are the 2 out of the 3 most widely used parameter-efficient finetuning methods for large language models (LLMs).

If you look closely, the recent LLaMA-Adapter method that made big waves is actually a prefix tuning method, not an adapter method.

1/2  twitter.com/rasbt/status/1…
------
For LoRA, during production, we can precompute and save the updated weights so there is zero overhead: 
W_updated = W_pretrained + ∆W 
= W_pretrained + A x B

8/9
------
Here's the full LoRA paper if you want to check it out: https://arxiv.org/abs/2106.09685

PS: We also added LoRA to Lit-LLaMA last week: https://github.com/Lightning-AI/lit-llama/blob/main/finetune_lora.py…

9/9
------
Got asked whether I collect these types of threads for easier reference somewhere.

I actually do! 

Or rather, I have more polished write-ups of lots of interesting topics like these in "Machine Learning Q and AI"
 https://leanpub.com/machine-learning-q-and-ai/…

Multi-GPU training, evaluating LLMs,… Show more
------
Prefix tuning and adapters are the 2 out of the 3 most widely used parameter-efficient finetuning methods for large language models (LLMs).

If you look closely, the recent LLaMA-Adapter method that made big waves is actually a prefix tuning method, not an adapter method.

1/2  twitter.com/rasbt/status/1…
------
Prefix tuning and adapters are the 2 out of the 3 most widely used parameter-efficient finetuning methods for large language models (LLMs).

If you look closely, the recent LLaMA-Adapter method that made big waves is actually a prefix tuning method, not an adapter method.

1/2
------
LLaMA-Adapter: finetuning large language models (LLMs) like LLaMA and matching Alpaca's modeling performance with greater finetuning efficiency

Let's have a look at this new paper (https://arxiv.org/abs/2303.16199) that proposes an adapter method for LLaMA instruction finetuning

1/5
------
In prefix tuning, we prepend trainable tensors (soft prompts) to each transformer block.

In the original adapter, fully connected layers were added after the multihead self-attention & existing fully connected layers in each transformer block.

Easier to see in pseudocode:

2/2
------
In addition, what is your experience with / are your thoughts on training LLMs for more epochs on the subset of that data we already have? 

It might increase undesireable memorization, but on the other hand, I don't think we fully utilize high quality data we already have
------
Want to adapt large language models (LLMs) to your target domain? Parameter-efficient finetuning is and will remain the way forward!

I just distilled the main concepts, from prefix tuning to LLaMA-Adapters, in a new blog article here:

  https://lightning.ai/pages/community/article/understanding-llama-adapters/…
------
Open-source efforts empower us to harness cutting-edge tech.  

And new tools like parameter-efficient fine-tuning techniques let us tailor LLMs for our needs  

Curious about LLaMA-Adapters?  Learn how they work in our latest blog post by @rasbt https://bit.ly/41ivvs6
------
It’s almost trivial nowadays to download & use a pretrained LLM for SOTA language translation but users will still flock to Google Translate because of the UI.
------
As we approach the one-year anniversary since the launch of PyTorch DataPipes, I still find myself using the conventional Dataset class .

Interested to hear if you have adopted DataPipes, and why / why not?

By the way, I blogged about DataPipes here:
------
In recent days, I shared various resources on finetuning large language models. Multiple people reached out asking what I think about in-context learning.

1/6
------
For LLMs, indexing with in-context learning transforms them into information retrieval systems, extracting data from external sources and websites. The indexing module dissects docs or websites into smaller pieces, converting them into vectors for storage in a database.

5/6
------
Upon receiving a user query, the indexing module calculates vector similarity betw the embedded query & stored vectors. It then retrieves the top k most similar embeddings to compose the response. A popular implementation of this is  LLaMA-Index: https://github.com/jerryjliu/llama_index…

6/6
------
In-context learning is quite impressive & capable, though (as we have already seen even back then with older models such as GPT-2 & GPT-3). Researchers have recently begun developing interesting exploits allowing LLMs to access  external databases & documents w/o finetuning.

4/6
------
Similar to "pretraining" can we please switch to "finetuning" instead of "fine-tuning"? 

The latter looks like mini-batch and on-line gradient descent training for fully-connected layers  
------
LLMs just hit a major milestone with the release of the new "Generative agents" paper.

By using LLMs, generative agents were able to simulate human-like behavior in an interactive sandbox inspired by The Sims.

The agent architecture extends Language Models to store a complete… Show more
------
Yesterday, I talked about 2 of the 3 most popular parameter-efficient techniques to finetune large language models (LLMs). 

The 3rd method is Low-Rank Adaptation (LoRA) of course!

1/9
------
Prefix tuning and adapters are the 2 out of the 3 most widely used parameter-efficient finetuning methods for large language models (LLMs).

If you look closely, the recent LLaMA-Adapter method that made big waves is actually a prefix tuning method, not an adapter method.

1/2  twitter.com/rasbt/status/1…
------
For LoRA, during production, we can precompute and save the updated weights so there is zero overhead: 
W_updated = W_pretrained + ∆W 
= W_pretrained + A x B

8/9
------
Here's the full LoRA paper if you want to check it out: https://arxiv.org/abs/2106.09685

PS: We also added LoRA to Lit-LLaMA last week: https://github.com/Lightning-AI/lit-llama/blob/main/finetune_lora.py…

9/9
------
Got asked whether I collect these types of threads for easier reference somewhere.

I actually do! 

Or rather, I have more polished write-ups of lots of interesting topics like these in "Machine Learning Q and AI"
 https://leanpub.com/machine-learning-q-and-ai/…

Multi-GPU training, evaluating LLMs,… Show more
------
Prefix tuning and adapters are the 2 out of the 3 most widely used parameter-efficient finetuning methods for large language models (LLMs).

If you look closely, the recent LLaMA-Adapter method that made big waves is actually a prefix tuning method, not an adapter method.

1/2  twitter.com/rasbt/status/1…
------
Prefix tuning and adapters are the 2 out of the 3 most widely used parameter-efficient finetuning methods for large language models (LLMs).

If you look closely, the recent LLaMA-Adapter method that made big waves is actually a prefix tuning method, not an adapter method.

1/2
------
LLaMA-Adapter: finetuning large language models (LLMs) like LLaMA and matching Alpaca's modeling performance with greater finetuning efficiency

Let's have a look at this new paper (https://arxiv.org/abs/2303.16199) that proposes an adapter method for LLaMA instruction finetuning

1/5
------
In prefix tuning, we prepend trainable tensors (soft prompts) to each transformer block.

In the original adapter, fully connected layers were added after the multihead self-attention & existing fully connected layers in each transformer block.

Easier to see in pseudocode:

2/2
------
In addition, what is your experience with / are your thoughts on training LLMs for more epochs on the subset of that data we already have? 

It might increase undesireable memorization, but on the other hand, I don't think we fully utilize high quality data we already have
------
Optimizing BOTH learning rates & schedulers is vital for efficient convergence in neural net training.

Want to learn more about learning rates & scheduling in PyTorch? I covered the essential techniques in this short series of videos (~30 min in total):

https://lightning.ai/pages/courses/deep-learning-fundamentals/unit-6-overview-essential-deep-learning-tips-tricks/unit-6.2-learning-rates-and-learning-rate-schedulers/…
------
(Fine)tuning is what allows us to adjust pretrained large language models for our practical use cases. 

Before diving into LoRA, let's talk about the different ways to modify the prompts and clarify the difference between prompt tuning and prefix tuning.
1/5
------
Yesterday, I covered the 3 classic ways to finetune LLMs. Let's now delve into parameter-efficient finetuning techniques. 

Parameter Efficient Finetuning Part I: let's start  with Prefix Finetuning. 

1/6  twitter.com/rasbt/status/1…
------
Prefix tuning is essentially a flavor or "soft" prompt tuning. However, here we add the trainable tensors (soft prompts) to each transformer block. 
(Note that transformer blocks usually have 2 fully connected layers instead of an embedding layer)

4/5
------
Some references if you want to learn more:

[Soft prompt tuning]
The Power of Scale for Parameter-Efficient Prompt Tuning (https://arxiv.org/abs/2104.08691)

[Prefix tuning]
Prefix-Tuning: Optimizing Continuous Prompts for Generation (https://arxiv.org/abs/2101.00190)

5/5
------
Progress update!

Lit-LLaMA now implements the LLaMA-Adapter method for efficient fine-tuning 

The core idea can be implemented in about 11 lines of code (see screenshot)

Link to repo https://github.com/Lightning-AI/lit-llama…
Link to Adapter paperhttp://arxiv.org/abs/2303.16199
------
Working with ordered data (like customer feedback) and want to leverage large language models?

Added a hands-on example using our CORN loss -- all it takes is swapping out the loss function!

Code: https://github.com/Raschka-research-group/coral-pytorch/blob/main/docs/tutorials/pytorch_lightning/distilbert-corn-tripadvisor.ipynb…

Paper: https://arxiv.org/abs/2111.08851
------
For LLMs, indexing with in-context learning transforms them into information retrieval systems, extracting data from external sources and websites. The indexing module dissects docs or websites into smaller pieces, converting them into vectors for storage in a database.

5/6
------
Upon receiving a user query, the indexing module calculates vector similarity betw the embedded query & stored vectors. It then retrieves the top k most similar embeddings to compose the response. A popular implementation of this is  LLaMA-Index: https://github.com/jerryjliu/llama_index…

6/6
------
In-context learning is quite impressive & capable, though (as we have already seen even back then with older models such as GPT-2 & GPT-3). Researchers have recently begun developing interesting exploits allowing LLMs to access  external databases & documents w/o finetuning.

4/6
------
Similar to "pretraining" can we please switch to "finetuning" instead of "fine-tuning"? 

The latter looks like mini-batch and on-line gradient descent training for fully-connected layers  
------
LLMs just hit a major milestone with the release of the new "Generative agents" paper.

By using LLMs, generative agents were able to simulate human-like behavior in an interactive sandbox inspired by The Sims.

The agent architecture extends Language Models to store a complete… Show more
------
Yesterday, I talked about 2 of the 3 most popular parameter-efficient techniques to finetune large language models (LLMs). 

The 3rd method is Low-Rank Adaptation (LoRA) of course!

1/9
------
Prefix tuning and adapters are the 2 out of the 3 most widely used parameter-efficient finetuning methods for large language models (LLMs).

If you look closely, the recent LLaMA-Adapter method that made big waves is actually a prefix tuning method, not an adapter method.

1/2  twitter.com/rasbt/status/1…
------
For LoRA, during production, we can precompute and save the updated weights so there is zero overhead: 
W_updated = W_pretrained + ∆W 
= W_pretrained + A x B

8/9
------
Here's the full LoRA paper if you want to check it out: https://arxiv.org/abs/2106.09685

PS: We also added LoRA to Lit-LLaMA last week: https://github.com/Lightning-AI/lit-llama/blob/main/finetune_lora.py…

9/9
------
Got asked whether I collect these types of threads for easier reference somewhere.

I actually do! 

Or rather, I have more polished write-ups of lots of interesting topics like these in "Machine Learning Q and AI"
 https://leanpub.com/machine-learning-q-and-ai/…

Multi-GPU training, evaluating LLMs,… Show more
------
Prefix tuning and adapters are the 2 out of the 3 most widely used parameter-efficient finetuning methods for large language models (LLMs).

If you look closely, the recent LLaMA-Adapter method that made big waves is actually a prefix tuning method, not an adapter method.

1/2  twitter.com/rasbt/status/1…
------
Prefix tuning and adapters are the 2 out of the 3 most widely used parameter-efficient finetuning methods for large language models (LLMs).

If you look closely, the recent LLaMA-Adapter method that made big waves is actually a prefix tuning method, not an adapter method.

1/2
------
LLaMA-Adapter: finetuning large language models (LLMs) like LLaMA and matching Alpaca's modeling performance with greater finetuning efficiency

Let's have a look at this new paper (https://arxiv.org/abs/2303.16199) that proposes an adapter method for LLaMA instruction finetuning

1/5
------
In prefix tuning, we prepend trainable tensors (soft prompts) to each transformer block.

In the original adapter, fully connected layers were added after the multihead self-attention & existing fully connected layers in each transformer block.

Easier to see in pseudocode:

2/2
------
In addition, what is your experience with / are your thoughts on training LLMs for more epochs on the subset of that data we already have? 

It might increase undesireable memorization, but on the other hand, I don't think we fully utilize high quality data we already have
------
Optimizing BOTH learning rates & schedulers is vital for efficient convergence in neural net training.

Want to learn more about learning rates & scheduling in PyTorch? I covered the essential techniques in this short series of videos (~30 min in total):

https://lightning.ai/pages/courses/deep-learning-fundamentals/unit-6-overview-essential-deep-learning-tips-tricks/unit-6.2-learning-rates-and-learning-rate-schedulers/…
------
(Fine)tuning is what allows us to adjust pretrained large language models for our practical use cases. 

Before diving into LoRA, let's talk about the different ways to modify the prompts and clarify the difference between prompt tuning and prefix tuning.
1/5
------
Yesterday, I covered the 3 classic ways to finetune LLMs. Let's now delve into parameter-efficient finetuning techniques. 

Parameter Efficient Finetuning Part I: let's start  with Prefix Finetuning. 

1/6  twitter.com/rasbt/status/1…
------
Prefix tuning is essentially a flavor or "soft" prompt tuning. However, here we add the trainable tensors (soft prompts) to each transformer block. 
(Note that transformer blocks usually have 2 fully connected layers instead of an embedding layer)

4/5
------
Some references if you want to learn more:

[Soft prompt tuning]
The Power of Scale for Parameter-Efficient Prompt Tuning (https://arxiv.org/abs/2104.08691)

[Prefix tuning]
Prefix-Tuning: Optimizing Continuous Prompts for Generation (https://arxiv.org/abs/2101.00190)

5/5
------
Progress update!

Lit-LLaMA now implements the LLaMA-Adapter method for efficient fine-tuning 

The core idea can be implemented in about 11 lines of code (see screenshot)

Link to repo https://github.com/Lightning-AI/lit-llama…
Link to Adapter paperhttp://arxiv.org/abs/2303.16199
------
Working with ordered data (like customer feedback) and want to leverage large language models?

Added a hands-on example using our CORN loss -- all it takes is swapping out the loss function!

Code: https://github.com/Raschka-research-group/coral-pytorch/blob/main/docs/tutorials/pytorch_lightning/distilbert-corn-tripadvisor.ipynb…

Paper: https://arxiv.org/abs/2111.08851
------
And accompanying the LLM code example, here's a video tutorial on "Conditional Ordinal Regression for Neural Networks (CORN)":
------
A new Deep Learning Fundamentals unit just launched!

 http://lightning.ai/pages/dlf

In this unit, I am excited to show you some of my favorite deep learning tips & tricks

- learning rate schedulers & cosine annealing
- CLI configs for hparam sweeps
- debugging w. batch overfitting
------
 Unit 6 of @rasbt's Deep Learning Fundamentals course just dropped! 

In this unit, he covers:  
model checkpointing 
learning rate finders and schedulers 
different optimization algorithms  
activation functions.  

Click here to start the unit now … Show more
------
A new Deep Learning Fundamentals unit just launched!

 http://lightning.ai/pages/dlf

In this unit, I am excited to show you some of my favorite deep learning tips & tricks

- learning rate schedulers & cosine annealing
- CLI configs for hparam sweeps
- debugging w. batch overfitting
------
 Unit 6 of @rasbt's Deep Learning Fundamentals course just dropped! 

In this unit, he covers:  
model checkpointing 
learning rate finders and schedulers 
different optimization algorithms  
activation functions.  

Click here to start the unit now … Show more
------
LLMs just hit a major milestone with the release of the new "Generative agents" paper.

By using LLMs, generative agents were able to simulate human-like behavior in an interactive sandbox inspired by The Sims.

The agent architecture extends Language Models to store a complete… Show more
------
Yesterday, I talked about 2 of the 3 most popular parameter-efficient techniques to finetune large language models (LLMs). 

The 3rd method is Low-Rank Adaptation (LoRA) of course!

1/9
------
Prefix tuning and adapters are the 2 out of the 3 most widely used parameter-efficient finetuning methods for large language models (LLMs).

If you look closely, the recent LLaMA-Adapter method that made big waves is actually a prefix tuning method, not an adapter method.

1/2  twitter.com/rasbt/status/1…
------
For LoRA, during production, we can precompute and save the updated weights so there is zero overhead: 
W_updated = W_pretrained + ∆W 
= W_pretrained + A x B

8/9
------
Here's the full LoRA paper if you want to check it out: https://arxiv.org/abs/2106.09685

PS: We also added LoRA to Lit-LLaMA last week: https://github.com/Lightning-AI/lit-llama/blob/main/finetune_lora.py…

9/9
------
Got asked whether I collect these types of threads for easier reference somewhere.

I actually do! 

Or rather, I have more polished write-ups of lots of interesting topics like these in "Machine Learning Q and AI"
 https://leanpub.com/machine-learning-q-and-ai/…

Multi-GPU training, evaluating LLMs,… Show more
------
Prefix tuning and adapters are the 2 out of the 3 most widely used parameter-efficient finetuning methods for large language models (LLMs).

If you look closely, the recent LLaMA-Adapter method that made big waves is actually a prefix tuning method, not an adapter method.

1/2  twitter.com/rasbt/status/1…
------
Prefix tuning and adapters are the 2 out of the 3 most widely used parameter-efficient finetuning methods for large language models (LLMs).

If you look closely, the recent LLaMA-Adapter method that made big waves is actually a prefix tuning method, not an adapter method.

1/2
------
LLaMA-Adapter: finetuning large language models (LLMs) like LLaMA and matching Alpaca's modeling performance with greater finetuning efficiency

Let's have a look at this new paper (https://arxiv.org/abs/2303.16199) that proposes an adapter method for LLaMA instruction finetuning

1/5
------
In prefix tuning, we prepend trainable tensors (soft prompts) to each transformer block.

In the original adapter, fully connected layers were added after the multihead self-attention & existing fully connected layers in each transformer block.

Easier to see in pseudocode:

2/2
------
In addition, what is your experience with / are your thoughts on training LLMs for more epochs on the subset of that data we already have? 

It might increase undesireable memorization, but on the other hand, I don't think we fully utilize high quality data we already have
------
Optimizing BOTH learning rates & schedulers is vital for efficient convergence in neural net training.

Want to learn more about learning rates & scheduling in PyTorch? I covered the essential techniques in this short series of videos (~30 min in total):

https://lightning.ai/pages/courses/deep-learning-fundamentals/unit-6-overview-essential-deep-learning-tips-tricks/unit-6.2-learning-rates-and-learning-rate-schedulers/…
------
(Fine)tuning is what allows us to adjust pretrained large language models for our practical use cases. 

Before diving into LoRA, let's talk about the different ways to modify the prompts and clarify the difference between prompt tuning and prefix tuning.
1/5
------
Yesterday, I covered the 3 classic ways to finetune LLMs. Let's now delve into parameter-efficient finetuning techniques. 

Parameter Efficient Finetuning Part I: let's start  with Prefix Finetuning. 

1/6  twitter.com/rasbt/status/1…
------
Prefix tuning is essentially a flavor or "soft" prompt tuning. However, here we add the trainable tensors (soft prompts) to each transformer block. 
(Note that transformer blocks usually have 2 fully connected layers instead of an embedding layer)

4/5
------
Some references if you want to learn more:

[Soft prompt tuning]
The Power of Scale for Parameter-Efficient Prompt Tuning (https://arxiv.org/abs/2104.08691)

[Prefix tuning]
Prefix-Tuning: Optimizing Continuous Prompts for Generation (https://arxiv.org/abs/2101.00190)

5/5
------
Progress update!

Lit-LLaMA now implements the LLaMA-Adapter method for efficient fine-tuning 

The core idea can be implemented in about 11 lines of code (see screenshot)

Link to repo https://github.com/Lightning-AI/lit-llama…
Link to Adapter paperhttp://arxiv.org/abs/2303.16199
------
Working with ordered data (like customer feedback) and want to leverage large language models?

Added a hands-on example using our CORN loss -- all it takes is swapping out the loss function!

Code: https://github.com/Raschka-research-group/coral-pytorch/blob/main/docs/tutorials/pytorch_lightning/distilbert-corn-tripadvisor.ipynb…

Paper: https://arxiv.org/abs/2111.08851
------
And accompanying the LLM code example, here's a video tutorial on "Conditional Ordinal Regression for Neural Networks (CORN)":
------
A new Deep Learning Fundamentals unit just launched!

 http://lightning.ai/pages/dlf

In this unit, I am excited to show you some of my favorite deep learning tips & tricks

- learning rate schedulers & cosine annealing
- CLI configs for hparam sweeps
- debugging w. batch overfitting
------
 Unit 6 of @rasbt's Deep Learning Fundamentals course just dropped! 

In this unit, he covers:  
model checkpointing 
learning rate finders and schedulers 
different optimization algorithms  
activation functions.  

Click here to start the unit now … Show more
------
A new Deep Learning Fundamentals unit just launched!

 http://lightning.ai/pages/dlf

In this unit, I am excited to show you some of my favorite deep learning tips & tricks

- learning rate schedulers & cosine annealing
- CLI configs for hparam sweeps
- debugging w. batch overfitting
------
 Unit 6 of @rasbt's Deep Learning Fundamentals course just dropped! 

In this unit, he covers:  
model checkpointing 
learning rate finders and schedulers 
different optimization algorithms  
activation functions.  

Click here to start the unit now … Show more
------
Btw. I am also covering step and reduce-on-plateau schedulers, techniques to reduce overfitting, and model checkpointing!

Lots of practical code examples in this one!
------
Large Language Models 3.0

In the new issue of Ahead of AI, I am discussing what's next for LLMs and developments centered around parameter efficiency and multimodality. 

It's also been a particularly strong month for open-source AI!


------
Should we train our own large language model (LLM) on domain-specific data from scratch? 
Researchers at Bloomberg did just that and shared a detailed technical report describing the dataset, model configuration, and training procedure. 

1/9
------
Assuming the ~53 days training time that was mentioned and doing some napkin-math assuming a discounted $1.1 per hour rate for a A100 GPU, the cost would be
1274 (hours) * 1.1 (dollars per hour) 64 * 8 (gpus) = $90k*

(not too bad maybe; I think in the LLaMA paper they reported… Show more
------
Omg how did I get $90k. I feel like my brain did a ChatGPT here and I guess I should have used a calculator. It's ~$700k of course!
------
We don't know. However, if you want to use the combined pretraining approach, BloombergGPT delivers an excellently described blueprint.

9/9
------
Got asked whether I collect these types of threads for easier reference somewhere.

I actually do! 

Or rather, I have more polished write-ups of lots of interesting topics like these in "Machine Learning Q and AI"
 https://leanpub.com/machine-learning-q-and-ai/…

Multi-GPU training, evaluating LLMs,… Show more
------
Prefix tuning and adapters are the 2 out of the 3 most widely used parameter-efficient finetuning methods for large language models (LLMs).

If you look closely, the recent LLaMA-Adapter method that made big waves is actually a prefix tuning method, not an adapter method.

1/2  twitter.com/rasbt/status/1…
------
Prefix tuning and adapters are the 2 out of the 3 most widely used parameter-efficient finetuning methods for large language models (LLMs).

If you look closely, the recent LLaMA-Adapter method that made big waves is actually a prefix tuning method, not an adapter method.

1/2
------
LLaMA-Adapter: finetuning large language models (LLMs) like LLaMA and matching Alpaca's modeling performance with greater finetuning efficiency

Let's have a look at this new paper (https://arxiv.org/abs/2303.16199) that proposes an adapter method for LLaMA instruction finetuning

1/5
------
In prefix tuning, we prepend trainable tensors (soft prompts) to each transformer block.

In the original adapter, fully connected layers were added after the multihead self-attention & existing fully connected layers in each transformer block.

Easier to see in pseudocode:

2/2
------
In addition, what is your experience with / are your thoughts on training LLMs for more epochs on the subset of that data we already have? 

It might increase undesireable memorization, but on the other hand, I don't think we fully utilize high quality data we already have
------
Optimizing BOTH learning rates & schedulers is vital for efficient convergence in neural net training.

Want to learn more about learning rates & scheduling in PyTorch? I covered the essential techniques in this short series of videos (~30 min in total):

https://lightning.ai/pages/courses/deep-learning-fundamentals/unit-6-overview-essential-deep-learning-tips-tricks/unit-6.2-learning-rates-and-learning-rate-schedulers/…
------
(Fine)tuning is what allows us to adjust pretrained large language models for our practical use cases. 

Before diving into LoRA, let's talk about the different ways to modify the prompts and clarify the difference between prompt tuning and prefix tuning.
1/5
------
Yesterday, I covered the 3 classic ways to finetune LLMs. Let's now delve into parameter-efficient finetuning techniques. 

Parameter Efficient Finetuning Part I: let's start  with Prefix Finetuning. 

1/6  twitter.com/rasbt/status/1…
------
Prefix tuning is essentially a flavor or "soft" prompt tuning. However, here we add the trainable tensors (soft prompts) to each transformer block. 
(Note that transformer blocks usually have 2 fully connected layers instead of an embedding layer)

4/5
------
Some references if you want to learn more:

[Soft prompt tuning]
The Power of Scale for Parameter-Efficient Prompt Tuning (https://arxiv.org/abs/2104.08691)

[Prefix tuning]
Prefix-Tuning: Optimizing Continuous Prompts for Generation (https://arxiv.org/abs/2101.00190)

5/5
------
Progress update!

Lit-LLaMA now implements the LLaMA-Adapter method for efficient fine-tuning 

The core idea can be implemented in about 11 lines of code (see screenshot)

Link to repo https://github.com/Lightning-AI/lit-llama…
Link to Adapter paperhttp://arxiv.org/abs/2303.16199
------
Working with ordered data (like customer feedback) and want to leverage large language models?

Added a hands-on example using our CORN loss -- all it takes is swapping out the loss function!

Code: https://github.com/Raschka-research-group/coral-pytorch/blob/main/docs/tutorials/pytorch_lightning/distilbert-corn-tripadvisor.ipynb…

Paper: https://arxiv.org/abs/2111.08851
------
And accompanying the LLM code example, here's a video tutorial on "Conditional Ordinal Regression for Neural Networks (CORN)":
------
A new Deep Learning Fundamentals unit just launched!

 http://lightning.ai/pages/dlf

In this unit, I am excited to show you some of my favorite deep learning tips & tricks

- learning rate schedulers & cosine annealing
- CLI configs for hparam sweeps
- debugging w. batch overfitting
------
 Unit 6 of @rasbt's Deep Learning Fundamentals course just dropped! 

In this unit, he covers:  
model checkpointing 
learning rate finders and schedulers 
different optimization algorithms  
activation functions.  

Click here to start the unit now … Show more
------
A new Deep Learning Fundamentals unit just launched!

 http://lightning.ai/pages/dlf

In this unit, I am excited to show you some of my favorite deep learning tips & tricks

- learning rate schedulers & cosine annealing
- CLI configs for hparam sweeps
- debugging w. batch overfitting
------
 Unit 6 of @rasbt's Deep Learning Fundamentals course just dropped! 

In this unit, he covers:  
model checkpointing 
learning rate finders and schedulers 
different optimization algorithms  
activation functions.  

Click here to start the unit now … Show more
------
Btw. I am also covering step and reduce-on-plateau schedulers, techniques to reduce overfitting, and model checkpointing!

Lots of practical code examples in this one!
------
Large Language Models 3.0

In the new issue of Ahead of AI, I am discussing what's next for LLMs and developments centered around parameter efficiency and multimodality. 

It's also been a particularly strong month for open-source AI!


------
Should we train our own large language model (LLM) on domain-specific data from scratch? 
Researchers at Bloomberg did just that and shared a detailed technical report describing the dataset, model configuration, and training procedure. 

1/9
------
Assuming the ~53 days training time that was mentioned and doing some napkin-math assuming a discounted $1.1 per hour rate for a A100 GPU, the cost would be
1274 (hours) * 1.1 (dollars per hour) 64 * 8 (gpus) = $90k*

(not too bad maybe; I think in the LLaMA paper they reported… Show more
------
Omg how did I get $90k. I feel like my brain did a ChatGPT here and I guess I should have used a calculator. It's ~$700k of course!
------
We don't know. However, if you want to use the combined pretraining approach, BloombergGPT delivers an excellently described blueprint.

9/9
------
While I was mentioning that I was curious to see comparisons (and ideally ablation studies), I want to emphasize again that this is an excellent technical report. 

For researchers and practitioners interested in LLMs, it's a really nice recipe that's worth bookmarking. 

Here's… Show more
------
CC 
@bhutanisanyam1
 since you were curious about my thoughts about this paper ^^
------
In prefix tuning, we prepend trainable tensors (soft prompts) to each transformer block.

In the original adapter, fully connected layers were added after the multihead self-attention & existing fully connected layers in each transformer block.

Easier to see in pseudocode:

2/2
------
In addition, what is your experience with / are your thoughts on training LLMs for more epochs on the subset of that data we already have? 

It might increase undesireable memorization, but on the other hand, I don't think we fully utilize high quality data we already have
------
Optimizing BOTH learning rates & schedulers is vital for efficient convergence in neural net training.

Want to learn more about learning rates & scheduling in PyTorch? I covered the essential techniques in this short series of videos (~30 min in total):

https://lightning.ai/pages/courses/deep-learning-fundamentals/unit-6-overview-essential-deep-learning-tips-tricks/unit-6.2-learning-rates-and-learning-rate-schedulers/…
------
(Fine)tuning is what allows us to adjust pretrained large language models for our practical use cases. 

Before diving into LoRA, let's talk about the different ways to modify the prompts and clarify the difference between prompt tuning and prefix tuning.
1/5
------
Yesterday, I covered the 3 classic ways to finetune LLMs. Let's now delve into parameter-efficient finetuning techniques. 

Parameter Efficient Finetuning Part I: let's start  with Prefix Finetuning. 

1/6  twitter.com/rasbt/status/1…
------
Prefix tuning is essentially a flavor or "soft" prompt tuning. However, here we add the trainable tensors (soft prompts) to each transformer block. 
(Note that transformer blocks usually have 2 fully connected layers instead of an embedding layer)

4/5
------
Some references if you want to learn more:

[Soft prompt tuning]
The Power of Scale for Parameter-Efficient Prompt Tuning (https://arxiv.org/abs/2104.08691)

[Prefix tuning]
Prefix-Tuning: Optimizing Continuous Prompts for Generation (https://arxiv.org/abs/2101.00190)

5/5
------
Progress update!

Lit-LLaMA now implements the LLaMA-Adapter method for efficient fine-tuning 

The core idea can be implemented in about 11 lines of code (see screenshot)

Link to repo https://github.com/Lightning-AI/lit-llama…
Link to Adapter paperhttp://arxiv.org/abs/2303.16199
------
Working with ordered data (like customer feedback) and want to leverage large language models?

Added a hands-on example using our CORN loss -- all it takes is swapping out the loss function!

Code: https://github.com/Raschka-research-group/coral-pytorch/blob/main/docs/tutorials/pytorch_lightning/distilbert-corn-tripadvisor.ipynb…

Paper: https://arxiv.org/abs/2111.08851
------
And accompanying the LLM code example, here's a video tutorial on "Conditional Ordinal Regression for Neural Networks (CORN)":
------
A new Deep Learning Fundamentals unit just launched!

 http://lightning.ai/pages/dlf

In this unit, I am excited to show you some of my favorite deep learning tips & tricks

- learning rate schedulers & cosine annealing
- CLI configs for hparam sweeps
- debugging w. batch overfitting
------
 Unit 6 of @rasbt's Deep Learning Fundamentals course just dropped! 

In this unit, he covers:  
model checkpointing 
learning rate finders and schedulers 
different optimization algorithms  
activation functions.  

Click here to start the unit now … Show more
------
A new Deep Learning Fundamentals unit just launched!

 http://lightning.ai/pages/dlf

In this unit, I am excited to show you some of my favorite deep learning tips & tricks

- learning rate schedulers & cosine annealing
- CLI configs for hparam sweeps
- debugging w. batch overfitting
------
 Unit 6 of @rasbt's Deep Learning Fundamentals course just dropped! 

In this unit, he covers:  
model checkpointing 
learning rate finders and schedulers 
different optimization algorithms  
activation functions.  

Click here to start the unit now … Show more
------
Btw. I am also covering step and reduce-on-plateau schedulers, techniques to reduce overfitting, and model checkpointing!

Lots of practical code examples in this one!
------
Large Language Models 3.0

In the new issue of Ahead of AI, I am discussing what's next for LLMs and developments centered around parameter efficiency and multimodality. 

It's also been a particularly strong month for open-source AI!


------
Should we train our own large language model (LLM) on domain-specific data from scratch? 
Researchers at Bloomberg did just that and shared a detailed technical report describing the dataset, model configuration, and training procedure. 

1/9
------
Assuming the ~53 days training time that was mentioned and doing some napkin-math assuming a discounted $1.1 per hour rate for a A100 GPU, the cost would be
1274 (hours) * 1.1 (dollars per hour) 64 * 8 (gpus) = $90k*

(not too bad maybe; I think in the LLaMA paper they reported… Show more
------
Omg how did I get $90k. I feel like my brain did a ChatGPT here and I guess I should have used a calculator. It's ~$700k of course!
------
We don't know. However, if you want to use the combined pretraining approach, BloombergGPT delivers an excellently described blueprint.

9/9
------
While I was mentioning that I was curious to see comparisons (and ideally ablation studies), I want to emphasize again that this is an excellent technical report. 

For researchers and practitioners interested in LLMs, it's a really nice recipe that's worth bookmarking. 

Here's… Show more
------
CC 
@bhutanisanyam1
 since you were curious about my thoughts about this paper ^^
------
Is it worth (pre)training the LLM on the combined dataset from scratch? BloombergGPT performs really well in the target domain. However, is it better than a) further pretraining a pretrained model on domain data or b) finetuning a pretrained model on domain-specific data?

8/9
------
Open Source Sunday!
Just released a new version of MLxtend: https://rasbt.github.io/mlxtend/

Featuring
- a snappier ExhaustiveFeatureSelector
- the H-Mine frequent pattern mining algo
- multiprocessing for plot_decision_regions

Thx to contributors, Fatih Sen, Nima Sarajpoor & others
------
Yesterday, I started discussing parameter-efficient finetuning methods for LLMs. Before I delve deeper into the topic and cover more methods in the next couple of days, I wanted to share a birds-eye view from the excellent "Scale Down to Scale Up" survey.

1/6
------
Yesterday, I covered the 3 classic ways to finetune LLMs. Let's now delve into parameter-efficient finetuning techniques. 

Parameter Efficient Finetuning Part I: let's start  with Prefix Finetuning. 

1/6  twitter.com/rasbt/status/1…
------
Both soft prompt-tuning (such as prefix-finetuning) and adapters keep the pretrained LLM frozen. They also both add a small, tunable number of parameters. Soft prompting methods usually focus on tuning the inputs. 

5/6
------
Adapters, in contrast, introduce small, trainable modules within the model's architecture. These modules, called adapter layers, are inserted between the pretrained layers of the model. I will dive into this deeper next.

6/6
------
Finally, it’s done!! 

Going the patent route instead of traditional peer review has been the best decision of my career so far. 

Let me know if you have any questions about the process.
------
Patents granted on April 1st are the best patents 
------
At tried to be nice this year so that 
@giffmana
 doesn’t waste a whole day of work again 
------
Btw I will release the code under Apache 2.0 license. I am not planning to enforce it. I filed it to prevent big tech or patent trolls enforcing it.
------
(Fine)tuning is what allows us to adjust pretrained large language models for our practical use cases. 

Before diving into LoRA, let's talk about the different ways to modify the prompts and clarify the difference between prompt tuning and prefix tuning.
1/5
------
Yesterday, I covered the 3 classic ways to finetune LLMs. Let's now delve into parameter-efficient finetuning techniques. 

Parameter Efficient Finetuning Part I: let's start  with Prefix Finetuning. 

1/6  twitter.com/rasbt/status/1…
------
Prefix tuning is essentially a flavor or "soft" prompt tuning. However, here we add the trainable tensors (soft prompts) to each transformer block. 
(Note that transformer blocks usually have 2 fully connected layers instead of an embedding layer)

4/5
------
Some references if you want to learn more:

[Soft prompt tuning]
The Power of Scale for Parameter-Efficient Prompt Tuning (https://arxiv.org/abs/2104.08691)

[Prefix tuning]
Prefix-Tuning: Optimizing Continuous Prompts for Generation (https://arxiv.org/abs/2101.00190)

5/5
------
Progress update!

Lit-LLaMA now implements the LLaMA-Adapter method for efficient fine-tuning 

The core idea can be implemented in about 11 lines of code (see screenshot)

Link to repo https://github.com/Lightning-AI/lit-llama…
Link to Adapter paperhttp://arxiv.org/abs/2303.16199
------
Working with ordered data (like customer feedback) and want to leverage large language models?

Added a hands-on example using our CORN loss -- all it takes is swapping out the loss function!

Code: https://github.com/Raschka-research-group/coral-pytorch/blob/main/docs/tutorials/pytorch_lightning/distilbert-corn-tripadvisor.ipynb…

Paper: https://arxiv.org/abs/2111.08851
------
And accompanying the LLM code example, here's a video tutorial on "Conditional Ordinal Regression for Neural Networks (CORN)":
------
A new Deep Learning Fundamentals unit just launched!

 http://lightning.ai/pages/dlf

In this unit, I am excited to show you some of my favorite deep learning tips & tricks

- learning rate schedulers & cosine annealing
- CLI configs for hparam sweeps
- debugging w. batch overfitting
------
 Unit 6 of @rasbt's Deep Learning Fundamentals course just dropped! 

In this unit, he covers:  
model checkpointing 
learning rate finders and schedulers 
different optimization algorithms  
activation functions.  

Click here to start the unit now … Show more
------
A new Deep Learning Fundamentals unit just launched!

 http://lightning.ai/pages/dlf

In this unit, I am excited to show you some of my favorite deep learning tips & tricks

- learning rate schedulers & cosine annealing
- CLI configs for hparam sweeps
- debugging w. batch overfitting
------
 Unit 6 of @rasbt's Deep Learning Fundamentals course just dropped! 

In this unit, he covers:  
model checkpointing 
learning rate finders and schedulers 
different optimization algorithms  
activation functions.  

Click here to start the unit now … Show more
------
Btw. I am also covering step and reduce-on-plateau schedulers, techniques to reduce overfitting, and model checkpointing!

Lots of practical code examples in this one!
------
Large Language Models 3.0

In the new issue of Ahead of AI, I am discussing what's next for LLMs and developments centered around parameter efficiency and multimodality. 

It's also been a particularly strong month for open-source AI!


------
Should we train our own large language model (LLM) on domain-specific data from scratch? 
Researchers at Bloomberg did just that and shared a detailed technical report describing the dataset, model configuration, and training procedure. 

1/9
------
Assuming the ~53 days training time that was mentioned and doing some napkin-math assuming a discounted $1.1 per hour rate for a A100 GPU, the cost would be
1274 (hours) * 1.1 (dollars per hour) 64 * 8 (gpus) = $90k*

(not too bad maybe; I think in the LLaMA paper they reported… Show more
------
Omg how did I get $90k. I feel like my brain did a ChatGPT here and I guess I should have used a calculator. It's ~$700k of course!
------
We don't know. However, if you want to use the combined pretraining approach, BloombergGPT delivers an excellently described blueprint.

9/9
------
While I was mentioning that I was curious to see comparisons (and ideally ablation studies), I want to emphasize again that this is an excellent technical report. 

For researchers and practitioners interested in LLMs, it's a really nice recipe that's worth bookmarking. 

Here's… Show more
------
CC 
@bhutanisanyam1
 since you were curious about my thoughts about this paper ^^
------
Open Source Sunday!
Just released a new version of MLxtend: https://rasbt.github.io/mlxtend/

Featuring
- a snappier ExhaustiveFeatureSelector
- the H-Mine frequent pattern mining algo
- multiprocessing for plot_decision_regions

Thx to contributors, Fatih Sen, Nima Sarajpoor & others
------
Yesterday, I started discussing parameter-efficient finetuning methods for LLMs. Before I delve deeper into the topic and cover more methods in the next couple of days, I wanted to share a birds-eye view from the excellent "Scale Down to Scale Up" survey.

1/6
------
Yesterday, I covered the 3 classic ways to finetune LLMs. Let's now delve into parameter-efficient finetuning techniques. 

Parameter Efficient Finetuning Part I: let's start  with Prefix Finetuning. 

1/6  twitter.com/rasbt/status/1…
------
Both soft prompt-tuning (such as prefix-finetuning) and adapters keep the pretrained LLM frozen. They also both add a small, tunable number of parameters. Soft prompting methods usually focus on tuning the inputs. 

5/6
------
Adapters, in contrast, introduce small, trainable modules within the model's architecture. These modules, called adapter layers, are inserted between the pretrained layers of the model. I will dive into this deeper next.

6/6
------
Finally, it’s done!! 

Going the patent route instead of traditional peer review has been the best decision of my career so far. 

Let me know if you have any questions about the process.
------
Patents granted on April 1st are the best patents 
------
At tried to be nice this year so that 
@giffmana
 doesn’t waste a whole day of work again 
------
Btw I will release the code under Apache 2.0 license. I am not planning to enforce it. I filed it to prevent big tech or patent trolls enforcing it.
------
Yesterday, I covered the 3 classic ways to finetune LLMs. Let's now delve into parameter-efficient finetuning techniques. 

Parameter Efficient Finetuning Part I: let's start  with Prefix Finetuning. 

1/6
------
How can we adapt and finetune large language models from an efficiency standpoint? 

Yesterday, I discussed the recent LLaMA-Adapters. Many of you were curious about how this approach compares to the alternatives (eg low-rank adaption finetuning, prefix finetuning & others).

1/5
------
In this prefix finetuning approach, the context size may exceed the original model's maximum sequence length if we are not careful. To ensure that the context size does not exceed the original model's maximum sequence length, we can apply truncate the input sequences. 

5/6
------
Based on "Prefix-Tuning: Optimizing Continuous Prompts for Generation" (https://arxiv.org/abs/2101.00190), this method is pretty competitive with "Fine-Tune" (here, "Fine-Tune" means updating all model parameters)

6/6
------
Who is porting Twitter’s models to PyTorch this weekend? 
https://github.com/twitter/the-algorithm/tree/main/trust_and_safety_models…
------
And even the magic random seed has a cameo
------
Matplotlib ftw!
------
Learning from Machine Learning featuring 
@rasbt
 Sebastian Raschka: Learning ML, Responsible AI, AGI  |  https://youtu.be/gOEey-pbM90 is now available on youtube and all podcast platforms!! #ml #ai #ResponsibleAI
------
How can we adapt and finetune large language models from an efficiency standpoint? 

Yesterday, I discussed the recent LLaMA-Adapters. Many of you were curious about how this approach compares to the alternatives (eg low-rank adaption finetuning, prefix finetuning & others).

1/5
------
Working with ordered data (like customer feedback) and want to leverage large language models?

Added a hands-on example using our CORN loss -- all it takes is swapping out the loss function!

Code: https://github.com/Raschka-research-group/coral-pytorch/blob/main/docs/tutorials/pytorch_lightning/distilbert-corn-tripadvisor.ipynb…

Paper: https://arxiv.org/abs/2111.08851
------
And accompanying the LLM code example, here's a video tutorial on "Conditional Ordinal Regression for Neural Networks (CORN)":
------
A new Deep Learning Fundamentals unit just launched!

 http://lightning.ai/pages/dlf

In this unit, I am excited to show you some of my favorite deep learning tips & tricks

- learning rate schedulers & cosine annealing
- CLI configs for hparam sweeps
- debugging w. batch overfitting
------
 Unit 6 of @rasbt's Deep Learning Fundamentals course just dropped! 

In this unit, he covers:  
model checkpointing 
learning rate finders and schedulers 
different optimization algorithms  
activation functions.  

Click here to start the unit now … Show more
------
A new Deep Learning Fundamentals unit just launched!

 http://lightning.ai/pages/dlf

In this unit, I am excited to show you some of my favorite deep learning tips & tricks

- learning rate schedulers & cosine annealing
- CLI configs for hparam sweeps
- debugging w. batch overfitting
------
 Unit 6 of @rasbt's Deep Learning Fundamentals course just dropped! 

In this unit, he covers:  
model checkpointing 
learning rate finders and schedulers 
different optimization algorithms  
activation functions.  

Click here to start the unit now … Show more
------
Btw. I am also covering step and reduce-on-plateau schedulers, techniques to reduce overfitting, and model checkpointing!

Lots of practical code examples in this one!
------
Large Language Models 3.0

In the new issue of Ahead of AI, I am discussing what's next for LLMs and developments centered around parameter efficiency and multimodality. 

It's also been a particularly strong month for open-source AI!


------
Should we train our own large language model (LLM) on domain-specific data from scratch? 
Researchers at Bloomberg did just that and shared a detailed technical report describing the dataset, model configuration, and training procedure. 

1/9
------
Assuming the ~53 days training time that was mentioned and doing some napkin-math assuming a discounted $1.1 per hour rate for a A100 GPU, the cost would be
1274 (hours) * 1.1 (dollars per hour) 64 * 8 (gpus) = $90k*

(not too bad maybe; I think in the LLaMA paper they reported… Show more
------
Omg how did I get $90k. I feel like my brain did a ChatGPT here and I guess I should have used a calculator. It's ~$700k of course!
------
We don't know. However, if you want to use the combined pretraining approach, BloombergGPT delivers an excellently described blueprint.

9/9
------
While I was mentioning that I was curious to see comparisons (and ideally ablation studies), I want to emphasize again that this is an excellent technical report. 

For researchers and practitioners interested in LLMs, it's a really nice recipe that's worth bookmarking. 

Here's… Show more
------
CC 
@bhutanisanyam1
 since you were curious about my thoughts about this paper ^^
------
Open Source Sunday!
Just released a new version of MLxtend: https://rasbt.github.io/mlxtend/

Featuring
- a snappier ExhaustiveFeatureSelector
- the H-Mine frequent pattern mining algo
- multiprocessing for plot_decision_regions

Thx to contributors, Fatih Sen, Nima Sarajpoor & others
------
Yesterday, I started discussing parameter-efficient finetuning methods for LLMs. Before I delve deeper into the topic and cover more methods in the next couple of days, I wanted to share a birds-eye view from the excellent "Scale Down to Scale Up" survey.

1/6
------
Yesterday, I covered the 3 classic ways to finetune LLMs. Let's now delve into parameter-efficient finetuning techniques. 

Parameter Efficient Finetuning Part I: let's start  with Prefix Finetuning. 

1/6  twitter.com/rasbt/status/1…
------
Both soft prompt-tuning (such as prefix-finetuning) and adapters keep the pretrained LLM frozen. They also both add a small, tunable number of parameters. Soft prompting methods usually focus on tuning the inputs. 

5/6
------
Adapters, in contrast, introduce small, trainable modules within the model's architecture. These modules, called adapter layers, are inserted between the pretrained layers of the model. I will dive into this deeper next.

6/6
------
Finally, it’s done!! 

Going the patent route instead of traditional peer review has been the best decision of my career so far. 

Let me know if you have any questions about the process.
------
Patents granted on April 1st are the best patents 
------
At tried to be nice this year so that 
@giffmana
 doesn’t waste a whole day of work again 
------
Btw I will release the code under Apache 2.0 license. I am not planning to enforce it. I filed it to prevent big tech or patent trolls enforcing it.
------
Yesterday, I covered the 3 classic ways to finetune LLMs. Let's now delve into parameter-efficient finetuning techniques. 

Parameter Efficient Finetuning Part I: let's start  with Prefix Finetuning. 

1/6
------
How can we adapt and finetune large language models from an efficiency standpoint? 

Yesterday, I discussed the recent LLaMA-Adapters. Many of you were curious about how this approach compares to the alternatives (eg low-rank adaption finetuning, prefix finetuning & others).

1/5
------
In this prefix finetuning approach, the context size may exceed the original model's maximum sequence length if we are not careful. To ensure that the context size does not exceed the original model's maximum sequence length, we can apply truncate the input sequences. 

5/6
------
Based on "Prefix-Tuning: Optimizing Continuous Prompts for Generation" (https://arxiv.org/abs/2101.00190), this method is pretty competitive with "Fine-Tune" (here, "Fine-Tune" means updating all model parameters)

6/6
------
Who is porting Twitter’s models to PyTorch this weekend? 
https://github.com/twitter/the-algorithm/tree/main/trust_and_safety_models…
------
And even the magic random seed has a cameo
------
Matplotlib ftw!
------
Learning from Machine Learning featuring 
@rasbt
 Sebastian Raschka: Learning ML, Responsible AI, AGI  |  https://youtu.be/gOEey-pbM90 is now available on youtube and all podcast platforms!! #ml #ai #ResponsibleAI
------
How can we adapt and finetune large language models from an efficiency standpoint? 

Yesterday, I discussed the recent LLaMA-Adapters. Many of you were curious about how this approach compares to the alternatives (eg low-rank adaption finetuning, prefix finetuning & others).

1/5
------
2) Finetuning I: related to the feature-based approach  but adds >=1 output layers to the LLM. The backbone of the LLM remains frozen & we only update the model parameters in these new layers. We don't need to backpropagate through the whole network so it's quite efficient.

4/5
------
3) Finetuning II: oad the model & add +1 output layers, as in Finetuning I. However, instead of only backpropagating through the last layer, we backpropagate through ALL layers, making this the most expensive approach. But it also results in the best modeling performance.

5/5
------
Speaking of better public discourse, 
@MelMitchell1
 has an excellent newsletter/blog on the state & problems of large language models. 

(It's free from attention-seeking headlines, and thus probably not as popular as it should be.)

Highly recommended: https://aiguide.substack.com
------
I didn't sign "the letter".  

Current AI poses lots of risks, but describing these systems as "ever more powerful digital minds" that no one can control is likely to make the problem even worse.  

What's needed: more transparency and better public discourse.
------
Btw. I am also covering step and reduce-on-plateau schedulers, techniques to reduce overfitting, and model checkpointing!

Lots of practical code examples in this one!
------
Large Language Models 3.0

In the new issue of Ahead of AI, I am discussing what's next for LLMs and developments centered around parameter efficiency and multimodality. 

It's also been a particularly strong month for open-source AI!


------
Should we train our own large language model (LLM) on domain-specific data from scratch? 
Researchers at Bloomberg did just that and shared a detailed technical report describing the dataset, model configuration, and training procedure. 

1/9
------
Assuming the ~53 days training time that was mentioned and doing some napkin-math assuming a discounted $1.1 per hour rate for a A100 GPU, the cost would be
1274 (hours) * 1.1 (dollars per hour) 64 * 8 (gpus) = $90k*

(not too bad maybe; I think in the LLaMA paper they reported… Show more
------
Omg how did I get $90k. I feel like my brain did a ChatGPT here and I guess I should have used a calculator. It's ~$700k of course!
------
We don't know. However, if you want to use the combined pretraining approach, BloombergGPT delivers an excellently described blueprint.

9/9
------
While I was mentioning that I was curious to see comparisons (and ideally ablation studies), I want to emphasize again that this is an excellent technical report. 

For researchers and practitioners interested in LLMs, it's a really nice recipe that's worth bookmarking. 

Here's… Show more
------
CC 
@bhutanisanyam1
 since you were curious about my thoughts about this paper ^^
------
Open Source Sunday!
Just released a new version of MLxtend: https://rasbt.github.io/mlxtend/

Featuring
- a snappier ExhaustiveFeatureSelector
- the H-Mine frequent pattern mining algo
- multiprocessing for plot_decision_regions

Thx to contributors, Fatih Sen, Nima Sarajpoor & others
------
Yesterday, I started discussing parameter-efficient finetuning methods for LLMs. Before I delve deeper into the topic and cover more methods in the next couple of days, I wanted to share a birds-eye view from the excellent "Scale Down to Scale Up" survey.

1/6
------
Yesterday, I covered the 3 classic ways to finetune LLMs. Let's now delve into parameter-efficient finetuning techniques. 

Parameter Efficient Finetuning Part I: let's start  with Prefix Finetuning. 

1/6  twitter.com/rasbt/status/1…
------
Both soft prompt-tuning (such as prefix-finetuning) and adapters keep the pretrained LLM frozen. They also both add a small, tunable number of parameters. Soft prompting methods usually focus on tuning the inputs. 

5/6
------
Adapters, in contrast, introduce small, trainable modules within the model's architecture. These modules, called adapter layers, are inserted between the pretrained layers of the model. I will dive into this deeper next.

6/6
------
Finally, it’s done!! 

Going the patent route instead of traditional peer review has been the best decision of my career so far. 

Let me know if you have any questions about the process.
------
Patents granted on April 1st are the best patents 
------
At tried to be nice this year so that 
@giffmana
 doesn’t waste a whole day of work again 
------
Btw I will release the code under Apache 2.0 license. I am not planning to enforce it. I filed it to prevent big tech or patent trolls enforcing it.
------
Yesterday, I covered the 3 classic ways to finetune LLMs. Let's now delve into parameter-efficient finetuning techniques. 

Parameter Efficient Finetuning Part I: let's start  with Prefix Finetuning. 

1/6
------
How can we adapt and finetune large language models from an efficiency standpoint? 

Yesterday, I discussed the recent LLaMA-Adapters. Many of you were curious about how this approach compares to the alternatives (eg low-rank adaption finetuning, prefix finetuning & others).

1/5
------
In this prefix finetuning approach, the context size may exceed the original model's maximum sequence length if we are not careful. To ensure that the context size does not exceed the original model's maximum sequence length, we can apply truncate the input sequences. 

5/6
------
Based on "Prefix-Tuning: Optimizing Continuous Prompts for Generation" (https://arxiv.org/abs/2101.00190), this method is pretty competitive with "Fine-Tune" (here, "Fine-Tune" means updating all model parameters)

6/6
------
Who is porting Twitter’s models to PyTorch this weekend? 
https://github.com/twitter/the-algorithm/tree/main/trust_and_safety_models…
------
And even the magic random seed has a cameo
------
Matplotlib ftw!
------
Learning from Machine Learning featuring 
@rasbt
 Sebastian Raschka: Learning ML, Responsible AI, AGI  |  https://youtu.be/gOEey-pbM90 is now available on youtube and all podcast platforms!! #ml #ai #ResponsibleAI
------
How can we adapt and finetune large language models from an efficiency standpoint? 

Yesterday, I discussed the recent LLaMA-Adapters. Many of you were curious about how this approach compares to the alternatives (eg low-rank adaption finetuning, prefix finetuning & others).

1/5
------
2) Finetuning I: related to the feature-based approach  but adds >=1 output layers to the LLM. The backbone of the LLM remains frozen & we only update the model parameters in these new layers. We don't need to backpropagate through the whole network so it's quite efficient.

4/5
------
3) Finetuning II: oad the model & add +1 output layers, as in Finetuning I. However, instead of only backpropagating through the last layer, we backpropagate through ALL layers, making this the most expensive approach. But it also results in the best modeling performance.

5/5
------
Speaking of better public discourse, 
@MelMitchell1
 has an excellent newsletter/blog on the state & problems of large language models. 

(It's free from attention-seeking headlines, and thus probably not as popular as it should be.)

Highly recommended: https://aiguide.substack.com
------
I didn't sign "the letter".  

Current AI poses lots of risks, but describing these systems as "ever more powerful digital minds" that no one can control is likely to make the problem even worse.  

What's needed: more transparency and better public discourse.
------
I didn't sign "the letter".  

Current AI poses lots of risks, but describing these systems as "ever more powerful digital minds" that no one can control is likely to make the problem even worse.  

What's needed: more transparency and better public discourse.
------
LLaMA-Adapter: finetuning large language models (LLMs) like LLaMA and matching Alpaca's modeling performance with greater finetuning efficiency

Let's have a look at this new paper (https://arxiv.org/abs/2303.16199) that proposes an adapter method for LLaMA instruction finetuning

1/5
------
To make it work well in practice, they also propose a gating mechanism, initialized to all zeros, to prevent the adapters from perturbing the performance of the pretrained LLaMA model at the beginning of the training.

4/5
------
Finally, this adapter method is particularly interesting as it allows us to add other input modalities like image and video tokens.

PS: code here, https://github.com/ZrrSkywalker/LLaMA-Adapter…

5/5
------
CC 
@bhutanisanyam1
 since you were curious about my thoughts about this paper ^^
------
Open Source Sunday!
Just released a new version of MLxtend: https://rasbt.github.io/mlxtend/

Featuring
- a snappier ExhaustiveFeatureSelector
- the H-Mine frequent pattern mining algo
- multiprocessing for plot_decision_regions

Thx to contributors, Fatih Sen, Nima Sarajpoor & others
------
Yesterday, I started discussing parameter-efficient finetuning methods for LLMs. Before I delve deeper into the topic and cover more methods in the next couple of days, I wanted to share a birds-eye view from the excellent "Scale Down to Scale Up" survey.

1/6
------
Yesterday, I covered the 3 classic ways to finetune LLMs. Let's now delve into parameter-efficient finetuning techniques. 

Parameter Efficient Finetuning Part I: let's start  with Prefix Finetuning. 

1/6  twitter.com/rasbt/status/1…
------
Both soft prompt-tuning (such as prefix-finetuning) and adapters keep the pretrained LLM frozen. They also both add a small, tunable number of parameters. Soft prompting methods usually focus on tuning the inputs. 

5/6
------
Adapters, in contrast, introduce small, trainable modules within the model's architecture. These modules, called adapter layers, are inserted between the pretrained layers of the model. I will dive into this deeper next.

6/6
------
Finally, it’s done!! 

Going the patent route instead of traditional peer review has been the best decision of my career so far. 

Let me know if you have any questions about the process.
------
Patents granted on April 1st are the best patents 
------
At tried to be nice this year so that 
@giffmana
 doesn’t waste a whole day of work again 
------
Btw I will release the code under Apache 2.0 license. I am not planning to enforce it. I filed it to prevent big tech or patent trolls enforcing it.
------
Yesterday, I covered the 3 classic ways to finetune LLMs. Let's now delve into parameter-efficient finetuning techniques. 

Parameter Efficient Finetuning Part I: let's start  with Prefix Finetuning. 

1/6
------
How can we adapt and finetune large language models from an efficiency standpoint? 

Yesterday, I discussed the recent LLaMA-Adapters. Many of you were curious about how this approach compares to the alternatives (eg low-rank adaption finetuning, prefix finetuning & others).

1/5
------
In this prefix finetuning approach, the context size may exceed the original model's maximum sequence length if we are not careful. To ensure that the context size does not exceed the original model's maximum sequence length, we can apply truncate the input sequences. 

5/6
------
Based on "Prefix-Tuning: Optimizing Continuous Prompts for Generation" (https://arxiv.org/abs/2101.00190), this method is pretty competitive with "Fine-Tune" (here, "Fine-Tune" means updating all model parameters)

6/6
------
Who is porting Twitter’s models to PyTorch this weekend? 
https://github.com/twitter/the-algorithm/tree/main/trust_and_safety_models…
------
And even the magic random seed has a cameo
------
Matplotlib ftw!
------
Learning from Machine Learning featuring 
@rasbt
 Sebastian Raschka: Learning ML, Responsible AI, AGI  |  https://youtu.be/gOEey-pbM90 is now available on youtube and all podcast platforms!! #ml #ai #ResponsibleAI
------
How can we adapt and finetune large language models from an efficiency standpoint? 

Yesterday, I discussed the recent LLaMA-Adapters. Many of you were curious about how this approach compares to the alternatives (eg low-rank adaption finetuning, prefix finetuning & others).

1/5
------
2) Finetuning I: related to the feature-based approach  but adds >=1 output layers to the LLM. The backbone of the LLM remains frozen & we only update the model parameters in these new layers. We don't need to backpropagate through the whole network so it's quite efficient.

4/5
------
3) Finetuning II: oad the model & add +1 output layers, as in Finetuning I. However, instead of only backpropagating through the last layer, we backpropagate through ALL layers, making this the most expensive approach. But it also results in the best modeling performance.

5/5
------
Speaking of better public discourse, 
@MelMitchell1
 has an excellent newsletter/blog on the state & problems of large language models. 

(It's free from attention-seeking headlines, and thus probably not as popular as it should be.)

Highly recommended: https://aiguide.substack.com
------
I didn't sign "the letter".  

Current AI poses lots of risks, but describing these systems as "ever more powerful digital minds" that no one can control is likely to make the problem even worse.  

What's needed: more transparency and better public discourse.
------
I didn't sign "the letter".  

Current AI poses lots of risks, but describing these systems as "ever more powerful digital minds" that no one can control is likely to make the problem even worse.  

What's needed: more transparency and better public discourse.
------
LLaMA-Adapter: finetuning large language models (LLMs) like LLaMA and matching Alpaca's modeling performance with greater finetuning efficiency

Let's have a look at this new paper (https://arxiv.org/abs/2303.16199) that proposes an adapter method for LLaMA instruction finetuning

1/5
------
To make it work well in practice, they also propose a gating mechanism, initialized to all zeros, to prevent the adapters from perturbing the performance of the pretrained LLaMA model at the beginning of the training.

4/5
------
Finally, this adapter method is particularly interesting as it allows us to add other input modalities like image and video tokens.

PS: code here, https://github.com/ZrrSkywalker/LLaMA-Adapter…

5/5
------
Using the same 52K Instruction-following data, responses are comparable to Alpaca, but in contrast to Alpaca, which took 3 hours on 8 A100 to finetune, LLaMA adapters can be finetuned in 1h.
 (Caveat: I couldn't find any metrics for the Alpaca modeling performance comparison.)… Show more
------
"we call on all AI labs to immediately pause for at least 6 months the training of AI systems more powerful than GPT-4"

So, if I get this right, this is essentially asking just one particular company since no other company knows how powerful GPT-4 is and what GPT-4 is exactly
------
New hardware-friendly blog post: 
Finetuning Large Language Models On A Single GPU Using Gradient Accumulation

 https://lightning.ai/pages/blog/gradient-accumulation/……

It's  about how to maintain good modeling performance despite small batch sizes.
Next up, I am planning to go into other improvements ...
------
Some additional experiments: use torch.compile to shave off another 10 min in training time 
------
And next up, I am planning to go into additional optimizations, covering 

1) LoRA: Low-Rank Adaptation of Large Language Models
2) LLM.int8(): 8-bit Matrix Multiplication for Transformers a
------
Want to get into AI? My book is a 775-page journey from the fundamentals of machine learning to finetuning large language models. 

Today is the last day to catch "Machine Learning with PyTorch & Scikit-Learn book" during the Spring Sale  – 25% off! 
------
Finally, it’s done!! 

Going the patent route instead of traditional peer review has been the best decision of my career so far. 

Let me know if you have any questions about the process.
------
Patents granted on April 1st are the best patents 
------
At tried to be nice this year so that 
@giffmana
 doesn’t waste a whole day of work again 
------
Btw I will release the code under Apache 2.0 license. I am not planning to enforce it. I filed it to prevent big tech or patent trolls enforcing it.
------
Yesterday, I covered the 3 classic ways to finetune LLMs. Let's now delve into parameter-efficient finetuning techniques. 

Parameter Efficient Finetuning Part I: let's start  with Prefix Finetuning. 

1/6
------
How can we adapt and finetune large language models from an efficiency standpoint? 

Yesterday, I discussed the recent LLaMA-Adapters. Many of you were curious about how this approach compares to the alternatives (eg low-rank adaption finetuning, prefix finetuning & others).

1/5
------
In this prefix finetuning approach, the context size may exceed the original model's maximum sequence length if we are not careful. To ensure that the context size does not exceed the original model's maximum sequence length, we can apply truncate the input sequences. 

5/6
------
Based on "Prefix-Tuning: Optimizing Continuous Prompts for Generation" (https://arxiv.org/abs/2101.00190), this method is pretty competitive with "Fine-Tune" (here, "Fine-Tune" means updating all model parameters)

6/6
------
Who is porting Twitter’s models to PyTorch this weekend? 
https://github.com/twitter/the-algorithm/tree/main/trust_and_safety_models…
------
And even the magic random seed has a cameo
------
Matplotlib ftw!
------
Learning from Machine Learning featuring 
@rasbt
 Sebastian Raschka: Learning ML, Responsible AI, AGI  |  https://youtu.be/gOEey-pbM90 is now available on youtube and all podcast platforms!! #ml #ai #ResponsibleAI
------
How can we adapt and finetune large language models from an efficiency standpoint? 

Yesterday, I discussed the recent LLaMA-Adapters. Many of you were curious about how this approach compares to the alternatives (eg low-rank adaption finetuning, prefix finetuning & others).

1/5
------
2) Finetuning I: related to the feature-based approach  but adds >=1 output layers to the LLM. The backbone of the LLM remains frozen & we only update the model parameters in these new layers. We don't need to backpropagate through the whole network so it's quite efficient.

4/5
------
3) Finetuning II: oad the model & add +1 output layers, as in Finetuning I. However, instead of only backpropagating through the last layer, we backpropagate through ALL layers, making this the most expensive approach. But it also results in the best modeling performance.

5/5
------
Speaking of better public discourse, 
@MelMitchell1
 has an excellent newsletter/blog on the state & problems of large language models. 

(It's free from attention-seeking headlines, and thus probably not as popular as it should be.)

Highly recommended: https://aiguide.substack.com
------
I didn't sign "the letter".  

Current AI poses lots of risks, but describing these systems as "ever more powerful digital minds" that no one can control is likely to make the problem even worse.  

What's needed: more transparency and better public discourse.
------
I didn't sign "the letter".  

Current AI poses lots of risks, but describing these systems as "ever more powerful digital minds" that no one can control is likely to make the problem even worse.  

What's needed: more transparency and better public discourse.
------
LLaMA-Adapter: finetuning large language models (LLMs) like LLaMA and matching Alpaca's modeling performance with greater finetuning efficiency

Let's have a look at this new paper (https://arxiv.org/abs/2303.16199) that proposes an adapter method for LLaMA instruction finetuning

1/5
------
To make it work well in practice, they also propose a gating mechanism, initialized to all zeros, to prevent the adapters from perturbing the performance of the pretrained LLaMA model at the beginning of the training.

4/5
------
Finally, this adapter method is particularly interesting as it allows us to add other input modalities like image and video tokens.

PS: code here, https://github.com/ZrrSkywalker/LLaMA-Adapter…

5/5
------
Using the same 52K Instruction-following data, responses are comparable to Alpaca, but in contrast to Alpaca, which took 3 hours on 8 A100 to finetune, LLaMA adapters can be finetuned in 1h.
 (Caveat: I couldn't find any metrics for the Alpaca modeling performance comparison.)… Show more
------
"we call on all AI labs to immediately pause for at least 6 months the training of AI systems more powerful than GPT-4"

So, if I get this right, this is essentially asking just one particular company since no other company knows how powerful GPT-4 is and what GPT-4 is exactly
------
New hardware-friendly blog post: 
Finetuning Large Language Models On A Single GPU Using Gradient Accumulation

 https://lightning.ai/pages/blog/gradient-accumulation/……

It's  about how to maintain good modeling performance despite small batch sizes.
Next up, I am planning to go into other improvements ...
------
Some additional experiments: use torch.compile to shave off another 10 min in training time 
------
And next up, I am planning to go into additional optimizations, covering 

1) LoRA: Low-Rank Adaptation of Large Language Models
2) LLM.int8(): 8-bit Matrix Multiplication for Transformers a
------
Want to get into AI? My book is a 775-page journey from the fundamentals of machine learning to finetuning large language models. 

Today is the last day to catch "Machine Learning with PyTorch & Scikit-Learn book" during the Spring Sale  – 25% off! 
------
I am old enough to remember people cheering AI when it was defeating the human Go champion
------
We're excited to release Lit-LLaMA, a minimal, optimized rewrite of LLaMA for training and inference licensed under Apache 2.0 

Check out the repo https://github.com/Lightning-AI/lit-llama…
------
Large language models (LLMs) are getting better and better, and it is difficult to say whether we are approaching the limit of what pure text LLMs are capable of. 

Either way, thinking about the next step for LLMs is interesting. The next trend ... 
1/
------
PaLM is a really interesting decoder-style language model that I initially kind of ignored when it was published last year: https://arxiv.org/abs/2204.02311

Turns out PaLM has 7 interesting architecture improvements over GPT.

1/9
------
Pretty simple, they pretrained networks to encode them into embeddings. For example, for images, they experiment with a 4B & 22B parameter vision transformer (ViT) to produce embedding vectors. These embedding vectors are then linearly projected to match the embedding dimensions… Show more
------
The interesting question is whether we should freeze the pretrained LLM and only train the ViT embedding network. It appears that finetuning the LLM is better and co-training on the “full mixture” achieves more than double the performance. I.e., the big take-away of this paper is… Show more
------
Based on "Prefix-Tuning: Optimizing Continuous Prompts for Generation" (https://arxiv.org/abs/2101.00190), this method is pretty competitive with "Fine-Tune" (here, "Fine-Tune" means updating all model parameters)

6/6
------
Who is porting Twitter’s models to PyTorch this weekend? 
https://github.com/twitter/the-algorithm/tree/main/trust_and_safety_models…
------
And even the magic random seed has a cameo
------
Matplotlib ftw!
------
Learning from Machine Learning featuring 
@rasbt
 Sebastian Raschka: Learning ML, Responsible AI, AGI  |  https://youtu.be/gOEey-pbM90 is now available on youtube and all podcast platforms!! #ml #ai #ResponsibleAI
------
How can we adapt and finetune large language models from an efficiency standpoint? 

Yesterday, I discussed the recent LLaMA-Adapters. Many of you were curious about how this approach compares to the alternatives (eg low-rank adaption finetuning, prefix finetuning & others).

1/5
------
2) Finetuning I: related to the feature-based approach  but adds >=1 output layers to the LLM. The backbone of the LLM remains frozen & we only update the model parameters in these new layers. We don't need to backpropagate through the whole network so it's quite efficient.

4/5
------
3) Finetuning II: oad the model & add +1 output layers, as in Finetuning I. However, instead of only backpropagating through the last layer, we backpropagate through ALL layers, making this the most expensive approach. But it also results in the best modeling performance.

5/5
------
Speaking of better public discourse, 
@MelMitchell1
 has an excellent newsletter/blog on the state & problems of large language models. 

(It's free from attention-seeking headlines, and thus probably not as popular as it should be.)

Highly recommended: https://aiguide.substack.com
------
I didn't sign "the letter".  

Current AI poses lots of risks, but describing these systems as "ever more powerful digital minds" that no one can control is likely to make the problem even worse.  

What's needed: more transparency and better public discourse.
------
I didn't sign "the letter".  

Current AI poses lots of risks, but describing these systems as "ever more powerful digital minds" that no one can control is likely to make the problem even worse.  

What's needed: more transparency and better public discourse.
------
LLaMA-Adapter: finetuning large language models (LLMs) like LLaMA and matching Alpaca's modeling performance with greater finetuning efficiency

Let's have a look at this new paper (https://arxiv.org/abs/2303.16199) that proposes an adapter method for LLaMA instruction finetuning

1/5
------
To make it work well in practice, they also propose a gating mechanism, initialized to all zeros, to prevent the adapters from perturbing the performance of the pretrained LLaMA model at the beginning of the training.

4/5
------
Finally, this adapter method is particularly interesting as it allows us to add other input modalities like image and video tokens.

PS: code here, https://github.com/ZrrSkywalker/LLaMA-Adapter…

5/5
------
Using the same 52K Instruction-following data, responses are comparable to Alpaca, but in contrast to Alpaca, which took 3 hours on 8 A100 to finetune, LLaMA adapters can be finetuned in 1h.
 (Caveat: I couldn't find any metrics for the Alpaca modeling performance comparison.)… Show more
------
"we call on all AI labs to immediately pause for at least 6 months the training of AI systems more powerful than GPT-4"

So, if I get this right, this is essentially asking just one particular company since no other company knows how powerful GPT-4 is and what GPT-4 is exactly
------
New hardware-friendly blog post: 
Finetuning Large Language Models On A Single GPU Using Gradient Accumulation

 https://lightning.ai/pages/blog/gradient-accumulation/……

It's  about how to maintain good modeling performance despite small batch sizes.
Next up, I am planning to go into other improvements ...
------
Some additional experiments: use torch.compile to shave off another 10 min in training time 
------
And next up, I am planning to go into additional optimizations, covering 

1) LoRA: Low-Rank Adaptation of Large Language Models
2) LLM.int8(): 8-bit Matrix Multiplication for Transformers a
------
Want to get into AI? My book is a 775-page journey from the fundamentals of machine learning to finetuning large language models. 

Today is the last day to catch "Machine Learning with PyTorch & Scikit-Learn book" during the Spring Sale  – 25% off! 
------
I am old enough to remember people cheering AI when it was defeating the human Go champion
------
We're excited to release Lit-LLaMA, a minimal, optimized rewrite of LLaMA for training and inference licensed under Apache 2.0 

Check out the repo https://github.com/Lightning-AI/lit-llama…
------
Large language models (LLMs) are getting better and better, and it is difficult to say whether we are approaching the limit of what pure text LLMs are capable of. 

Either way, thinking about the next step for LLMs is interesting. The next trend ... 
1/
------
PaLM is a really interesting decoder-style language model that I initially kind of ignored when it was published last year: https://arxiv.org/abs/2204.02311

Turns out PaLM has 7 interesting architecture improvements over GPT.

1/9
------
Pretty simple, they pretrained networks to encode them into embeddings. For example, for images, they experiment with a 4B & 22B parameter vision transformer (ViT) to produce embedding vectors. These embedding vectors are then linearly projected to match the embedding dimensions… Show more
------
The interesting question is whether we should freeze the pretrained LLM and only train the ViT embedding network. It appears that finetuning the LLM is better and co-training on the “full mixture” achieves more than double the performance. I.e., the big take-away of this paper is… Show more
------
It should be noted that PaLM-E continues to be trained as a solely decoder-based LLM, which autoregressively generates text completions based on a given prefix or prompt.

How do they enable the input of state representations or images? 

5/
------
LLMs literally transform, not disrupt. 
Tech writers are missing out on a good pun.
------
Wow, just saw that Ahead of AI surpassed the 10k mark last week! Thanks for the support, everyone!
https://magazine.sebastianraschka.com

I am particularly glad that the paper reviews are so well received!

(And, of course, there will be a new issue  coming out next week!)
------
The week is off to a good start ... Can't push to GitHub. 

1. I thought I screwed up my git settings
2. then I was wondering if my account got hacked, 
3. now, after checking 1 & 2,  I'm wondering if it's "just" a GitHub issue. 

Is anyone else having trouble?
------
And it seems to be working again
------
How do we assess large language models (LLMs)? 

Evaluating Large Language Models IV: 
After discussing perplexity, BLEU, and ROUGE, the approaches are getting slightly better ... let's talk about BERTScore!

1/9
------
Understanding the shortcomings of large language models (LLMs) requires understanding the shortcomings of the underlying evaluation metrics.

Evaluating Large Language Models III: 
After covering perplexity and BLEU. Let's now discuss ROUGE.

1/8 twitter.com/rasbt/status/1…
------
How can we adapt and finetune large language models from an efficiency standpoint? 

Yesterday, I discussed the recent LLaMA-Adapters. Many of you were curious about how this approach compares to the alternatives (eg low-rank adaption finetuning, prefix finetuning & others).

1/5
------
2) Finetuning I: related to the feature-based approach  but adds >=1 output layers to the LLM. The backbone of the LLM remains frozen & we only update the model parameters in these new layers. We don't need to backpropagate through the whole network so it's quite efficient.

4/5
------
3) Finetuning II: oad the model & add +1 output layers, as in Finetuning I. However, instead of only backpropagating through the last layer, we backpropagate through ALL layers, making this the most expensive approach. But it also results in the best modeling performance.

5/5
------
Speaking of better public discourse, 
@MelMitchell1
 has an excellent newsletter/blog on the state & problems of large language models. 

(It's free from attention-seeking headlines, and thus probably not as popular as it should be.)

Highly recommended: https://aiguide.substack.com
------
I didn't sign "the letter".  

Current AI poses lots of risks, but describing these systems as "ever more powerful digital minds" that no one can control is likely to make the problem even worse.  

What's needed: more transparency and better public discourse.
------
I didn't sign "the letter".  

Current AI poses lots of risks, but describing these systems as "ever more powerful digital minds" that no one can control is likely to make the problem even worse.  

What's needed: more transparency and better public discourse.
------
LLaMA-Adapter: finetuning large language models (LLMs) like LLaMA and matching Alpaca's modeling performance with greater finetuning efficiency

Let's have a look at this new paper (https://arxiv.org/abs/2303.16199) that proposes an adapter method for LLaMA instruction finetuning

1/5
------
To make it work well in practice, they also propose a gating mechanism, initialized to all zeros, to prevent the adapters from perturbing the performance of the pretrained LLaMA model at the beginning of the training.

4/5
------
Finally, this adapter method is particularly interesting as it allows us to add other input modalities like image and video tokens.

PS: code here, https://github.com/ZrrSkywalker/LLaMA-Adapter…

5/5
------
Using the same 52K Instruction-following data, responses are comparable to Alpaca, but in contrast to Alpaca, which took 3 hours on 8 A100 to finetune, LLaMA adapters can be finetuned in 1h.
 (Caveat: I couldn't find any metrics for the Alpaca modeling performance comparison.)… Show more
------
"we call on all AI labs to immediately pause for at least 6 months the training of AI systems more powerful than GPT-4"

So, if I get this right, this is essentially asking just one particular company since no other company knows how powerful GPT-4 is and what GPT-4 is exactly
------
New hardware-friendly blog post: 
Finetuning Large Language Models On A Single GPU Using Gradient Accumulation

 https://lightning.ai/pages/blog/gradient-accumulation/……

It's  about how to maintain good modeling performance despite small batch sizes.
Next up, I am planning to go into other improvements ...
------
Some additional experiments: use torch.compile to shave off another 10 min in training time 
------
And next up, I am planning to go into additional optimizations, covering 

1) LoRA: Low-Rank Adaptation of Large Language Models
2) LLM.int8(): 8-bit Matrix Multiplication for Transformers a
------
Want to get into AI? My book is a 775-page journey from the fundamentals of machine learning to finetuning large language models. 

Today is the last day to catch "Machine Learning with PyTorch & Scikit-Learn book" during the Spring Sale  – 25% off! 
------
I am old enough to remember people cheering AI when it was defeating the human Go champion
------
We're excited to release Lit-LLaMA, a minimal, optimized rewrite of LLaMA for training and inference licensed under Apache 2.0 

Check out the repo https://github.com/Lightning-AI/lit-llama…
------
Large language models (LLMs) are getting better and better, and it is difficult to say whether we are approaching the limit of what pure text LLMs are capable of. 

Either way, thinking about the next step for LLMs is interesting. The next trend ... 
1/
------
PaLM is a really interesting decoder-style language model that I initially kind of ignored when it was published last year: https://arxiv.org/abs/2204.02311

Turns out PaLM has 7 interesting architecture improvements over GPT.

1/9
------
Pretty simple, they pretrained networks to encode them into embeddings. For example, for images, they experiment with a 4B & 22B parameter vision transformer (ViT) to produce embedding vectors. These embedding vectors are then linearly projected to match the embedding dimensions… Show more
------
The interesting question is whether we should freeze the pretrained LLM and only train the ViT embedding network. It appears that finetuning the LLM is better and co-training on the “full mixture” achieves more than double the performance. I.e., the big take-away of this paper is… Show more
------
It should be noted that PaLM-E continues to be trained as a solely decoder-based LLM, which autoregressively generates text completions based on a given prefix or prompt.

How do they enable the input of state representations or images? 

5/
------
LLMs literally transform, not disrupt. 
Tech writers are missing out on a good pun.
------
Wow, just saw that Ahead of AI surpassed the 10k mark last week! Thanks for the support, everyone!
https://magazine.sebastianraschka.com

I am particularly glad that the paper reviews are so well received!

(And, of course, there will be a new issue  coming out next week!)
------
The week is off to a good start ... Can't push to GitHub. 

1. I thought I screwed up my git settings
2. then I was wondering if my account got hacked, 
3. now, after checking 1 & 2,  I'm wondering if it's "just" a GitHub issue. 

Is anyone else having trouble?
------
And it seems to be working again
------
How do we assess large language models (LLMs)? 

Evaluating Large Language Models IV: 
After discussing perplexity, BLEU, and ROUGE, the approaches are getting slightly better ... let's talk about BERTScore!

1/9
------
Understanding the shortcomings of large language models (LLMs) requires understanding the shortcomings of the underlying evaluation metrics.

Evaluating Large Language Models III: 
After covering perplexity and BLEU. Let's now discuss ROUGE.

1/8 twitter.com/rasbt/status/1…
------
And, of course, I also prepared a code summary for you. (The full code notebook is available here: https://github.com/rasbt/MachineLearning-QandAI-book/blob/main/supplementary/q20-evaluation-llms/BERTScore.ipynb…)

8/9
------
It's important to note that while BERTScore provides a useful automatic evaluation metric, it is not perfect and should be used alongside other evaluation techniques, including human judgment.

More info in the original research paper: https://arxiv.org/abs/1904.09675

9/9
------
To clarify: there will be language models in 5 years, but they won't be auto-regressive.
Because auto-regressive models are uncontrollable and suffer from exponential divergence as more tokens are produced.
------
I didn't sign "the letter".  

Current AI poses lots of risks, but describing these systems as "ever more powerful digital minds" that no one can control is likely to make the problem even worse.  

What's needed: more transparency and better public discourse.
------
LLaMA-Adapter: finetuning large language models (LLMs) like LLaMA and matching Alpaca's modeling performance with greater finetuning efficiency

Let's have a look at this new paper (https://arxiv.org/abs/2303.16199) that proposes an adapter method for LLaMA instruction finetuning

1/5
------
To make it work well in practice, they also propose a gating mechanism, initialized to all zeros, to prevent the adapters from perturbing the performance of the pretrained LLaMA model at the beginning of the training.

4/5
------
Finally, this adapter method is particularly interesting as it allows us to add other input modalities like image and video tokens.

PS: code here, https://github.com/ZrrSkywalker/LLaMA-Adapter…

5/5
------
Using the same 52K Instruction-following data, responses are comparable to Alpaca, but in contrast to Alpaca, which took 3 hours on 8 A100 to finetune, LLaMA adapters can be finetuned in 1h.
 (Caveat: I couldn't find any metrics for the Alpaca modeling performance comparison.)… Show more
------
"we call on all AI labs to immediately pause for at least 6 months the training of AI systems more powerful than GPT-4"

So, if I get this right, this is essentially asking just one particular company since no other company knows how powerful GPT-4 is and what GPT-4 is exactly
------
New hardware-friendly blog post: 
Finetuning Large Language Models On A Single GPU Using Gradient Accumulation

 https://lightning.ai/pages/blog/gradient-accumulation/……

It's  about how to maintain good modeling performance despite small batch sizes.
Next up, I am planning to go into other improvements ...
------
Some additional experiments: use torch.compile to shave off another 10 min in training time 
------
And next up, I am planning to go into additional optimizations, covering 

1) LoRA: Low-Rank Adaptation of Large Language Models
2) LLM.int8(): 8-bit Matrix Multiplication for Transformers a
------
Want to get into AI? My book is a 775-page journey from the fundamentals of machine learning to finetuning large language models. 

Today is the last day to catch "Machine Learning with PyTorch & Scikit-Learn book" during the Spring Sale  – 25% off! 
------
I am old enough to remember people cheering AI when it was defeating the human Go champion
------
We're excited to release Lit-LLaMA, a minimal, optimized rewrite of LLaMA for training and inference licensed under Apache 2.0 

Check out the repo https://github.com/Lightning-AI/lit-llama…
------
Large language models (LLMs) are getting better and better, and it is difficult to say whether we are approaching the limit of what pure text LLMs are capable of. 

Either way, thinking about the next step for LLMs is interesting. The next trend ... 
1/
------
PaLM is a really interesting decoder-style language model that I initially kind of ignored when it was published last year: https://arxiv.org/abs/2204.02311

Turns out PaLM has 7 interesting architecture improvements over GPT.

1/9
------
Pretty simple, they pretrained networks to encode them into embeddings. For example, for images, they experiment with a 4B & 22B parameter vision transformer (ViT) to produce embedding vectors. These embedding vectors are then linearly projected to match the embedding dimensions… Show more
------
The interesting question is whether we should freeze the pretrained LLM and only train the ViT embedding network. It appears that finetuning the LLM is better and co-training on the “full mixture” achieves more than double the performance. I.e., the big take-away of this paper is… Show more
------
It should be noted that PaLM-E continues to be trained as a solely decoder-based LLM, which autoregressively generates text completions based on a given prefix or prompt.

How do they enable the input of state representations or images? 

5/
------
LLMs literally transform, not disrupt. 
Tech writers are missing out on a good pun.
------
Wow, just saw that Ahead of AI surpassed the 10k mark last week! Thanks for the support, everyone!
https://magazine.sebastianraschka.com

I am particularly glad that the paper reviews are so well received!

(And, of course, there will be a new issue  coming out next week!)
------
The week is off to a good start ... Can't push to GitHub. 

1. I thought I screwed up my git settings
2. then I was wondering if my account got hacked, 
3. now, after checking 1 & 2,  I'm wondering if it's "just" a GitHub issue. 

Is anyone else having trouble?
------
And it seems to be working again
------
How do we assess large language models (LLMs)? 

Evaluating Large Language Models IV: 
After discussing perplexity, BLEU, and ROUGE, the approaches are getting slightly better ... let's talk about BERTScore!

1/9
------
Understanding the shortcomings of large language models (LLMs) requires understanding the shortcomings of the underlying evaluation metrics.

Evaluating Large Language Models III: 
After covering perplexity and BLEU. Let's now discuss ROUGE.

1/8 twitter.com/rasbt/status/1…
------
And, of course, I also prepared a code summary for you. (The full code notebook is available here: https://github.com/rasbt/MachineLearning-QandAI-book/blob/main/supplementary/q20-evaluation-llms/BERTScore.ipynb…)

8/9
------
It's important to note that while BERTScore provides a useful automatic evaluation metric, it is not perfect and should be used alongside other evaluation techniques, including human judgment.

More info in the original research paper: https://arxiv.org/abs/1904.09675

9/9
------
To clarify: there will be language models in 5 years, but they won't be auto-regressive.
Because auto-regressive models are uncontrollable and suffer from exponential divergence as more tokens are produced.
------
Understanding the shortcomings of large language models (LLMs) requires understanding the shortcomings of the underlying evaluation metrics.

Evaluating Large Language Models III: 
After covering perplexity and BLEU. Let's now discuss ROUGE.

1/8
------
Evaluating Large Language Models II: Today, we are covering BLEU.

It's used in almost all large language models capable of translation, including popular tools such as OpenAI's Whisper and GPT-3.

 1/9 twitter.com/rasbt/status/1…
------
Of course, ROUGE has similar shortcomings as BLEU, and studies found that ROUGE does not significantly correlate with content generated by humans.

8/8
------
* I meant ratings of course :)
------
There are 5 languages for AI? I thought it’s just Python, CUDA, and maybe Triton 
------
What are the most important programming languages in AI? Our new blog post explains their strengths and weaknesses. 

Read it now: https://hubs.la/Q01J5dxw0
------
Spoiler: the answer is Python, C++, R, MATLAB, and Java.

Tbh I don’t think R, MATLAB, and Java are “important” languages for AI. Maybe last-gen AI and traditional ML. But I never heard of an LLM or ViT implemented or trained in either of these languages. Or did I miss something?
------
I learned and used all these languages in college. I even taught classes in R as a professor in a stats department. They are all fine languages and have their use cases. I just don’t think they are important languages for AI.
------
Reviewers:  If you ask the authors to do something and they have followed through successfully, or you made a claim authors successfully refuted, then you need to be prepared to change your recommendation to positive. 
#ICML2023 
1/3
------
This blog on attention and the intuition by 
@rasbt
 is so well written! Bonus: It also has the code to run with.
------
Evaluating Large Language Models II: Today, we are covering BLEU.

It's used in almost all large language models capable of translation, including popular tools such as OpenAI's Whisper and GPT-3.

 1/9
------
Yes, yes, large language models are everywhere! But how do we evaluate the quality of their generated text? 

There are intrinsic metrics, such as perplexity, and extrinsic ones such as BLEU & ROUGE. 

Let's start with the overview & perplexity.

1/6
------
After all, these shortcomings are mostly owed to the fact that BLEU measures string similarity, and similarity alone is not sufficient for capturing quality.

8/9
------
As 
@bnjmn_marie
 reported, there are more than 100 alternatives to BLEU, which are never or rarely used (https://arxiv.org/abs/2106.15195). Among those, the most popular alternatives to BLEU are probably BLEURT, METEOR, COMET, and BERTScore, but again, these are also not without flaws

9/9
------
Using the same 52K Instruction-following data, responses are comparable to Alpaca, but in contrast to Alpaca, which took 3 hours on 8 A100 to finetune, LLaMA adapters can be finetuned in 1h.
 (Caveat: I couldn't find any metrics for the Alpaca modeling performance comparison.)… Show more
------
"we call on all AI labs to immediately pause for at least 6 months the training of AI systems more powerful than GPT-4"

So, if I get this right, this is essentially asking just one particular company since no other company knows how powerful GPT-4 is and what GPT-4 is exactly
------
New hardware-friendly blog post: 
Finetuning Large Language Models On A Single GPU Using Gradient Accumulation

 https://lightning.ai/pages/blog/gradient-accumulation/……

It's  about how to maintain good modeling performance despite small batch sizes.
Next up, I am planning to go into other improvements ...
------
Some additional experiments: use torch.compile to shave off another 10 min in training time 
------
And next up, I am planning to go into additional optimizations, covering 

1) LoRA: Low-Rank Adaptation of Large Language Models
2) LLM.int8(): 8-bit Matrix Multiplication for Transformers a
------
Want to get into AI? My book is a 775-page journey from the fundamentals of machine learning to finetuning large language models. 

Today is the last day to catch "Machine Learning with PyTorch & Scikit-Learn book" during the Spring Sale  – 25% off! 
------
I am old enough to remember people cheering AI when it was defeating the human Go champion
------
We're excited to release Lit-LLaMA, a minimal, optimized rewrite of LLaMA for training and inference licensed under Apache 2.0 

Check out the repo https://github.com/Lightning-AI/lit-llama…
------
Large language models (LLMs) are getting better and better, and it is difficult to say whether we are approaching the limit of what pure text LLMs are capable of. 

Either way, thinking about the next step for LLMs is interesting. The next trend ... 
1/
------
PaLM is a really interesting decoder-style language model that I initially kind of ignored when it was published last year: https://arxiv.org/abs/2204.02311

Turns out PaLM has 7 interesting architecture improvements over GPT.

1/9
------
Pretty simple, they pretrained networks to encode them into embeddings. For example, for images, they experiment with a 4B & 22B parameter vision transformer (ViT) to produce embedding vectors. These embedding vectors are then linearly projected to match the embedding dimensions… Show more
------
The interesting question is whether we should freeze the pretrained LLM and only train the ViT embedding network. It appears that finetuning the LLM is better and co-training on the “full mixture” achieves more than double the performance. I.e., the big take-away of this paper is… Show more
------
It should be noted that PaLM-E continues to be trained as a solely decoder-based LLM, which autoregressively generates text completions based on a given prefix or prompt.

How do they enable the input of state representations or images? 

5/
------
LLMs literally transform, not disrupt. 
Tech writers are missing out on a good pun.
------
Wow, just saw that Ahead of AI surpassed the 10k mark last week! Thanks for the support, everyone!
https://magazine.sebastianraschka.com

I am particularly glad that the paper reviews are so well received!

(And, of course, there will be a new issue  coming out next week!)
------
The week is off to a good start ... Can't push to GitHub. 

1. I thought I screwed up my git settings
2. then I was wondering if my account got hacked, 
3. now, after checking 1 & 2,  I'm wondering if it's "just" a GitHub issue. 

Is anyone else having trouble?
------
And it seems to be working again
------
How do we assess large language models (LLMs)? 

Evaluating Large Language Models IV: 
After discussing perplexity, BLEU, and ROUGE, the approaches are getting slightly better ... let's talk about BERTScore!

1/9
------
Understanding the shortcomings of large language models (LLMs) requires understanding the shortcomings of the underlying evaluation metrics.

Evaluating Large Language Models III: 
After covering perplexity and BLEU. Let's now discuss ROUGE.

1/8 twitter.com/rasbt/status/1…
------
And, of course, I also prepared a code summary for you. (The full code notebook is available here: https://github.com/rasbt/MachineLearning-QandAI-book/blob/main/supplementary/q20-evaluation-llms/BERTScore.ipynb…)

8/9
------
It's important to note that while BERTScore provides a useful automatic evaluation metric, it is not perfect and should be used alongside other evaluation techniques, including human judgment.

More info in the original research paper: https://arxiv.org/abs/1904.09675

9/9
------
To clarify: there will be language models in 5 years, but they won't be auto-regressive.
Because auto-regressive models are uncontrollable and suffer from exponential divergence as more tokens are produced.
------
Understanding the shortcomings of large language models (LLMs) requires understanding the shortcomings of the underlying evaluation metrics.

Evaluating Large Language Models III: 
After covering perplexity and BLEU. Let's now discuss ROUGE.

1/8
------
Evaluating Large Language Models II: Today, we are covering BLEU.

It's used in almost all large language models capable of translation, including popular tools such as OpenAI's Whisper and GPT-3.

 1/9 twitter.com/rasbt/status/1…
------
Of course, ROUGE has similar shortcomings as BLEU, and studies found that ROUGE does not significantly correlate with content generated by humans.

8/8
------
* I meant ratings of course :)
------
There are 5 languages for AI? I thought it’s just Python, CUDA, and maybe Triton 
------
What are the most important programming languages in AI? Our new blog post explains their strengths and weaknesses. 

Read it now: https://hubs.la/Q01J5dxw0
------
Spoiler: the answer is Python, C++, R, MATLAB, and Java.

Tbh I don’t think R, MATLAB, and Java are “important” languages for AI. Maybe last-gen AI and traditional ML. But I never heard of an LLM or ViT implemented or trained in either of these languages. Or did I miss something?
------
I learned and used all these languages in college. I even taught classes in R as a professor in a stats department. They are all fine languages and have their use cases. I just don’t think they are important languages for AI.
------
Reviewers:  If you ask the authors to do something and they have followed through successfully, or you made a claim authors successfully refuted, then you need to be prepared to change your recommendation to positive. 
#ICML2023 
1/3
------
This blog on attention and the intuition by 
@rasbt
 is so well written! Bonus: It also has the code to run with.
------
Evaluating Large Language Models II: Today, we are covering BLEU.

It's used in almost all large language models capable of translation, including popular tools such as OpenAI's Whisper and GPT-3.

 1/9
------
Yes, yes, large language models are everywhere! But how do we evaluate the quality of their generated text? 

There are intrinsic metrics, such as perplexity, and extrinsic ones such as BLEU & ROUGE. 

Let's start with the overview & perplexity.

1/6
------
After all, these shortcomings are mostly owed to the fact that BLEU measures string similarity, and similarity alone is not sufficient for capturing quality.

8/9
------
As 
@bnjmn_marie
 reported, there are more than 100 alternatives to BLEU, which are never or rarely used (https://arxiv.org/abs/2106.15195). Among those, the most popular alternatives to BLEU are probably BLEURT, METEOR, COMET, and BERTScore, but again, these are also not without flaws

9/9
------
While BLEU is the most widely used metric for measuring the performance language translation systems, it's not a very good metric as summarized in 
@bnjmn_marie
's "12 Critical Flaws of BLEU" (https://medium.com/@bnjmn_marie/12-critical-flaws-of-bleu-1d790ccbe1b1…). 

7/9
------
They will, but not in the way you expect. These new-age productive AI gurus are all shilling AI courses and products to their gullible audiences, and people are unfortunately lapping it up. Covered some examples here. 

…https://codinginterviewsmadesimple.substack.com/p/how-influencers-are-stealing-from…
------
 Want to improve your PyTorch model's training performance without sacrificing accuracy? 

Learn how you can cut training time on a single GPU from 22.53 mins to 2.75 mins and maintain prediction accuracy

Check out this blog by 
@rasbt
: https://lightning.ai/pages/community/tutorial/how-to-speed-up-pytorch-model-training/…

#PyTorch… Show more
------
New AI research & news everywhere! A short post on my personal approach to keeping up with things.
------
I am old enough to remember people cheering AI when it was defeating the human Go champion
------
We're excited to release Lit-LLaMA, a minimal, optimized rewrite of LLaMA for training and inference licensed under Apache 2.0 

Check out the repo https://github.com/Lightning-AI/lit-llama…
------
Large language models (LLMs) are getting better and better, and it is difficult to say whether we are approaching the limit of what pure text LLMs are capable of. 

Either way, thinking about the next step for LLMs is interesting. The next trend ... 
1/
------
PaLM is a really interesting decoder-style language model that I initially kind of ignored when it was published last year: https://arxiv.org/abs/2204.02311

Turns out PaLM has 7 interesting architecture improvements over GPT.

1/9
------
Pretty simple, they pretrained networks to encode them into embeddings. For example, for images, they experiment with a 4B & 22B parameter vision transformer (ViT) to produce embedding vectors. These embedding vectors are then linearly projected to match the embedding dimensions… Show more
------
The interesting question is whether we should freeze the pretrained LLM and only train the ViT embedding network. It appears that finetuning the LLM is better and co-training on the “full mixture” achieves more than double the performance. I.e., the big take-away of this paper is… Show more
------
It should be noted that PaLM-E continues to be trained as a solely decoder-based LLM, which autoregressively generates text completions based on a given prefix or prompt.

How do they enable the input of state representations or images? 

5/
------
LLMs literally transform, not disrupt. 
Tech writers are missing out on a good pun.
------
Wow, just saw that Ahead of AI surpassed the 10k mark last week! Thanks for the support, everyone!
https://magazine.sebastianraschka.com

I am particularly glad that the paper reviews are so well received!

(And, of course, there will be a new issue  coming out next week!)
------
The week is off to a good start ... Can't push to GitHub. 

1. I thought I screwed up my git settings
2. then I was wondering if my account got hacked, 
3. now, after checking 1 & 2,  I'm wondering if it's "just" a GitHub issue. 

Is anyone else having trouble?
------
And it seems to be working again
------
How do we assess large language models (LLMs)? 

Evaluating Large Language Models IV: 
After discussing perplexity, BLEU, and ROUGE, the approaches are getting slightly better ... let's talk about BERTScore!

1/9
------
Understanding the shortcomings of large language models (LLMs) requires understanding the shortcomings of the underlying evaluation metrics.

Evaluating Large Language Models III: 
After covering perplexity and BLEU. Let's now discuss ROUGE.

1/8 twitter.com/rasbt/status/1…
------
And, of course, I also prepared a code summary for you. (The full code notebook is available here: https://github.com/rasbt/MachineLearning-QandAI-book/blob/main/supplementary/q20-evaluation-llms/BERTScore.ipynb…)

8/9
------
It's important to note that while BERTScore provides a useful automatic evaluation metric, it is not perfect and should be used alongside other evaluation techniques, including human judgment.

More info in the original research paper: https://arxiv.org/abs/1904.09675

9/9
------
To clarify: there will be language models in 5 years, but they won't be auto-regressive.
Because auto-regressive models are uncontrollable and suffer from exponential divergence as more tokens are produced.
------
Understanding the shortcomings of large language models (LLMs) requires understanding the shortcomings of the underlying evaluation metrics.

Evaluating Large Language Models III: 
After covering perplexity and BLEU. Let's now discuss ROUGE.

1/8
------
Evaluating Large Language Models II: Today, we are covering BLEU.

It's used in almost all large language models capable of translation, including popular tools such as OpenAI's Whisper and GPT-3.

 1/9 twitter.com/rasbt/status/1…
------
Of course, ROUGE has similar shortcomings as BLEU, and studies found that ROUGE does not significantly correlate with content generated by humans.

8/8
------
* I meant ratings of course :)
------
There are 5 languages for AI? I thought it’s just Python, CUDA, and maybe Triton 
------
What are the most important programming languages in AI? Our new blog post explains their strengths and weaknesses. 

Read it now: https://hubs.la/Q01J5dxw0
------
Spoiler: the answer is Python, C++, R, MATLAB, and Java.

Tbh I don’t think R, MATLAB, and Java are “important” languages for AI. Maybe last-gen AI and traditional ML. But I never heard of an LLM or ViT implemented or trained in either of these languages. Or did I miss something?
------
I learned and used all these languages in college. I even taught classes in R as a professor in a stats department. They are all fine languages and have their use cases. I just don’t think they are important languages for AI.
------
Reviewers:  If you ask the authors to do something and they have followed through successfully, or you made a claim authors successfully refuted, then you need to be prepared to change your recommendation to positive. 
#ICML2023 
1/3
------
This blog on attention and the intuition by 
@rasbt
 is so well written! Bonus: It also has the code to run with.
------
Evaluating Large Language Models II: Today, we are covering BLEU.

It's used in almost all large language models capable of translation, including popular tools such as OpenAI's Whisper and GPT-3.

 1/9
------
Yes, yes, large language models are everywhere! But how do we evaluate the quality of their generated text? 

There are intrinsic metrics, such as perplexity, and extrinsic ones such as BLEU & ROUGE. 

Let's start with the overview & perplexity.

1/6
------
After all, these shortcomings are mostly owed to the fact that BLEU measures string similarity, and similarity alone is not sufficient for capturing quality.

8/9
------
As 
@bnjmn_marie
 reported, there are more than 100 alternatives to BLEU, which are never or rarely used (https://arxiv.org/abs/2106.15195). Among those, the most popular alternatives to BLEU are probably BLEURT, METEOR, COMET, and BERTScore, but again, these are also not without flaws

9/9
------
While BLEU is the most widely used metric for measuring the performance language translation systems, it's not a very good metric as summarized in 
@bnjmn_marie
's "12 Critical Flaws of BLEU" (https://medium.com/@bnjmn_marie/12-critical-flaws-of-bleu-1d790ccbe1b1…). 

7/9
------
They will, but not in the way you expect. These new-age productive AI gurus are all shilling AI courses and products to their gullible audiences, and people are unfortunately lapping it up. Covered some examples here. 

…https://codinginterviewsmadesimple.substack.com/p/how-influencers-are-stealing-from…
------
 Want to improve your PyTorch model's training performance without sacrificing accuracy? 

Learn how you can cut training time on a single GPU from 22.53 mins to 2.75 mins and maintain prediction accuracy

Check out this blog by 
@rasbt
: https://lightning.ai/pages/community/tutorial/how-to-speed-up-pytorch-model-training/…

#PyTorch… Show more
------
New AI research & news everywhere! A short post on my personal approach to keeping up with things.
------
There should be a revision of the peer review systems where authors can include a quiz for the reviewers.

ACs & editors should then use that quiz to weed out comments from reviewers who didn’t read the paper carefully or misunderstood crucial aspects.
------
Yes, yes, large language models are everywhere! But how do we evaluate the quality of their generated text? 

There are intrinsic metrics, such as perplexity, and extrinsic ones such as BLEU & ROUGE. 

Let's start with the overview & perplexity.

1/6
------
Perplexity is defined as 2^(H(p, q) / N), where H(p, q) is the cross entropy between the true distribution of words p and the predicted distribution of words q. As cross entropy decreases, perplexity decreases as well (the lower the better).

4/6
------
Also to make this relationship between perplexity and cross entropy more clear (I am using base-2 log, but it would of course also work for the natural log):
------
In practice, if we have the predicted probability score for each word in a sentence s, we can also compute the perplexity directly as shown below:

5/6
------
It should be noted that PaLM-E continues to be trained as a solely decoder-based LLM, which autoregressively generates text completions based on a given prefix or prompt.

How do they enable the input of state representations or images? 

5/
------
LLMs literally transform, not disrupt. 
Tech writers are missing out on a good pun.
------
Wow, just saw that Ahead of AI surpassed the 10k mark last week! Thanks for the support, everyone!
https://magazine.sebastianraschka.com

I am particularly glad that the paper reviews are so well received!

(And, of course, there will be a new issue  coming out next week!)
------
The week is off to a good start ... Can't push to GitHub. 

1. I thought I screwed up my git settings
2. then I was wondering if my account got hacked, 
3. now, after checking 1 & 2,  I'm wondering if it's "just" a GitHub issue. 

Is anyone else having trouble?
------
And it seems to be working again
------
How do we assess large language models (LLMs)? 

Evaluating Large Language Models IV: 
After discussing perplexity, BLEU, and ROUGE, the approaches are getting slightly better ... let's talk about BERTScore!

1/9
------
Understanding the shortcomings of large language models (LLMs) requires understanding the shortcomings of the underlying evaluation metrics.

Evaluating Large Language Models III: 
After covering perplexity and BLEU. Let's now discuss ROUGE.

1/8 twitter.com/rasbt/status/1…
------
And, of course, I also prepared a code summary for you. (The full code notebook is available here: https://github.com/rasbt/MachineLearning-QandAI-book/blob/main/supplementary/q20-evaluation-llms/BERTScore.ipynb…)

8/9
------
It's important to note that while BERTScore provides a useful automatic evaluation metric, it is not perfect and should be used alongside other evaluation techniques, including human judgment.

More info in the original research paper: https://arxiv.org/abs/1904.09675

9/9
------
To clarify: there will be language models in 5 years, but they won't be auto-regressive.
Because auto-regressive models are uncontrollable and suffer from exponential divergence as more tokens are produced.
------
Understanding the shortcomings of large language models (LLMs) requires understanding the shortcomings of the underlying evaluation metrics.

Evaluating Large Language Models III: 
After covering perplexity and BLEU. Let's now discuss ROUGE.

1/8
------
Evaluating Large Language Models II: Today, we are covering BLEU.

It's used in almost all large language models capable of translation, including popular tools such as OpenAI's Whisper and GPT-3.

 1/9 twitter.com/rasbt/status/1…
------
Of course, ROUGE has similar shortcomings as BLEU, and studies found that ROUGE does not significantly correlate with content generated by humans.

8/8
------
* I meant ratings of course :)
------
There are 5 languages for AI? I thought it’s just Python, CUDA, and maybe Triton 
------
What are the most important programming languages in AI? Our new blog post explains their strengths and weaknesses. 

Read it now: https://hubs.la/Q01J5dxw0
------
Spoiler: the answer is Python, C++, R, MATLAB, and Java.

Tbh I don’t think R, MATLAB, and Java are “important” languages for AI. Maybe last-gen AI and traditional ML. But I never heard of an LLM or ViT implemented or trained in either of these languages. Or did I miss something?
------
I learned and used all these languages in college. I even taught classes in R as a professor in a stats department. They are all fine languages and have their use cases. I just don’t think they are important languages for AI.
------
Reviewers:  If you ask the authors to do something and they have followed through successfully, or you made a claim authors successfully refuted, then you need to be prepared to change your recommendation to positive. 
#ICML2023 
1/3
------
This blog on attention and the intuition by 
@rasbt
 is so well written! Bonus: It also has the code to run with.
------
Evaluating Large Language Models II: Today, we are covering BLEU.

It's used in almost all large language models capable of translation, including popular tools such as OpenAI's Whisper and GPT-3.

 1/9
------
Yes, yes, large language models are everywhere! But how do we evaluate the quality of their generated text? 

There are intrinsic metrics, such as perplexity, and extrinsic ones such as BLEU & ROUGE. 

Let's start with the overview & perplexity.

1/6
------
After all, these shortcomings are mostly owed to the fact that BLEU measures string similarity, and similarity alone is not sufficient for capturing quality.

8/9
------
As 
@bnjmn_marie
 reported, there are more than 100 alternatives to BLEU, which are never or rarely used (https://arxiv.org/abs/2106.15195). Among those, the most popular alternatives to BLEU are probably BLEURT, METEOR, COMET, and BERTScore, but again, these are also not without flaws

9/9
------
While BLEU is the most widely used metric for measuring the performance language translation systems, it's not a very good metric as summarized in 
@bnjmn_marie
's "12 Critical Flaws of BLEU" (https://medium.com/@bnjmn_marie/12-critical-flaws-of-bleu-1d790ccbe1b1…). 

7/9
------
They will, but not in the way you expect. These new-age productive AI gurus are all shilling AI courses and products to their gullible audiences, and people are unfortunately lapping it up. Covered some examples here. 

…https://codinginterviewsmadesimple.substack.com/p/how-influencers-are-stealing-from…
------
 Want to improve your PyTorch model's training performance without sacrificing accuracy? 

Learn how you can cut training time on a single GPU from 22.53 mins to 2.75 mins and maintain prediction accuracy

Check out this blog by 
@rasbt
: https://lightning.ai/pages/community/tutorial/how-to-speed-up-pytorch-model-training/…

#PyTorch… Show more
------
New AI research & news everywhere! A short post on my personal approach to keeping up with things.
------
There should be a revision of the peer review systems where authors can include a quiz for the reviewers.

ACs & editors should then use that quiz to weed out comments from reviewers who didn’t read the paper carefully or misunderstood crucial aspects.
------
Yes, yes, large language models are everywhere! But how do we evaluate the quality of their generated text? 

There are intrinsic metrics, such as perplexity, and extrinsic ones such as BLEU & ROUGE. 

Let's start with the overview & perplexity.

1/6
------
Perplexity is defined as 2^(H(p, q) / N), where H(p, q) is the cross entropy between the true distribution of words p and the predicted distribution of words q. As cross entropy decreases, perplexity decreases as well (the lower the better).

4/6
------
Also to make this relationship between perplexity and cross entropy more clear (I am using base-2 log, but it would of course also work for the natural log):
------
In practice, if we have the predicted probability score for each word in a sentence s, we can also compute the perplexity directly as shown below:

5/6
------
A bit more annotation to make it more clear:
------
A high perplexity means that a language model is worse at predicting the next word, implying a greater degree of uncertainty in its predictions.
We aim for a low perplexity in language models, as it indicates better performance in predicting the next word in a sequence.

6/6
------
And, of course, I also prepared a code summary for you. (The full code notebook is available here: https://github.com/rasbt/MachineLearning-QandAI-book/blob/main/supplementary/q20-evaluation-llms/BERTScore.ipynb…)

8/9
------
It's important to note that while BERTScore provides a useful automatic evaluation metric, it is not perfect and should be used alongside other evaluation techniques, including human judgment.

More info in the original research paper: https://arxiv.org/abs/1904.09675

9/9
------
To clarify: there will be language models in 5 years, but they won't be auto-regressive.
Because auto-regressive models are uncontrollable and suffer from exponential divergence as more tokens are produced.
------
Understanding the shortcomings of large language models (LLMs) requires understanding the shortcomings of the underlying evaluation metrics.

Evaluating Large Language Models III: 
After covering perplexity and BLEU. Let's now discuss ROUGE.

1/8
------
Evaluating Large Language Models II: Today, we are covering BLEU.

It's used in almost all large language models capable of translation, including popular tools such as OpenAI's Whisper and GPT-3.

 1/9 twitter.com/rasbt/status/1…
------
Of course, ROUGE has similar shortcomings as BLEU, and studies found that ROUGE does not significantly correlate with content generated by humans.

8/8
------
* I meant ratings of course :)
------
There are 5 languages for AI? I thought it’s just Python, CUDA, and maybe Triton 
------
What are the most important programming languages in AI? Our new blog post explains their strengths and weaknesses. 

Read it now: https://hubs.la/Q01J5dxw0
------
Spoiler: the answer is Python, C++, R, MATLAB, and Java.

Tbh I don’t think R, MATLAB, and Java are “important” languages for AI. Maybe last-gen AI and traditional ML. But I never heard of an LLM or ViT implemented or trained in either of these languages. Or did I miss something?
------
I learned and used all these languages in college. I even taught classes in R as a professor in a stats department. They are all fine languages and have their use cases. I just don’t think they are important languages for AI.
------
Reviewers:  If you ask the authors to do something and they have followed through successfully, or you made a claim authors successfully refuted, then you need to be prepared to change your recommendation to positive. 
#ICML2023 
1/3
------
This blog on attention and the intuition by 
@rasbt
 is so well written! Bonus: It also has the code to run with.
------
Evaluating Large Language Models II: Today, we are covering BLEU.

It's used in almost all large language models capable of translation, including popular tools such as OpenAI's Whisper and GPT-3.

 1/9
------
Yes, yes, large language models are everywhere! But how do we evaluate the quality of their generated text? 

There are intrinsic metrics, such as perplexity, and extrinsic ones such as BLEU & ROUGE. 

Let's start with the overview & perplexity.

1/6
------
After all, these shortcomings are mostly owed to the fact that BLEU measures string similarity, and similarity alone is not sufficient for capturing quality.

8/9
------
As 
@bnjmn_marie
 reported, there are more than 100 alternatives to BLEU, which are never or rarely used (https://arxiv.org/abs/2106.15195). Among those, the most popular alternatives to BLEU are probably BLEURT, METEOR, COMET, and BERTScore, but again, these are also not without flaws

9/9
------
While BLEU is the most widely used metric for measuring the performance language translation systems, it's not a very good metric as summarized in 
@bnjmn_marie
's "12 Critical Flaws of BLEU" (https://medium.com/@bnjmn_marie/12-critical-flaws-of-bleu-1d790ccbe1b1…). 

7/9
------
They will, but not in the way you expect. These new-age productive AI gurus are all shilling AI courses and products to their gullible audiences, and people are unfortunately lapping it up. Covered some examples here. 

…https://codinginterviewsmadesimple.substack.com/p/how-influencers-are-stealing-from…
------
 Want to improve your PyTorch model's training performance without sacrificing accuracy? 

Learn how you can cut training time on a single GPU from 22.53 mins to 2.75 mins and maintain prediction accuracy

Check out this blog by 
@rasbt
: https://lightning.ai/pages/community/tutorial/how-to-speed-up-pytorch-model-training/…

#PyTorch… Show more
------
New AI research & news everywhere! A short post on my personal approach to keeping up with things.
------
There should be a revision of the peer review systems where authors can include a quiz for the reviewers.

ACs & editors should then use that quiz to weed out comments from reviewers who didn’t read the paper carefully or misunderstood crucial aspects.
------
Yes, yes, large language models are everywhere! But how do we evaluate the quality of their generated text? 

There are intrinsic metrics, such as perplexity, and extrinsic ones such as BLEU & ROUGE. 

Let's start with the overview & perplexity.

1/6
------
Perplexity is defined as 2^(H(p, q) / N), where H(p, q) is the cross entropy between the true distribution of words p and the predicted distribution of words q. As cross entropy decreases, perplexity decreases as well (the lower the better).

4/6
------
Also to make this relationship between perplexity and cross entropy more clear (I am using base-2 log, but it would of course also work for the natural log):
------
In practice, if we have the predicted probability score for each word in a sentence s, we can also compute the perplexity directly as shown below:

5/6
------
A bit more annotation to make it more clear:
------
A high perplexity means that a language model is worse at predicting the next word, implying a greater degree of uncertainty in its predictions.
We aim for a low perplexity in language models, as it indicates better performance in predicting the next word in a sequence.

6/6
------
Do duplicates in the training set matter in classification?

Nice blog post giving away one of my exam q's!

TLDR
Nonparametric methods: either keep or de-duplicate them depending on the problem.

Parametric methods: no need to worry about duplicates (except for comp efficiency)
------
a simple thought experiment on when duplicates matter for classification

https://kyunghyuncho.me/when-do-duplicates-frequencies-matter-in-classification/…
------
Congrats on the new book 
@__mharrison__
 ! 
I think this is probably going to be the final word on tree-based methods for tabular data.
------
My new book, Effective XGBoost, is out!

Months in the making. Today only, use code TWEET20 for a discount.

https://store.metasnake.com/xgboost
------
"Meet in the Middle: A New Pre-training Paradigm" for large language models (LLM). 
In this paper, the authors propose to develop a bidirectional LLM using the full sequence information during pretraining and using context from both sides during inference. https://arxiv.org/abs/2303.07295… Show more
------
I’ve seen several tweets criticizing arXiv saying that citing arXiv articles can be considered harmful. 

I favor the opposite, pointing to arXiv versions for peer reviewed work as well.  

Why? Because authors can amend, fix, and update their work there, which  can be important.
------
Little update to do torch.compile justice in my previous article: https://sebastianraschka.com/blog/2023/pytorch-faster.html…

I was able to speed up the model from 8.4 -> 5.6 min with torch.compile. It required two little tricks,

1) placing the compilation before the timing starts;
2) priming the model with an… Show more
------
What an awesome  week for open source and the PyTorch ecosystem with three big launches!

- PyTorch 2.0
- Lightning Trainer 2.0 for PyTorch
- Fabric for PyTorch!

Just updated my "faster PyTorch" article to include the latest tools!

 https://sebastianraschka.com/blog/2023/pytorch-faster.html…
------
Classical Theory:  garbage in garbage out
Minor domain shift:  gold in garbage out
Diffusion models:  garbage in gold out
------
Some unsolicited writing advice when you are trying to fit things into an 8-page paper limit / 1-page rebuttal limit:

... without any use of bias units ...
... without using bias units ...

Does LayerNorm have an effect on ...?
Does LayerNorm affect ...?

The theorem does not… Show more
------
For a hot second, I was wondering how relevant weight decay still is. 
Instead of asking ChatGPT, I ran a simple experiment (*when a picture says more than a thousand words*)
------
PaLM is a really interesting decoder-style language model that I initially kind of ignored when it was published last year: https://arxiv.org/abs/2204.02311

Turns out PaLM has 7 interesting architecture improvements over GPT.

1/9
------
I learned and used all these languages in college. I even taught classes in R as a professor in a stats department. They are all fine languages and have their use cases. I just don’t think they are important languages for AI.
------
Reviewers:  If you ask the authors to do something and they have followed through successfully, or you made a claim authors successfully refuted, then you need to be prepared to change your recommendation to positive. 
#ICML2023 
1/3
------
This blog on attention and the intuition by 
@rasbt
 is so well written! Bonus: It also has the code to run with.
------
Evaluating Large Language Models II: Today, we are covering BLEU.

It's used in almost all large language models capable of translation, including popular tools such as OpenAI's Whisper and GPT-3.

 1/9
------
Yes, yes, large language models are everywhere! But how do we evaluate the quality of their generated text? 

There are intrinsic metrics, such as perplexity, and extrinsic ones such as BLEU & ROUGE. 

Let's start with the overview & perplexity.

1/6
------
After all, these shortcomings are mostly owed to the fact that BLEU measures string similarity, and similarity alone is not sufficient for capturing quality.

8/9
------
As 
@bnjmn_marie
 reported, there are more than 100 alternatives to BLEU, which are never or rarely used (https://arxiv.org/abs/2106.15195). Among those, the most popular alternatives to BLEU are probably BLEURT, METEOR, COMET, and BERTScore, but again, these are also not without flaws

9/9
------
While BLEU is the most widely used metric for measuring the performance language translation systems, it's not a very good metric as summarized in 
@bnjmn_marie
's "12 Critical Flaws of BLEU" (https://medium.com/@bnjmn_marie/12-critical-flaws-of-bleu-1d790ccbe1b1…). 

7/9
------
They will, but not in the way you expect. These new-age productive AI gurus are all shilling AI courses and products to their gullible audiences, and people are unfortunately lapping it up. Covered some examples here. 

…https://codinginterviewsmadesimple.substack.com/p/how-influencers-are-stealing-from…
------
 Want to improve your PyTorch model's training performance without sacrificing accuracy? 

Learn how you can cut training time on a single GPU from 22.53 mins to 2.75 mins and maintain prediction accuracy

Check out this blog by 
@rasbt
: https://lightning.ai/pages/community/tutorial/how-to-speed-up-pytorch-model-training/…

#PyTorch… Show more
------
New AI research & news everywhere! A short post on my personal approach to keeping up with things.
------
There should be a revision of the peer review systems where authors can include a quiz for the reviewers.

ACs & editors should then use that quiz to weed out comments from reviewers who didn’t read the paper carefully or misunderstood crucial aspects.
------
Yes, yes, large language models are everywhere! But how do we evaluate the quality of their generated text? 

There are intrinsic metrics, such as perplexity, and extrinsic ones such as BLEU & ROUGE. 

Let's start with the overview & perplexity.

1/6
------
Perplexity is defined as 2^(H(p, q) / N), where H(p, q) is the cross entropy between the true distribution of words p and the predicted distribution of words q. As cross entropy decreases, perplexity decreases as well (the lower the better).

4/6
------
Also to make this relationship between perplexity and cross entropy more clear (I am using base-2 log, but it would of course also work for the natural log):
------
In practice, if we have the predicted probability score for each word in a sentence s, we can also compute the perplexity directly as shown below:

5/6
------
A bit more annotation to make it more clear:
------
A high perplexity means that a language model is worse at predicting the next word, implying a greater degree of uncertainty in its predictions.
We aim for a low perplexity in language models, as it indicates better performance in predicting the next word in a sequence.

6/6
------
Do duplicates in the training set matter in classification?

Nice blog post giving away one of my exam q's!

TLDR
Nonparametric methods: either keep or de-duplicate them depending on the problem.

Parametric methods: no need to worry about duplicates (except for comp efficiency)
------
a simple thought experiment on when duplicates matter for classification

https://kyunghyuncho.me/when-do-duplicates-frequencies-matter-in-classification/…
------
Congrats on the new book 
@__mharrison__
 ! 
I think this is probably going to be the final word on tree-based methods for tabular data.
------
My new book, Effective XGBoost, is out!

Months in the making. Today only, use code TWEET20 for a discount.

https://store.metasnake.com/xgboost
------
"Meet in the Middle: A New Pre-training Paradigm" for large language models (LLM). 
In this paper, the authors propose to develop a bidirectional LLM using the full sequence information during pretraining and using context from both sides during inference. https://arxiv.org/abs/2303.07295… Show more
------
I’ve seen several tweets criticizing arXiv saying that citing arXiv articles can be considered harmful. 

I favor the opposite, pointing to arXiv versions for peer reviewed work as well.  

Why? Because authors can amend, fix, and update their work there, which  can be important.
------
Little update to do torch.compile justice in my previous article: https://sebastianraschka.com/blog/2023/pytorch-faster.html…

I was able to speed up the model from 8.4 -> 5.6 min with torch.compile. It required two little tricks,

1) placing the compilation before the timing starts;
2) priming the model with an… Show more
------
What an awesome  week for open source and the PyTorch ecosystem with three big launches!

- PyTorch 2.0
- Lightning Trainer 2.0 for PyTorch
- Fabric for PyTorch!

Just updated my "faster PyTorch" article to include the latest tools!

 https://sebastianraschka.com/blog/2023/pytorch-faster.html…
------
Classical Theory:  garbage in garbage out
Minor domain shift:  gold in garbage out
Diffusion models:  garbage in gold out
------
Some unsolicited writing advice when you are trying to fit things into an 8-page paper limit / 1-page rebuttal limit:

... without any use of bias units ...
... without using bias units ...

Does LayerNorm have an effect on ...?
Does LayerNorm affect ...?

The theorem does not… Show more
------
For a hot second, I was wondering how relevant weight decay still is. 
Instead of asking ChatGPT, I ran a simple experiment (*when a picture says more than a thousand words*)
------
PaLM is a really interesting decoder-style language model that I initially kind of ignored when it was published last year: https://arxiv.org/abs/2204.02311

Turns out PaLM has 7 interesting architecture improvements over GPT.

1/9
------
Google started rolling out PaLM API access to developers: https://blog.google/technology/ai/ai-developers-google-cloud-workspace/….

It will be interesting to see how it will compare to ChatGPT API.

If you want to learn more about PaLM, make sure to check out 
@cwolferesearch
's excellent post here: https://cameronrwolfe.substack.com/p/palm-efficiently-training-massive…

9/9
------
And maybe most interestingly, it outperforms GPT in terms of both training efficiency and modeling performance:
------
While BLEU is the most widely used metric for measuring the performance language translation systems, it's not a very good metric as summarized in 
@bnjmn_marie
's "12 Critical Flaws of BLEU" (https://medium.com/@bnjmn_marie/12-critical-flaws-of-bleu-1d790ccbe1b1…). 

7/9
------
They will, but not in the way you expect. These new-age productive AI gurus are all shilling AI courses and products to their gullible audiences, and people are unfortunately lapping it up. Covered some examples here. 

…https://codinginterviewsmadesimple.substack.com/p/how-influencers-are-stealing-from…
------
 Want to improve your PyTorch model's training performance without sacrificing accuracy? 

Learn how you can cut training time on a single GPU from 22.53 mins to 2.75 mins and maintain prediction accuracy

Check out this blog by 
@rasbt
: https://lightning.ai/pages/community/tutorial/how-to-speed-up-pytorch-model-training/…

#PyTorch… Show more
------
New AI research & news everywhere! A short post on my personal approach to keeping up with things.
------
There should be a revision of the peer review systems where authors can include a quiz for the reviewers.

ACs & editors should then use that quiz to weed out comments from reviewers who didn’t read the paper carefully or misunderstood crucial aspects.
------
Yes, yes, large language models are everywhere! But how do we evaluate the quality of their generated text? 

There are intrinsic metrics, such as perplexity, and extrinsic ones such as BLEU & ROUGE. 

Let's start with the overview & perplexity.

1/6
------
Perplexity is defined as 2^(H(p, q) / N), where H(p, q) is the cross entropy between the true distribution of words p and the predicted distribution of words q. As cross entropy decreases, perplexity decreases as well (the lower the better).

4/6
------
Also to make this relationship between perplexity and cross entropy more clear (I am using base-2 log, but it would of course also work for the natural log):
------
In practice, if we have the predicted probability score for each word in a sentence s, we can also compute the perplexity directly as shown below:

5/6
------
A bit more annotation to make it more clear:
------
A high perplexity means that a language model is worse at predicting the next word, implying a greater degree of uncertainty in its predictions.
We aim for a low perplexity in language models, as it indicates better performance in predicting the next word in a sequence.

6/6
------
Do duplicates in the training set matter in classification?

Nice blog post giving away one of my exam q's!

TLDR
Nonparametric methods: either keep or de-duplicate them depending on the problem.

Parametric methods: no need to worry about duplicates (except for comp efficiency)
------
a simple thought experiment on when duplicates matter for classification

https://kyunghyuncho.me/when-do-duplicates-frequencies-matter-in-classification/…
------
Congrats on the new book 
@__mharrison__
 ! 
I think this is probably going to be the final word on tree-based methods for tabular data.
------
My new book, Effective XGBoost, is out!

Months in the making. Today only, use code TWEET20 for a discount.

https://store.metasnake.com/xgboost
------
"Meet in the Middle: A New Pre-training Paradigm" for large language models (LLM). 
In this paper, the authors propose to develop a bidirectional LLM using the full sequence information during pretraining and using context from both sides during inference. https://arxiv.org/abs/2303.07295… Show more
------
I’ve seen several tweets criticizing arXiv saying that citing arXiv articles can be considered harmful. 

I favor the opposite, pointing to arXiv versions for peer reviewed work as well.  

Why? Because authors can amend, fix, and update their work there, which  can be important.
------
Little update to do torch.compile justice in my previous article: https://sebastianraschka.com/blog/2023/pytorch-faster.html…

I was able to speed up the model from 8.4 -> 5.6 min with torch.compile. It required two little tricks,

1) placing the compilation before the timing starts;
2) priming the model with an… Show more
------
What an awesome  week for open source and the PyTorch ecosystem with three big launches!

- PyTorch 2.0
- Lightning Trainer 2.0 for PyTorch
- Fabric for PyTorch!

Just updated my "faster PyTorch" article to include the latest tools!

 https://sebastianraschka.com/blog/2023/pytorch-faster.html…
------
Classical Theory:  garbage in garbage out
Minor domain shift:  gold in garbage out
Diffusion models:  garbage in gold out
------
Some unsolicited writing advice when you are trying to fit things into an 8-page paper limit / 1-page rebuttal limit:

... without any use of bias units ...
... without using bias units ...

Does LayerNorm have an effect on ...?
Does LayerNorm affect ...?

The theorem does not… Show more
------
For a hot second, I was wondering how relevant weight decay still is. 
Instead of asking ChatGPT, I ran a simple experiment (*when a picture says more than a thousand words*)
------
PaLM is a really interesting decoder-style language model that I initially kind of ignored when it was published last year: https://arxiv.org/abs/2204.02311

Turns out PaLM has 7 interesting architecture improvements over GPT.

1/9
------
Google started rolling out PaLM API access to developers: https://blog.google/technology/ai/ai-developers-google-cloud-workspace/….

It will be interesting to see how it will compare to ChatGPT API.

If you want to learn more about PaLM, make sure to check out 
@cwolferesearch
's excellent post here: https://cameronrwolfe.substack.com/p/palm-efficiently-training-massive…

9/9
------
And maybe most interestingly, it outperforms GPT in terms of both training efficiency and modeling performance:
------
7) Vocabulary: SentencePiece vocabulary with 256k tokens. SentencePiece is a language-independent subword tokenizer
(Ref: https://arxiv.org/abs/1808.06226)

8/9
------
The two scenarios when fully connected layers are equivalent to convolutional networks.

PyTorch-based code illustration here: https://github.com/rasbt/MachineLearning-QandAI-book/blob/main/supplementary/q12-fc-cnn-equivalence/q12-fc-cnn-equivalence.ipynb…
------
Such an eventful week, and I am just catching up with Alpaca, which deserves a big shoutout.

Alpaca is an instruction-finetuned 7B language transformer based on the 7B LLaMA GPT-3 alternative by Meta released a few weeks ago.
https://crfm.stanford.edu/2023/03/13/alpaca.html…

1/6
------
Here's a small wishlist in case the authors are working on an Alpaca research paper:

1) Why 52k instruction-output pairs (and not more or less)? What are the scaling laws for finetuning here?

5/6
------
2) How does the performance of supervised finetuning here compare with proximal policy optimization-based finetuning?

3) Is there a significant performance difference when swapping text-davinci-003 with ChatGPT's GPT 3.5 or GPT 4?

6/6
------
Exactly! This is a great example of how CVPR publicity restrictions very effectively prevent unfair public visibility on social media for research from prestigious institutes like Ivy League. 

oh wait…
------
I don’t mean to be disrespectful to either @fchollet or the authors of ViperGPT (its great work!), but i have to point out that our work VisProg did something similar 4 months ago but it just went unnoticed because of CVPR’s publicity restrictions. @CVPR @CVPRConf twitter.com/fchollet/statu…
------
Plot twist: they didn’t disclose any details because there was actually no innovation to report, just a bit more finetuning, which would have looked underwhelming given all the hype.

By not sharing any details they made GPT-4 seem like a bigger innovation/deal than it really is.
------
GPT-4 was interesting for a hot second, I'll admit. 
But today is a new day: time to move on and get back to discussing research and open source.
------
But note that vision transformers (ViTs) are not free from any inductive biases!

ViTs focus more on global relationships due to the patchification & self-attention mechanism, which often leads to the perception that they act as low-pass filters, emphasizing (or recognizing)… Show more
------
Similar to fully-connected networks, the ViT architecture (and transformer architecture in general) lacks the inductive bias for spatial invariance/equivariance that convolutional networks have. Consequently, ViTs require more data for pretraining to acquire useful "priors" from… Show more
------
There should be a revision of the peer review systems where authors can include a quiz for the reviewers.

ACs & editors should then use that quiz to weed out comments from reviewers who didn’t read the paper carefully or misunderstood crucial aspects.
------
Yes, yes, large language models are everywhere! But how do we evaluate the quality of their generated text? 

There are intrinsic metrics, such as perplexity, and extrinsic ones such as BLEU & ROUGE. 

Let's start with the overview & perplexity.

1/6
------
Perplexity is defined as 2^(H(p, q) / N), where H(p, q) is the cross entropy between the true distribution of words p and the predicted distribution of words q. As cross entropy decreases, perplexity decreases as well (the lower the better).

4/6
------
Also to make this relationship between perplexity and cross entropy more clear (I am using base-2 log, but it would of course also work for the natural log):
------
In practice, if we have the predicted probability score for each word in a sentence s, we can also compute the perplexity directly as shown below:

5/6
------
A bit more annotation to make it more clear:
------
A high perplexity means that a language model is worse at predicting the next word, implying a greater degree of uncertainty in its predictions.
We aim for a low perplexity in language models, as it indicates better performance in predicting the next word in a sequence.

6/6
------
Do duplicates in the training set matter in classification?

Nice blog post giving away one of my exam q's!

TLDR
Nonparametric methods: either keep or de-duplicate them depending on the problem.

Parametric methods: no need to worry about duplicates (except for comp efficiency)
------
a simple thought experiment on when duplicates matter for classification

https://kyunghyuncho.me/when-do-duplicates-frequencies-matter-in-classification/…
------
Congrats on the new book 
@__mharrison__
 ! 
I think this is probably going to be the final word on tree-based methods for tabular data.
------
My new book, Effective XGBoost, is out!

Months in the making. Today only, use code TWEET20 for a discount.

https://store.metasnake.com/xgboost
------
"Meet in the Middle: A New Pre-training Paradigm" for large language models (LLM). 
In this paper, the authors propose to develop a bidirectional LLM using the full sequence information during pretraining and using context from both sides during inference. https://arxiv.org/abs/2303.07295… Show more
------
I’ve seen several tweets criticizing arXiv saying that citing arXiv articles can be considered harmful. 

I favor the opposite, pointing to arXiv versions for peer reviewed work as well.  

Why? Because authors can amend, fix, and update their work there, which  can be important.
------
Little update to do torch.compile justice in my previous article: https://sebastianraschka.com/blog/2023/pytorch-faster.html…

I was able to speed up the model from 8.4 -> 5.6 min with torch.compile. It required two little tricks,

1) placing the compilation before the timing starts;
2) priming the model with an… Show more
------
What an awesome  week for open source and the PyTorch ecosystem with three big launches!

- PyTorch 2.0
- Lightning Trainer 2.0 for PyTorch
- Fabric for PyTorch!

Just updated my "faster PyTorch" article to include the latest tools!

 https://sebastianraschka.com/blog/2023/pytorch-faster.html…
------
Classical Theory:  garbage in garbage out
Minor domain shift:  gold in garbage out
Diffusion models:  garbage in gold out
------
Some unsolicited writing advice when you are trying to fit things into an 8-page paper limit / 1-page rebuttal limit:

... without any use of bias units ...
... without using bias units ...

Does LayerNorm have an effect on ...?
Does LayerNorm affect ...?

The theorem does not… Show more
------
For a hot second, I was wondering how relevant weight decay still is. 
Instead of asking ChatGPT, I ran a simple experiment (*when a picture says more than a thousand words*)
------
PaLM is a really interesting decoder-style language model that I initially kind of ignored when it was published last year: https://arxiv.org/abs/2204.02311

Turns out PaLM has 7 interesting architecture improvements over GPT.

1/9
------
Google started rolling out PaLM API access to developers: https://blog.google/technology/ai/ai-developers-google-cloud-workspace/….

It will be interesting to see how it will compare to ChatGPT API.

If you want to learn more about PaLM, make sure to check out 
@cwolferesearch
's excellent post here: https://cameronrwolfe.substack.com/p/palm-efficiently-training-massive…

9/9
------
And maybe most interestingly, it outperforms GPT in terms of both training efficiency and modeling performance:
------
7) Vocabulary: SentencePiece vocabulary with 256k tokens. SentencePiece is a language-independent subword tokenizer
(Ref: https://arxiv.org/abs/1808.06226)

8/9
------
The two scenarios when fully connected layers are equivalent to convolutional networks.

PyTorch-based code illustration here: https://github.com/rasbt/MachineLearning-QandAI-book/blob/main/supplementary/q12-fc-cnn-equivalence/q12-fc-cnn-equivalence.ipynb…
------
Such an eventful week, and I am just catching up with Alpaca, which deserves a big shoutout.

Alpaca is an instruction-finetuned 7B language transformer based on the 7B LLaMA GPT-3 alternative by Meta released a few weeks ago.
https://crfm.stanford.edu/2023/03/13/alpaca.html…

1/6
------
Here's a small wishlist in case the authors are working on an Alpaca research paper:

1) Why 52k instruction-output pairs (and not more or less)? What are the scaling laws for finetuning here?

5/6
------
2) How does the performance of supervised finetuning here compare with proximal policy optimization-based finetuning?

3) Is there a significant performance difference when swapping text-davinci-003 with ChatGPT's GPT 3.5 or GPT 4?

6/6
------
Exactly! This is a great example of how CVPR publicity restrictions very effectively prevent unfair public visibility on social media for research from prestigious institutes like Ivy League. 

oh wait…
------
I don’t mean to be disrespectful to either @fchollet or the authors of ViperGPT (its great work!), but i have to point out that our work VisProg did something similar 4 months ago but it just went unnoticed because of CVPR’s publicity restrictions. @CVPR @CVPRConf twitter.com/fchollet/statu…
------
Plot twist: they didn’t disclose any details because there was actually no innovation to report, just a bit more finetuning, which would have looked underwhelming given all the hype.

By not sharing any details they made GPT-4 seem like a bigger innovation/deal than it really is.
------
GPT-4 was interesting for a hot second, I'll admit. 
But today is a new day: time to move on and get back to discussing research and open source.
------
But note that vision transformers (ViTs) are not free from any inductive biases!

ViTs focus more on global relationships due to the patchification & self-attention mechanism, which often leads to the perception that they act as low-pass filters, emphasizing (or recognizing)… Show more
------
Similar to fully-connected networks, the ViT architecture (and transformer architecture in general) lacks the inductive bias for spatial invariance/equivariance that convolutional networks have. Consequently, ViTs require more data for pretraining to acquire useful "priors" from… Show more
------
Btw if you have an hour to spare on the weekend, I am covering how to use the Trainer to fit PyTorch models in the newly released Unit 5: 
https://lightning.ai/pages/courses/deep-learning-fundamentals/overview-organizing-your-code-with-pytorch-lightning/…
------
What an awesome  week for open source and the PyTorch ecosystem with three big launches!

- PyTorch 2.0
- Lightning Trainer 2.0 for PyTorch
- Fabric for PyTorch!

Just updated my "faster PyTorch" article to include the latest tools!

 https://sebastianraschka.com/blog/2023/pytorch-faster.html…
------
A bit more annotation to make it more clear:
------
A high perplexity means that a language model is worse at predicting the next word, implying a greater degree of uncertainty in its predictions.
We aim for a low perplexity in language models, as it indicates better performance in predicting the next word in a sequence.

6/6
------
Do duplicates in the training set matter in classification?

Nice blog post giving away one of my exam q's!

TLDR
Nonparametric methods: either keep or de-duplicate them depending on the problem.

Parametric methods: no need to worry about duplicates (except for comp efficiency)
------
a simple thought experiment on when duplicates matter for classification

https://kyunghyuncho.me/when-do-duplicates-frequencies-matter-in-classification/…
------
Congrats on the new book 
@__mharrison__
 ! 
I think this is probably going to be the final word on tree-based methods for tabular data.
------
My new book, Effective XGBoost, is out!

Months in the making. Today only, use code TWEET20 for a discount.

https://store.metasnake.com/xgboost
------
"Meet in the Middle: A New Pre-training Paradigm" for large language models (LLM). 
In this paper, the authors propose to develop a bidirectional LLM using the full sequence information during pretraining and using context from both sides during inference. https://arxiv.org/abs/2303.07295… Show more
------
I’ve seen several tweets criticizing arXiv saying that citing arXiv articles can be considered harmful. 

I favor the opposite, pointing to arXiv versions for peer reviewed work as well.  

Why? Because authors can amend, fix, and update their work there, which  can be important.
------
Little update to do torch.compile justice in my previous article: https://sebastianraschka.com/blog/2023/pytorch-faster.html…

I was able to speed up the model from 8.4 -> 5.6 min with torch.compile. It required two little tricks,

1) placing the compilation before the timing starts;
2) priming the model with an… Show more
------
What an awesome  week for open source and the PyTorch ecosystem with three big launches!

- PyTorch 2.0
- Lightning Trainer 2.0 for PyTorch
- Fabric for PyTorch!

Just updated my "faster PyTorch" article to include the latest tools!

 https://sebastianraschka.com/blog/2023/pytorch-faster.html…
------
Classical Theory:  garbage in garbage out
Minor domain shift:  gold in garbage out
Diffusion models:  garbage in gold out
------
Some unsolicited writing advice when you are trying to fit things into an 8-page paper limit / 1-page rebuttal limit:

... without any use of bias units ...
... without using bias units ...

Does LayerNorm have an effect on ...?
Does LayerNorm affect ...?

The theorem does not… Show more
------
For a hot second, I was wondering how relevant weight decay still is. 
Instead of asking ChatGPT, I ran a simple experiment (*when a picture says more than a thousand words*)
------
PaLM is a really interesting decoder-style language model that I initially kind of ignored when it was published last year: https://arxiv.org/abs/2204.02311

Turns out PaLM has 7 interesting architecture improvements over GPT.

1/9
------
Google started rolling out PaLM API access to developers: https://blog.google/technology/ai/ai-developers-google-cloud-workspace/….

It will be interesting to see how it will compare to ChatGPT API.

If you want to learn more about PaLM, make sure to check out 
@cwolferesearch
's excellent post here: https://cameronrwolfe.substack.com/p/palm-efficiently-training-massive…

9/9
------
And maybe most interestingly, it outperforms GPT in terms of both training efficiency and modeling performance:
------
7) Vocabulary: SentencePiece vocabulary with 256k tokens. SentencePiece is a language-independent subword tokenizer
(Ref: https://arxiv.org/abs/1808.06226)

8/9
------
The two scenarios when fully connected layers are equivalent to convolutional networks.

PyTorch-based code illustration here: https://github.com/rasbt/MachineLearning-QandAI-book/blob/main/supplementary/q12-fc-cnn-equivalence/q12-fc-cnn-equivalence.ipynb…
------
Such an eventful week, and I am just catching up with Alpaca, which deserves a big shoutout.

Alpaca is an instruction-finetuned 7B language transformer based on the 7B LLaMA GPT-3 alternative by Meta released a few weeks ago.
https://crfm.stanford.edu/2023/03/13/alpaca.html…

1/6
------
Here's a small wishlist in case the authors are working on an Alpaca research paper:

1) Why 52k instruction-output pairs (and not more or less)? What are the scaling laws for finetuning here?

5/6
------
2) How does the performance of supervised finetuning here compare with proximal policy optimization-based finetuning?

3) Is there a significant performance difference when swapping text-davinci-003 with ChatGPT's GPT 3.5 or GPT 4?

6/6
------
Exactly! This is a great example of how CVPR publicity restrictions very effectively prevent unfair public visibility on social media for research from prestigious institutes like Ivy League. 

oh wait…
------
I don’t mean to be disrespectful to either @fchollet or the authors of ViperGPT (its great work!), but i have to point out that our work VisProg did something similar 4 months ago but it just went unnoticed because of CVPR’s publicity restrictions. @CVPR @CVPRConf twitter.com/fchollet/statu…
------
Plot twist: they didn’t disclose any details because there was actually no innovation to report, just a bit more finetuning, which would have looked underwhelming given all the hype.

By not sharing any details they made GPT-4 seem like a bigger innovation/deal than it really is.
------
GPT-4 was interesting for a hot second, I'll admit. 
But today is a new day: time to move on and get back to discussing research and open source.
------
But note that vision transformers (ViTs) are not free from any inductive biases!

ViTs focus more on global relationships due to the patchification & self-attention mechanism, which often leads to the perception that they act as low-pass filters, emphasizing (or recognizing)… Show more
------
Similar to fully-connected networks, the ViT architecture (and transformer architecture in general) lacks the inductive bias for spatial invariance/equivariance that convolutional networks have. Consequently, ViTs require more data for pretraining to acquire useful "priors" from… Show more
------
Btw if you have an hour to spare on the weekend, I am covering how to use the Trainer to fit PyTorch models in the newly released Unit 5: 
https://lightning.ai/pages/courses/deep-learning-fundamentals/overview-organizing-your-code-with-pytorch-lightning/…
------
What an awesome  week for open source and the PyTorch ecosystem with three big launches!

- PyTorch 2.0
- Lightning Trainer 2.0 for PyTorch
- Fabric for PyTorch!

Just updated my "faster PyTorch" article to include the latest tools!

 https://sebastianraschka.com/blog/2023/pytorch-faster.html…
------
What an awesome  week for open source and the PyTorch ecosystem with three big launches!

- PyTorch 2.0
- Lightning Trainer 2.0 for PyTorch
- Fabric for PyTorch!

Just updated my "faster PyTorch" article to include the latest tools!

 https://sebastianraschka.com/blog/2023/pytorch-faster.html…
------
Easy to miss in the 
@pytorch
 2.0 release notes, they've added a small, but useful feature: torch.device, which previously just returned a device object, can now be used as a context manager.

0/8
------
Similar to fully-connected networks, the ViT architecture (and transformer architecture in general) lacks the inductive bias for spatial invariance/equivariance that convolutional networks have. Consequently, ViTs require more data for pretraining to acquire useful "priors" from… Show more
------
Does the road for (sample, not compute) efficient priors lead back to RNNs and the original Bahdanau attention mechanism (https://arxiv.org/abs/1409.0473)?  
------
Open source ftw !
What a week!
------
We’re excited to announce the release of PyTorch 2.0! 

This version includes:
 100% backward compatible
 Out of the box performance
 Significant speed improvements 

Learn more 
https://hubs.la/Q01H4L1t0
------
A comparison of the exact same model training for all 3 approaches

1) Pure PyTorch: https://github.com/rasbt/pytorch-fabric-demo/blob/main/src/1_pytorch-distilbert.py…
(25 min)

2) PyTorch + Fabric: https://github.com/rasbt/pytorch-fabric-demo/blob/main/src/2_pytorch-fabric-distilbert.py…
(1.7 min)

3) PyTorch + Trainer: https://github.com/rasbt/faster-pytorch-blog/blob/main/6_deepspeed.py… (2.7 min, the 1 extra min is for logging+checkpointing)
------
Little update to do torch.compile justice in my previous article: https://sebastianraschka.com/blog/2023/pytorch-faster.html…

I was able to speed up the model from 8.4 -> 5.6 min with torch.compile. It required two little tricks,

1) placing the compilation before the timing starts;
2) priming the model with an… Show more
------
What an awesome  week for open source and the PyTorch ecosystem with three big launches!

- PyTorch 2.0
- Lightning Trainer 2.0 for PyTorch
- Fabric for PyTorch!

Just updated my "faster PyTorch" article to include the latest tools!

 https://sebastianraschka.com/blog/2023/pytorch-faster.html…
------
Classical Theory:  garbage in garbage out
Minor domain shift:  gold in garbage out
Diffusion models:  garbage in gold out
------
Some unsolicited writing advice when you are trying to fit things into an 8-page paper limit / 1-page rebuttal limit:

... without any use of bias units ...
... without using bias units ...

Does LayerNorm have an effect on ...?
Does LayerNorm affect ...?

The theorem does not… Show more
------
For a hot second, I was wondering how relevant weight decay still is. 
Instead of asking ChatGPT, I ran a simple experiment (*when a picture says more than a thousand words*)
------
PaLM is a really interesting decoder-style language model that I initially kind of ignored when it was published last year: https://arxiv.org/abs/2204.02311

Turns out PaLM has 7 interesting architecture improvements over GPT.

1/9
------
Google started rolling out PaLM API access to developers: https://blog.google/technology/ai/ai-developers-google-cloud-workspace/….

It will be interesting to see how it will compare to ChatGPT API.

If you want to learn more about PaLM, make sure to check out 
@cwolferesearch
's excellent post here: https://cameronrwolfe.substack.com/p/palm-efficiently-training-massive…

9/9
------
And maybe most interestingly, it outperforms GPT in terms of both training efficiency and modeling performance:
------
7) Vocabulary: SentencePiece vocabulary with 256k tokens. SentencePiece is a language-independent subword tokenizer
(Ref: https://arxiv.org/abs/1808.06226)

8/9
------
The two scenarios when fully connected layers are equivalent to convolutional networks.

PyTorch-based code illustration here: https://github.com/rasbt/MachineLearning-QandAI-book/blob/main/supplementary/q12-fc-cnn-equivalence/q12-fc-cnn-equivalence.ipynb…
------
Such an eventful week, and I am just catching up with Alpaca, which deserves a big shoutout.

Alpaca is an instruction-finetuned 7B language transformer based on the 7B LLaMA GPT-3 alternative by Meta released a few weeks ago.
https://crfm.stanford.edu/2023/03/13/alpaca.html…

1/6
------
Here's a small wishlist in case the authors are working on an Alpaca research paper:

1) Why 52k instruction-output pairs (and not more or less)? What are the scaling laws for finetuning here?

5/6
------
2) How does the performance of supervised finetuning here compare with proximal policy optimization-based finetuning?

3) Is there a significant performance difference when swapping text-davinci-003 with ChatGPT's GPT 3.5 or GPT 4?

6/6
------
Exactly! This is a great example of how CVPR publicity restrictions very effectively prevent unfair public visibility on social media for research from prestigious institutes like Ivy League. 

oh wait…
------
I don’t mean to be disrespectful to either @fchollet or the authors of ViperGPT (its great work!), but i have to point out that our work VisProg did something similar 4 months ago but it just went unnoticed because of CVPR’s publicity restrictions. @CVPR @CVPRConf twitter.com/fchollet/statu…
------
Plot twist: they didn’t disclose any details because there was actually no innovation to report, just a bit more finetuning, which would have looked underwhelming given all the hype.

By not sharing any details they made GPT-4 seem like a bigger innovation/deal than it really is.
------
GPT-4 was interesting for a hot second, I'll admit. 
But today is a new day: time to move on and get back to discussing research and open source.
------
But note that vision transformers (ViTs) are not free from any inductive biases!

ViTs focus more on global relationships due to the patchification & self-attention mechanism, which often leads to the perception that they act as low-pass filters, emphasizing (or recognizing)… Show more
------
Similar to fully-connected networks, the ViT architecture (and transformer architecture in general) lacks the inductive bias for spatial invariance/equivariance that convolutional networks have. Consequently, ViTs require more data for pretraining to acquire useful "priors" from… Show more
------
Btw if you have an hour to spare on the weekend, I am covering how to use the Trainer to fit PyTorch models in the newly released Unit 5: 
https://lightning.ai/pages/courses/deep-learning-fundamentals/overview-organizing-your-code-with-pytorch-lightning/…
------
What an awesome  week for open source and the PyTorch ecosystem with three big launches!

- PyTorch 2.0
- Lightning Trainer 2.0 for PyTorch
- Fabric for PyTorch!

Just updated my "faster PyTorch" article to include the latest tools!

 https://sebastianraschka.com/blog/2023/pytorch-faster.html…
------
What an awesome  week for open source and the PyTorch ecosystem with three big launches!

- PyTorch 2.0
- Lightning Trainer 2.0 for PyTorch
- Fabric for PyTorch!

Just updated my "faster PyTorch" article to include the latest tools!

 https://sebastianraschka.com/blog/2023/pytorch-faster.html…
------
Easy to miss in the 
@pytorch
 2.0 release notes, they've added a small, but useful feature: torch.device, which previously just returned a device object, can now be used as a context manager.

0/8
------
Similar to fully-connected networks, the ViT architecture (and transformer architecture in general) lacks the inductive bias for spatial invariance/equivariance that convolutional networks have. Consequently, ViTs require more data for pretraining to acquire useful "priors" from… Show more
------
Does the road for (sample, not compute) efficient priors lead back to RNNs and the original Bahdanau attention mechanism (https://arxiv.org/abs/1409.0473)?  
------
Open source ftw !
What a week!
------
We’re excited to announce the release of PyTorch 2.0! 

This version includes:
 100% backward compatible
 Out of the box performance
 Significant speed improvements 

Learn more 
https://hubs.la/Q01H4L1t0
------
A comparison of the exact same model training for all 3 approaches

1) Pure PyTorch: https://github.com/rasbt/pytorch-fabric-demo/blob/main/src/1_pytorch-distilbert.py…
(25 min)

2) PyTorch + Fabric: https://github.com/rasbt/pytorch-fabric-demo/blob/main/src/2_pytorch-fabric-distilbert.py…
(1.7 min)

3) PyTorch + Trainer: https://github.com/rasbt/faster-pytorch-blog/blob/main/6_deepspeed.py… (2.7 min, the 1 extra min is for logging+checkpointing)
------
One of the Lightning 2.0 highlights is the open source Fabric library (https://lightning.ai/docs/fabric/stable/…)! 

Fabric let's you scale you PyTorch code with only a few lines of code.

Used the pre-release for my research & it's amazing!

Here's a quick demo + code: 
https://github.com/rasbt/pytorch-fabric-demo…
------
With the help of our community, we are excited to announce PyTorch Lightning 2.0

Install now https://lnkd.in/gYtwmJJU

Highlights Include:
 Commitment to backward compatibility in the 2.0 series
Simplified abstraction layers, removed legacy functionality, integrations… Show more
------
Sorry, the GitHub link should work now! https://github.com/rasbt/pytorch-fabric-demo…
------
And PyTorch Lightning 2.0 also features a leaner Trainer for extra code organization. Of course, I made sure that my DL Fundamentals course is fully compatible with the Trainer 2.0
------
Yesterday, AI became about corporate self interests. A divorce from the broad AI research field that made these companies even possible. 

PyTorch Lightning and 
@LightningAI
 will not sell out, we commit to continuing to give back to the AI community and opensource.
------
Since OpenAI’s surprise release of its GPT-4 model yesterday, there has been a raft of online criticism about the accompanying 'technical report.' Thanks to @_willfalcon for sharing his thoughts in this new Q&A: https://venturebeat.com/ai/lightning-ai-ceo-slams-openais-gpt-4-paper-as-masquerading-as-research/…
------
Google started rolling out PaLM API access to developers: https://blog.google/technology/ai/ai-developers-google-cloud-workspace/….

It will be interesting to see how it will compare to ChatGPT API.

If you want to learn more about PaLM, make sure to check out 
@cwolferesearch
's excellent post here: https://cameronrwolfe.substack.com/p/palm-efficiently-training-massive…

9/9
------
And maybe most interestingly, it outperforms GPT in terms of both training efficiency and modeling performance:
------
7) Vocabulary: SentencePiece vocabulary with 256k tokens. SentencePiece is a language-independent subword tokenizer
(Ref: https://arxiv.org/abs/1808.06226)

8/9
------
The two scenarios when fully connected layers are equivalent to convolutional networks.

PyTorch-based code illustration here: https://github.com/rasbt/MachineLearning-QandAI-book/blob/main/supplementary/q12-fc-cnn-equivalence/q12-fc-cnn-equivalence.ipynb…
------
Such an eventful week, and I am just catching up with Alpaca, which deserves a big shoutout.

Alpaca is an instruction-finetuned 7B language transformer based on the 7B LLaMA GPT-3 alternative by Meta released a few weeks ago.
https://crfm.stanford.edu/2023/03/13/alpaca.html…

1/6
------
Here's a small wishlist in case the authors are working on an Alpaca research paper:

1) Why 52k instruction-output pairs (and not more or less)? What are the scaling laws for finetuning here?

5/6
------
2) How does the performance of supervised finetuning here compare with proximal policy optimization-based finetuning?

3) Is there a significant performance difference when swapping text-davinci-003 with ChatGPT's GPT 3.5 or GPT 4?

6/6
------
Exactly! This is a great example of how CVPR publicity restrictions very effectively prevent unfair public visibility on social media for research from prestigious institutes like Ivy League. 

oh wait…
------
I don’t mean to be disrespectful to either @fchollet or the authors of ViperGPT (its great work!), but i have to point out that our work VisProg did something similar 4 months ago but it just went unnoticed because of CVPR’s publicity restrictions. @CVPR @CVPRConf twitter.com/fchollet/statu…
------
Plot twist: they didn’t disclose any details because there was actually no innovation to report, just a bit more finetuning, which would have looked underwhelming given all the hype.

By not sharing any details they made GPT-4 seem like a bigger innovation/deal than it really is.
------
GPT-4 was interesting for a hot second, I'll admit. 
But today is a new day: time to move on and get back to discussing research and open source.
------
But note that vision transformers (ViTs) are not free from any inductive biases!

ViTs focus more on global relationships due to the patchification & self-attention mechanism, which often leads to the perception that they act as low-pass filters, emphasizing (or recognizing)… Show more
------
Similar to fully-connected networks, the ViT architecture (and transformer architecture in general) lacks the inductive bias for spatial invariance/equivariance that convolutional networks have. Consequently, ViTs require more data for pretraining to acquire useful "priors" from… Show more
------
Btw if you have an hour to spare on the weekend, I am covering how to use the Trainer to fit PyTorch models in the newly released Unit 5: 
https://lightning.ai/pages/courses/deep-learning-fundamentals/overview-organizing-your-code-with-pytorch-lightning/…
------
What an awesome  week for open source and the PyTorch ecosystem with three big launches!

- PyTorch 2.0
- Lightning Trainer 2.0 for PyTorch
- Fabric for PyTorch!

Just updated my "faster PyTorch" article to include the latest tools!

 https://sebastianraschka.com/blog/2023/pytorch-faster.html…
------
What an awesome  week for open source and the PyTorch ecosystem with three big launches!

- PyTorch 2.0
- Lightning Trainer 2.0 for PyTorch
- Fabric for PyTorch!

Just updated my "faster PyTorch" article to include the latest tools!

 https://sebastianraschka.com/blog/2023/pytorch-faster.html…
------
Easy to miss in the 
@pytorch
 2.0 release notes, they've added a small, but useful feature: torch.device, which previously just returned a device object, can now be used as a context manager.

0/8
------
Similar to fully-connected networks, the ViT architecture (and transformer architecture in general) lacks the inductive bias for spatial invariance/equivariance that convolutional networks have. Consequently, ViTs require more data for pretraining to acquire useful "priors" from… Show more
------
Does the road for (sample, not compute) efficient priors lead back to RNNs and the original Bahdanau attention mechanism (https://arxiv.org/abs/1409.0473)?  
------
Open source ftw !
What a week!
------
We’re excited to announce the release of PyTorch 2.0! 

This version includes:
 100% backward compatible
 Out of the box performance
 Significant speed improvements 

Learn more 
https://hubs.la/Q01H4L1t0
------
A comparison of the exact same model training for all 3 approaches

1) Pure PyTorch: https://github.com/rasbt/pytorch-fabric-demo/blob/main/src/1_pytorch-distilbert.py…
(25 min)

2) PyTorch + Fabric: https://github.com/rasbt/pytorch-fabric-demo/blob/main/src/2_pytorch-fabric-distilbert.py…
(1.7 min)

3) PyTorch + Trainer: https://github.com/rasbt/faster-pytorch-blog/blob/main/6_deepspeed.py… (2.7 min, the 1 extra min is for logging+checkpointing)
------
One of the Lightning 2.0 highlights is the open source Fabric library (https://lightning.ai/docs/fabric/stable/…)! 

Fabric let's you scale you PyTorch code with only a few lines of code.

Used the pre-release for my research & it's amazing!

Here's a quick demo + code: 
https://github.com/rasbt/pytorch-fabric-demo…
------
With the help of our community, we are excited to announce PyTorch Lightning 2.0

Install now https://lnkd.in/gYtwmJJU

Highlights Include:
 Commitment to backward compatibility in the 2.0 series
Simplified abstraction layers, removed legacy functionality, integrations… Show more
------
Sorry, the GitHub link should work now! https://github.com/rasbt/pytorch-fabric-demo…
------
And PyTorch Lightning 2.0 also features a leaner Trainer for extra code organization. Of course, I made sure that my DL Fundamentals course is fully compatible with the Trainer 2.0
------
Yesterday, AI became about corporate self interests. A divorce from the broad AI research field that made these companies even possible. 

PyTorch Lightning and 
@LightningAI
 will not sell out, we commit to continuing to give back to the AI community and opensource.
------
Since OpenAI’s surprise release of its GPT-4 model yesterday, there has been a raft of online criticism about the accompanying 'technical report.' Thanks to @_willfalcon for sharing his thoughts in this new Q&A: https://venturebeat.com/ai/lightning-ai-ceo-slams-openais-gpt-4-paper-as-masquerading-as-research/…
------
GPT-4 was interesting for a hot second, I'll admit. 
But today is a new day: time to move on and get back to discussing research and open source.
------
I usually discuss relevant papers in my monthly newsletter.
And if useful, I have a section on how I stay up to date in issue No 2 here: https://magazine.sebastianraschka.com/p/ahead-of-ai-2-transformers-fast-and…
------
You love using PyTorch for Deep Learning but want it a bit more organized, so it's easier to take advantage of more advanced features?

Great news: Unit 5 is finally live! In Unit 5, I'll show you how to train PyTorch models with the Lightning Trainer!

 https://lightning.ai/pages/courses/deep-learning-fundamentals/overview-organizing-your-code-with-pytorch-lightning/…
------
Btw in this unit I am teaching you the exact same approach I use to finetune large language models. 

Using the Trainer with mixed-precision & multi-GPU training lets you train an LLM classifierl in ~3 min* on 50k example target data: https://sebastianraschka.com/blog/2023/pytorch-faster.html…

*if you have 4 GPUs
------
You love using PyTorch for Deep Learning but want it a bit more organized, so it's easier to take advantage of more advanced features?

Great news: Unit 5 is finally live! In Unit 5, I'll show you how to train PyTorch models with the Lightning Trainer!

 https://lightning.ai/pages/courses/deep-learning-fundamentals/overview-organizing-your-code-with-pytorch-lightning/…
------
Such an eventful week, and I am just catching up with Alpaca, which deserves a big shoutout.

Alpaca is an instruction-finetuned 7B language transformer based on the 7B LLaMA GPT-3 alternative by Meta released a few weeks ago.
https://crfm.stanford.edu/2023/03/13/alpaca.html…

1/6
------
Here's a small wishlist in case the authors are working on an Alpaca research paper:

1) Why 52k instruction-output pairs (and not more or less)? What are the scaling laws for finetuning here?

5/6
------
2) How does the performance of supervised finetuning here compare with proximal policy optimization-based finetuning?

3) Is there a significant performance difference when swapping text-davinci-003 with ChatGPT's GPT 3.5 or GPT 4?

6/6
------
Exactly! This is a great example of how CVPR publicity restrictions very effectively prevent unfair public visibility on social media for research from prestigious institutes like Ivy League. 

oh wait…
------
I don’t mean to be disrespectful to either @fchollet or the authors of ViperGPT (its great work!), but i have to point out that our work VisProg did something similar 4 months ago but it just went unnoticed because of CVPR’s publicity restrictions. @CVPR @CVPRConf twitter.com/fchollet/statu…
------
Plot twist: they didn’t disclose any details because there was actually no innovation to report, just a bit more finetuning, which would have looked underwhelming given all the hype.

By not sharing any details they made GPT-4 seem like a bigger innovation/deal than it really is.
------
GPT-4 was interesting for a hot second, I'll admit. 
But today is a new day: time to move on and get back to discussing research and open source.
------
But note that vision transformers (ViTs) are not free from any inductive biases!

ViTs focus more on global relationships due to the patchification & self-attention mechanism, which often leads to the perception that they act as low-pass filters, emphasizing (or recognizing)… Show more
------
Similar to fully-connected networks, the ViT architecture (and transformer architecture in general) lacks the inductive bias for spatial invariance/equivariance that convolutional networks have. Consequently, ViTs require more data for pretraining to acquire useful "priors" from… Show more
------
Btw if you have an hour to spare on the weekend, I am covering how to use the Trainer to fit PyTorch models in the newly released Unit 5: 
https://lightning.ai/pages/courses/deep-learning-fundamentals/overview-organizing-your-code-with-pytorch-lightning/…
------
What an awesome  week for open source and the PyTorch ecosystem with three big launches!

- PyTorch 2.0
- Lightning Trainer 2.0 for PyTorch
- Fabric for PyTorch!

Just updated my "faster PyTorch" article to include the latest tools!

 https://sebastianraschka.com/blog/2023/pytorch-faster.html…
------
What an awesome  week for open source and the PyTorch ecosystem with three big launches!

- PyTorch 2.0
- Lightning Trainer 2.0 for PyTorch
- Fabric for PyTorch!

Just updated my "faster PyTorch" article to include the latest tools!

 https://sebastianraschka.com/blog/2023/pytorch-faster.html…
------
Easy to miss in the 
@pytorch
 2.0 release notes, they've added a small, but useful feature: torch.device, which previously just returned a device object, can now be used as a context manager.

0/8
------
Similar to fully-connected networks, the ViT architecture (and transformer architecture in general) lacks the inductive bias for spatial invariance/equivariance that convolutional networks have. Consequently, ViTs require more data for pretraining to acquire useful "priors" from… Show more
------
Does the road for (sample, not compute) efficient priors lead back to RNNs and the original Bahdanau attention mechanism (https://arxiv.org/abs/1409.0473)?  
------
Open source ftw !
What a week!
------
We’re excited to announce the release of PyTorch 2.0! 

This version includes:
 100% backward compatible
 Out of the box performance
 Significant speed improvements 

Learn more 
https://hubs.la/Q01H4L1t0
------
A comparison of the exact same model training for all 3 approaches

1) Pure PyTorch: https://github.com/rasbt/pytorch-fabric-demo/blob/main/src/1_pytorch-distilbert.py…
(25 min)

2) PyTorch + Fabric: https://github.com/rasbt/pytorch-fabric-demo/blob/main/src/2_pytorch-fabric-distilbert.py…
(1.7 min)

3) PyTorch + Trainer: https://github.com/rasbt/faster-pytorch-blog/blob/main/6_deepspeed.py… (2.7 min, the 1 extra min is for logging+checkpointing)
------
One of the Lightning 2.0 highlights is the open source Fabric library (https://lightning.ai/docs/fabric/stable/…)! 

Fabric let's you scale you PyTorch code with only a few lines of code.

Used the pre-release for my research & it's amazing!

Here's a quick demo + code: 
https://github.com/rasbt/pytorch-fabric-demo…
------
With the help of our community, we are excited to announce PyTorch Lightning 2.0

Install now https://lnkd.in/gYtwmJJU

Highlights Include:
 Commitment to backward compatibility in the 2.0 series
Simplified abstraction layers, removed legacy functionality, integrations… Show more
------
Sorry, the GitHub link should work now! https://github.com/rasbt/pytorch-fabric-demo…
------
And PyTorch Lightning 2.0 also features a leaner Trainer for extra code organization. Of course, I made sure that my DL Fundamentals course is fully compatible with the Trainer 2.0
------
Yesterday, AI became about corporate self interests. A divorce from the broad AI research field that made these companies even possible. 

PyTorch Lightning and 
@LightningAI
 will not sell out, we commit to continuing to give back to the AI community and opensource.
------
Since OpenAI’s surprise release of its GPT-4 model yesterday, there has been a raft of online criticism about the accompanying 'technical report.' Thanks to @_willfalcon for sharing his thoughts in this new Q&A: https://venturebeat.com/ai/lightning-ai-ceo-slams-openais-gpt-4-paper-as-masquerading-as-research/…
------
GPT-4 was interesting for a hot second, I'll admit. 
But today is a new day: time to move on and get back to discussing research and open source.
------
I usually discuss relevant papers in my monthly newsletter.
And if useful, I have a section on how I stay up to date in issue No 2 here: https://magazine.sebastianraschka.com/p/ahead-of-ai-2-transformers-fast-and…
------
You love using PyTorch for Deep Learning but want it a bit more organized, so it's easier to take advantage of more advanced features?

Great news: Unit 5 is finally live! In Unit 5, I'll show you how to train PyTorch models with the Lightning Trainer!

 https://lightning.ai/pages/courses/deep-learning-fundamentals/overview-organizing-your-code-with-pytorch-lightning/…
------
Btw in this unit I am teaching you the exact same approach I use to finetune large language models. 

Using the Trainer with mixed-precision & multi-GPU training lets you train an LLM classifierl in ~3 min* on 50k example target data: https://sebastianraschka.com/blog/2023/pytorch-faster.html…

*if you have 4 GPUs
------
You love using PyTorch for Deep Learning but want it a bit more organized, so it's easier to take advantage of more advanced features?

Great news: Unit 5 is finally live! In Unit 5, I'll show you how to train PyTorch models with the Lightning Trainer!

 https://lightning.ai/pages/courses/deep-learning-fundamentals/overview-organizing-your-code-with-pytorch-lightning/…
------
Some takeaways after reading the GPT-4 paper: https://cdn.openai.com/papers/gpt-4.pdf…

The good:

1)  The model now accepts multimodal inputs: images and text

2) The paper is surprisingly honest: "While less capable than humans in many real-world scenarios...", "GPT-4's capabilities and… Show more
------
You love using PyTorch for Deep Learning but want it a bit more organized, so it's easier to take advantage of more advanced features?

Great news: Unit 5 is finally live! In Unit 5, I'll show you how to train PyTorch models with the Lightning Trainer!

 https://lightning.ai/pages/courses/deep-learning-fundamentals/overview-organizing-your-code-with-pytorch-lightning/…
------
I should note that this is also the foundation for the upcoming ones in the next weeks, where we'll tackle lots of great stuff:

- learning rate schedulers
- mixed precision training
- checkpointing
- multi-GPU training
- neural network debugging

and more!
------
PS: I also cover 
- code reproducibility, 
- logging, 
- and creating custom callbacks 
as a warmup for the next unit on Essential Deep Learning Tips & Tricks using the 
@LightningAI
 Trainer!
------
Machine learning competitions are often a good indicator of what techniques actually work well in practice on new datasets. 
The very comprehensive State of Competitive Machine Learning 2022 report just came out and contained many interesting and https://mlcontests.com/state-of-competitive-machine-learning-2022/…… Show more
------
My two main surprises here were that ViT already halfway caught up with CNNs in vision. And another positive surprise is that k-fold CV is more widely used than I thought. 

Where’s scikit-learn? Is it still relevant? Scikit-learn implements fundamental models, and they are not… Show more
------
But note that vision transformers (ViTs) are not free from any inductive biases!

ViTs focus more on global relationships due to the patchification & self-attention mechanism, which often leads to the perception that they act as low-pass filters, emphasizing (or recognizing)… Show more
------
Similar to fully-connected networks, the ViT architecture (and transformer architecture in general) lacks the inductive bias for spatial invariance/equivariance that convolutional networks have. Consequently, ViTs require more data for pretraining to acquire useful "priors" from… Show more
------
Btw if you have an hour to spare on the weekend, I am covering how to use the Trainer to fit PyTorch models in the newly released Unit 5: 
https://lightning.ai/pages/courses/deep-learning-fundamentals/overview-organizing-your-code-with-pytorch-lightning/…
------
What an awesome  week for open source and the PyTorch ecosystem with three big launches!

- PyTorch 2.0
- Lightning Trainer 2.0 for PyTorch
- Fabric for PyTorch!

Just updated my "faster PyTorch" article to include the latest tools!

 https://sebastianraschka.com/blog/2023/pytorch-faster.html…
------
What an awesome  week for open source and the PyTorch ecosystem with three big launches!

- PyTorch 2.0
- Lightning Trainer 2.0 for PyTorch
- Fabric for PyTorch!

Just updated my "faster PyTorch" article to include the latest tools!

 https://sebastianraschka.com/blog/2023/pytorch-faster.html…
------
Easy to miss in the 
@pytorch
 2.0 release notes, they've added a small, but useful feature: torch.device, which previously just returned a device object, can now be used as a context manager.

0/8
------
Similar to fully-connected networks, the ViT architecture (and transformer architecture in general) lacks the inductive bias for spatial invariance/equivariance that convolutional networks have. Consequently, ViTs require more data for pretraining to acquire useful "priors" from… Show more
------
Does the road for (sample, not compute) efficient priors lead back to RNNs and the original Bahdanau attention mechanism (https://arxiv.org/abs/1409.0473)?  
------
Open source ftw !
What a week!
------
We’re excited to announce the release of PyTorch 2.0! 

This version includes:
 100% backward compatible
 Out of the box performance
 Significant speed improvements 

Learn more 
https://hubs.la/Q01H4L1t0
------
A comparison of the exact same model training for all 3 approaches

1) Pure PyTorch: https://github.com/rasbt/pytorch-fabric-demo/blob/main/src/1_pytorch-distilbert.py…
(25 min)

2) PyTorch + Fabric: https://github.com/rasbt/pytorch-fabric-demo/blob/main/src/2_pytorch-fabric-distilbert.py…
(1.7 min)

3) PyTorch + Trainer: https://github.com/rasbt/faster-pytorch-blog/blob/main/6_deepspeed.py… (2.7 min, the 1 extra min is for logging+checkpointing)
------
One of the Lightning 2.0 highlights is the open source Fabric library (https://lightning.ai/docs/fabric/stable/…)! 

Fabric let's you scale you PyTorch code with only a few lines of code.

Used the pre-release for my research & it's amazing!

Here's a quick demo + code: 
https://github.com/rasbt/pytorch-fabric-demo…
------
With the help of our community, we are excited to announce PyTorch Lightning 2.0

Install now https://lnkd.in/gYtwmJJU

Highlights Include:
 Commitment to backward compatibility in the 2.0 series
Simplified abstraction layers, removed legacy functionality, integrations… Show more
------
Sorry, the GitHub link should work now! https://github.com/rasbt/pytorch-fabric-demo…
------
And PyTorch Lightning 2.0 also features a leaner Trainer for extra code organization. Of course, I made sure that my DL Fundamentals course is fully compatible with the Trainer 2.0
------
Yesterday, AI became about corporate self interests. A divorce from the broad AI research field that made these companies even possible. 

PyTorch Lightning and 
@LightningAI
 will not sell out, we commit to continuing to give back to the AI community and opensource.
------
Since OpenAI’s surprise release of its GPT-4 model yesterday, there has been a raft of online criticism about the accompanying 'technical report.' Thanks to @_willfalcon for sharing his thoughts in this new Q&A: https://venturebeat.com/ai/lightning-ai-ceo-slams-openais-gpt-4-paper-as-masquerading-as-research/…
------
GPT-4 was interesting for a hot second, I'll admit. 
But today is a new day: time to move on and get back to discussing research and open source.
------
I usually discuss relevant papers in my monthly newsletter.
And if useful, I have a section on how I stay up to date in issue No 2 here: https://magazine.sebastianraschka.com/p/ahead-of-ai-2-transformers-fast-and…
------
You love using PyTorch for Deep Learning but want it a bit more organized, so it's easier to take advantage of more advanced features?

Great news: Unit 5 is finally live! In Unit 5, I'll show you how to train PyTorch models with the Lightning Trainer!

 https://lightning.ai/pages/courses/deep-learning-fundamentals/overview-organizing-your-code-with-pytorch-lightning/…
------
Btw in this unit I am teaching you the exact same approach I use to finetune large language models. 

Using the Trainer with mixed-precision & multi-GPU training lets you train an LLM classifierl in ~3 min* on 50k example target data: https://sebastianraschka.com/blog/2023/pytorch-faster.html…

*if you have 4 GPUs
------
You love using PyTorch for Deep Learning but want it a bit more organized, so it's easier to take advantage of more advanced features?

Great news: Unit 5 is finally live! In Unit 5, I'll show you how to train PyTorch models with the Lightning Trainer!

 https://lightning.ai/pages/courses/deep-learning-fundamentals/overview-organizing-your-code-with-pytorch-lightning/…
------
Some takeaways after reading the GPT-4 paper: https://cdn.openai.com/papers/gpt-4.pdf…

The good:

1)  The model now accepts multimodal inputs: images and text

2) The paper is surprisingly honest: "While less capable than humans in many real-world scenarios...", "GPT-4's capabilities and… Show more
------
You love using PyTorch for Deep Learning but want it a bit more organized, so it's easier to take advantage of more advanced features?

Great news: Unit 5 is finally live! In Unit 5, I'll show you how to train PyTorch models with the Lightning Trainer!

 https://lightning.ai/pages/courses/deep-learning-fundamentals/overview-organizing-your-code-with-pytorch-lightning/…
------
I should note that this is also the foundation for the upcoming ones in the next weeks, where we'll tackle lots of great stuff:

- learning rate schedulers
- mixed precision training
- checkpointing
- multi-GPU training
- neural network debugging

and more!
------
PS: I also cover 
- code reproducibility, 
- logging, 
- and creating custom callbacks 
as a warmup for the next unit on Essential Deep Learning Tips & Tricks using the 
@LightningAI
 Trainer!
------
Machine learning competitions are often a good indicator of what techniques actually work well in practice on new datasets. 
The very comprehensive State of Competitive Machine Learning 2022 report just came out and contained many interesting and https://mlcontests.com/state-of-competitive-machine-learning-2022/…… Show more
------
My two main surprises here were that ViT already halfway caught up with CNNs in vision. And another positive surprise is that k-fold CV is more widely used than I thought. 

Where’s scikit-learn? Is it still relevant? Scikit-learn implements fundamental models, and they are not… Show more
------
Thanks for all the comments on this so far! Very interesting! I am particularly surprised that they are dominated by TF vs PyTorch discussions rather than DL vs gradient boosting for tabular data, or cross-validation vs fixed validation sets.
------
Just see there's also another nice summary thread via 
@ml_contests
 here:
------
 The State of Competitive Machine Learning in 2022 
-  Analysis of 200+ ML competitions and 67 winning solutions from 2022 
-  Winners mostly use a common toolkit - Python, PyData, PyTorch, and a few other key tools.
Full report here: https://mlcontests.com/state-of-competitive-machine-learning-2022?ref=mlctwitter…
 (1/n)
------
Just took the new Lion optimizer (https://arxiv.org/abs/2302.06675) for a spin, and I am positively surprised.

With a bit of tinkering, I got it to perform similar to Adam for finetuning DistilBERT (never had any luck with SGD on that model). 

Code here for ref: https://github.com/rasbt/try-lion-optimizer…
------
I should say that I didn't notice any speedup though. All methods were around 4.3 min on avg with the same hardware setup.
------
Just saw that the final 
@PyTorch
 2.0 release candidate is out now! 

CPU: 
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/test/cpu

CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/test/cu118

ROCm 5.4
pip https://download.pytorch.org/whl/test/rocm5.4/……
------
Ahead of AI #6: TrAIn Differently is out!
https://magazine.sebastianraschka.com/p/ahead-of-ai-6-train-differently?sd=pf…

This issue covers alternative training techniques for large language models and reinforcement learning with human feedback.

Also, I have an extensive section on how to read a research paper!

Happy reading!
------
Similar to fully-connected networks, the ViT architecture (and transformer architecture in general) lacks the inductive bias for spatial invariance/equivariance that convolutional networks have. Consequently, ViTs require more data for pretraining to acquire useful "priors" from… Show more
------
Does the road for (sample, not compute) efficient priors lead back to RNNs and the original Bahdanau attention mechanism (https://arxiv.org/abs/1409.0473)?  
------
Open source ftw !
What a week!
------
We’re excited to announce the release of PyTorch 2.0! 

This version includes:
 100% backward compatible
 Out of the box performance
 Significant speed improvements 

Learn more 
https://hubs.la/Q01H4L1t0
------
A comparison of the exact same model training for all 3 approaches

1) Pure PyTorch: https://github.com/rasbt/pytorch-fabric-demo/blob/main/src/1_pytorch-distilbert.py…
(25 min)

2) PyTorch + Fabric: https://github.com/rasbt/pytorch-fabric-demo/blob/main/src/2_pytorch-fabric-distilbert.py…
(1.7 min)

3) PyTorch + Trainer: https://github.com/rasbt/faster-pytorch-blog/blob/main/6_deepspeed.py… (2.7 min, the 1 extra min is for logging+checkpointing)
------
One of the Lightning 2.0 highlights is the open source Fabric library (https://lightning.ai/docs/fabric/stable/…)! 

Fabric let's you scale you PyTorch code with only a few lines of code.

Used the pre-release for my research & it's amazing!

Here's a quick demo + code: 
https://github.com/rasbt/pytorch-fabric-demo…
------
With the help of our community, we are excited to announce PyTorch Lightning 2.0

Install now https://lnkd.in/gYtwmJJU

Highlights Include:
 Commitment to backward compatibility in the 2.0 series
Simplified abstraction layers, removed legacy functionality, integrations… Show more
------
Sorry, the GitHub link should work now! https://github.com/rasbt/pytorch-fabric-demo…
------
And PyTorch Lightning 2.0 also features a leaner Trainer for extra code organization. Of course, I made sure that my DL Fundamentals course is fully compatible with the Trainer 2.0
------
Yesterday, AI became about corporate self interests. A divorce from the broad AI research field that made these companies even possible. 

PyTorch Lightning and 
@LightningAI
 will not sell out, we commit to continuing to give back to the AI community and opensource.
------
Since OpenAI’s surprise release of its GPT-4 model yesterday, there has been a raft of online criticism about the accompanying 'technical report.' Thanks to @_willfalcon for sharing his thoughts in this new Q&A: https://venturebeat.com/ai/lightning-ai-ceo-slams-openais-gpt-4-paper-as-masquerading-as-research/…
------
GPT-4 was interesting for a hot second, I'll admit. 
But today is a new day: time to move on and get back to discussing research and open source.
------
I usually discuss relevant papers in my monthly newsletter.
And if useful, I have a section on how I stay up to date in issue No 2 here: https://magazine.sebastianraschka.com/p/ahead-of-ai-2-transformers-fast-and…
------
You love using PyTorch for Deep Learning but want it a bit more organized, so it's easier to take advantage of more advanced features?

Great news: Unit 5 is finally live! In Unit 5, I'll show you how to train PyTorch models with the Lightning Trainer!

 https://lightning.ai/pages/courses/deep-learning-fundamentals/overview-organizing-your-code-with-pytorch-lightning/…
------
Btw in this unit I am teaching you the exact same approach I use to finetune large language models. 

Using the Trainer with mixed-precision & multi-GPU training lets you train an LLM classifierl in ~3 min* on 50k example target data: https://sebastianraschka.com/blog/2023/pytorch-faster.html…

*if you have 4 GPUs
------
You love using PyTorch for Deep Learning but want it a bit more organized, so it's easier to take advantage of more advanced features?

Great news: Unit 5 is finally live! In Unit 5, I'll show you how to train PyTorch models with the Lightning Trainer!

 https://lightning.ai/pages/courses/deep-learning-fundamentals/overview-organizing-your-code-with-pytorch-lightning/…
------
Some takeaways after reading the GPT-4 paper: https://cdn.openai.com/papers/gpt-4.pdf…

The good:

1)  The model now accepts multimodal inputs: images and text

2) The paper is surprisingly honest: "While less capable than humans in many real-world scenarios...", "GPT-4's capabilities and… Show more
------
You love using PyTorch for Deep Learning but want it a bit more organized, so it's easier to take advantage of more advanced features?

Great news: Unit 5 is finally live! In Unit 5, I'll show you how to train PyTorch models with the Lightning Trainer!

 https://lightning.ai/pages/courses/deep-learning-fundamentals/overview-organizing-your-code-with-pytorch-lightning/…
------
I should note that this is also the foundation for the upcoming ones in the next weeks, where we'll tackle lots of great stuff:

- learning rate schedulers
- mixed precision training
- checkpointing
- multi-GPU training
- neural network debugging

and more!
------
PS: I also cover 
- code reproducibility, 
- logging, 
- and creating custom callbacks 
as a warmup for the next unit on Essential Deep Learning Tips & Tricks using the 
@LightningAI
 Trainer!
------
Machine learning competitions are often a good indicator of what techniques actually work well in practice on new datasets. 
The very comprehensive State of Competitive Machine Learning 2022 report just came out and contained many interesting and https://mlcontests.com/state-of-competitive-machine-learning-2022/…… Show more
------
My two main surprises here were that ViT already halfway caught up with CNNs in vision. And another positive surprise is that k-fold CV is more widely used than I thought. 

Where’s scikit-learn? Is it still relevant? Scikit-learn implements fundamental models, and they are not… Show more
------
Thanks for all the comments on this so far! Very interesting! I am particularly surprised that they are dominated by TF vs PyTorch discussions rather than DL vs gradient boosting for tabular data, or cross-validation vs fixed validation sets.
------
Just see there's also another nice summary thread via 
@ml_contests
 here:
------
 The State of Competitive Machine Learning in 2022 
-  Analysis of 200+ ML competitions and 67 winning solutions from 2022 
-  Winners mostly use a common toolkit - Python, PyData, PyTorch, and a few other key tools.
Full report here: https://mlcontests.com/state-of-competitive-machine-learning-2022?ref=mlctwitter…
 (1/n)
------
Just took the new Lion optimizer (https://arxiv.org/abs/2302.06675) for a spin, and I am positively surprised.

With a bit of tinkering, I got it to perform similar to Adam for finetuning DistilBERT (never had any luck with SGD on that model). 

Code here for ref: https://github.com/rasbt/try-lion-optimizer…
------
I should say that I didn't notice any speedup though. All methods were around 4.3 min on avg with the same hardware setup.
------
Just saw that the final 
@PyTorch
 2.0 release candidate is out now! 

CPU: 
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/test/cpu

CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/test/cu118

ROCm 5.4
pip https://download.pytorch.org/whl/test/rocm5.4/……
------
Ahead of AI #6: TrAIn Differently is out!
https://magazine.sebastianraschka.com/p/ahead-of-ai-6-train-differently?sd=pf…

This issue covers alternative training techniques for large language models and reinforcement learning with human feedback.

Also, I have an extensive section on how to read a research paper!

Happy reading!
------
Now, while RLHF seems to be a very popular way for finetune the latest LLMs, it's of course not the only promising solution moving forward.

In the recent "The Wisdom of Hindsight makes Language Models Better Instruction Followers"(https://arxiv.org/abs/2302.05206), researchers propose a…
------
Why do latest language transformers (LLMs like ChatGPT etc.) use reinforcement learning (RL) for finetuning instead of regular supervised learning (SL)?

There are at least 5 reasons ...
[1/10]
------
We need more nuanced discussions around the risk of open sourcing models. 

Open source brings valuable access, but it is absurd to ignore the fact that it lowers the barriers to entry for both useful use cases and potential misuse.
------
I love Pandas! I've been using it ever since I started doing machine learning more than a decade ago. 

Now that the Pandas 2.0 release candidate is out, I was just taking the new PyArrow backend for a test drive. It's a significant boost over the original https://github.com/rasbt/machine-learning-notes/blob/main/benchmark/pandas-pyarrow/pandas2-pyarrow.ipynb……
------
One of the Lightning 2.0 highlights is the open source Fabric library (https://lightning.ai/docs/fabric/stable/…)! 

Fabric let's you scale you PyTorch code with only a few lines of code.

Used the pre-release for my research & it's amazing!

Here's a quick demo + code: 
https://github.com/rasbt/pytorch-fabric-demo…
------
With the help of our community, we are excited to announce PyTorch Lightning 2.0

Install now https://lnkd.in/gYtwmJJU

Highlights Include:
 Commitment to backward compatibility in the 2.0 series
Simplified abstraction layers, removed legacy functionality, integrations… Show more
------
Sorry, the GitHub link should work now! https://github.com/rasbt/pytorch-fabric-demo…
------
And PyTorch Lightning 2.0 also features a leaner Trainer for extra code organization. Of course, I made sure that my DL Fundamentals course is fully compatible with the Trainer 2.0
------
Yesterday, AI became about corporate self interests. A divorce from the broad AI research field that made these companies even possible. 

PyTorch Lightning and 
@LightningAI
 will not sell out, we commit to continuing to give back to the AI community and opensource.
------
Since OpenAI’s surprise release of its GPT-4 model yesterday, there has been a raft of online criticism about the accompanying 'technical report.' Thanks to @_willfalcon for sharing his thoughts in this new Q&A: https://venturebeat.com/ai/lightning-ai-ceo-slams-openais-gpt-4-paper-as-masquerading-as-research/…
------
GPT-4 was interesting for a hot second, I'll admit. 
But today is a new day: time to move on and get back to discussing research and open source.
------
I usually discuss relevant papers in my monthly newsletter.
And if useful, I have a section on how I stay up to date in issue No 2 here: https://magazine.sebastianraschka.com/p/ahead-of-ai-2-transformers-fast-and…
------
You love using PyTorch for Deep Learning but want it a bit more organized, so it's easier to take advantage of more advanced features?

Great news: Unit 5 is finally live! In Unit 5, I'll show you how to train PyTorch models with the Lightning Trainer!

 https://lightning.ai/pages/courses/deep-learning-fundamentals/overview-organizing-your-code-with-pytorch-lightning/…
------
Btw in this unit I am teaching you the exact same approach I use to finetune large language models. 

Using the Trainer with mixed-precision & multi-GPU training lets you train an LLM classifierl in ~3 min* on 50k example target data: https://sebastianraschka.com/blog/2023/pytorch-faster.html…

*if you have 4 GPUs
------
You love using PyTorch for Deep Learning but want it a bit more organized, so it's easier to take advantage of more advanced features?

Great news: Unit 5 is finally live! In Unit 5, I'll show you how to train PyTorch models with the Lightning Trainer!

 https://lightning.ai/pages/courses/deep-learning-fundamentals/overview-organizing-your-code-with-pytorch-lightning/…
------
Some takeaways after reading the GPT-4 paper: https://cdn.openai.com/papers/gpt-4.pdf…

The good:

1)  The model now accepts multimodal inputs: images and text

2) The paper is surprisingly honest: "While less capable than humans in many real-world scenarios...", "GPT-4's capabilities and… Show more
------
You love using PyTorch for Deep Learning but want it a bit more organized, so it's easier to take advantage of more advanced features?

Great news: Unit 5 is finally live! In Unit 5, I'll show you how to train PyTorch models with the Lightning Trainer!

 https://lightning.ai/pages/courses/deep-learning-fundamentals/overview-organizing-your-code-with-pytorch-lightning/…
------
I should note that this is also the foundation for the upcoming ones in the next weeks, where we'll tackle lots of great stuff:

- learning rate schedulers
- mixed precision training
- checkpointing
- multi-GPU training
- neural network debugging

and more!
------
PS: I also cover 
- code reproducibility, 
- logging, 
- and creating custom callbacks 
as a warmup for the next unit on Essential Deep Learning Tips & Tricks using the 
@LightningAI
 Trainer!
------
Machine learning competitions are often a good indicator of what techniques actually work well in practice on new datasets. 
The very comprehensive State of Competitive Machine Learning 2022 report just came out and contained many interesting and https://mlcontests.com/state-of-competitive-machine-learning-2022/…… Show more
------
My two main surprises here were that ViT already halfway caught up with CNNs in vision. And another positive surprise is that k-fold CV is more widely used than I thought. 

Where’s scikit-learn? Is it still relevant? Scikit-learn implements fundamental models, and they are not… Show more
------
Thanks for all the comments on this so far! Very interesting! I am particularly surprised that they are dominated by TF vs PyTorch discussions rather than DL vs gradient boosting for tabular data, or cross-validation vs fixed validation sets.
------
Just see there's also another nice summary thread via 
@ml_contests
 here:
------
 The State of Competitive Machine Learning in 2022 
-  Analysis of 200+ ML competitions and 67 winning solutions from 2022 
-  Winners mostly use a common toolkit - Python, PyData, PyTorch, and a few other key tools.
Full report here: https://mlcontests.com/state-of-competitive-machine-learning-2022?ref=mlctwitter…
 (1/n)
------
Just took the new Lion optimizer (https://arxiv.org/abs/2302.06675) for a spin, and I am positively surprised.

With a bit of tinkering, I got it to perform similar to Adam for finetuning DistilBERT (never had any luck with SGD on that model). 

Code here for ref: https://github.com/rasbt/try-lion-optimizer…
------
I should say that I didn't notice any speedup though. All methods were around 4.3 min on avg with the same hardware setup.
------
Just saw that the final 
@PyTorch
 2.0 release candidate is out now! 

CPU: 
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/test/cpu

CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/test/cu118

ROCm 5.4
pip https://download.pytorch.org/whl/test/rocm5.4/……
------
Ahead of AI #6: TrAIn Differently is out!
https://magazine.sebastianraschka.com/p/ahead-of-ai-6-train-differently?sd=pf…

This issue covers alternative training techniques for large language models and reinforcement learning with human feedback.

Also, I have an extensive section on how to read a research paper!

Happy reading!
------
Now, while RLHF seems to be a very popular way for finetune the latest LLMs, it's of course not the only promising solution moving forward.

In the recent "The Wisdom of Hindsight makes Language Models Better Instruction Followers"(https://arxiv.org/abs/2302.05206), researchers propose a…
------
Why do latest language transformers (LLMs like ChatGPT etc.) use reinforcement learning (RL) for finetuning instead of regular supervised learning (SL)?

There are at least 5 reasons ...
[1/10]
------
We need more nuanced discussions around the risk of open sourcing models. 

Open source brings valuable access, but it is absurd to ignore the fact that it lowers the barriers to entry for both useful use cases and potential misuse.
------
I love Pandas! I've been using it ever since I started doing machine learning more than a decade ago. 

Now that the Pandas 2.0 release candidate is out, I was just taking the new PyArrow backend for a test drive. It's a significant boost over the original https://github.com/rasbt/machine-learning-notes/blob/main/benchmark/pandas-pyarrow/pandas2-pyarrow.ipynb……
------
Pro tip: Instead of using the pyarrow backend explicitely on very DataFrame via 

df = pd.DataFrame(numbers, dtype="float64[pyarrow]")

you can enable it globally via 

pd.options.mode.dtype_backend = "pyarrow"
------
When using automatic mixed-precision training to accelerate model training, there are two common options: float16 and bfloat16 (16-bit "brain" floating points). What's the difference?

Compared to float16, bfloat16 has the same dynamic range as float32 but lower precision.

In my…
------
Productive week! We recorded 5 new units of Deep Learning Fundamentals! And the new Units will start dropping mid-March: https://lightning.ai/pages/courses/deep-learning-fundamentals/…

We'll cover code organization, computer vision, transformers, and performance tricks (mixed precision, multi-GPU paradigms & more!)
------
Are transformers truly more easy to parallelize than recurrent neural networks? Yes and no. For encoder-style models like BERT, this is of course true. But in recent months, transformer has become synonymous to GPT (a decoder-style model). 

Sure, we can parallelize each…
------
Why do latest language transformers (LLMs like ChatGPT etc.) use reinforcement learning (RL) for finetuning instead of regular supervised learning (SL)?

There are at least 5 reasons ...
[1/10]
------
GPT-4 was interesting for a hot second, I'll admit. 
But today is a new day: time to move on and get back to discussing research and open source.
------
I usually discuss relevant papers in my monthly newsletter.
And if useful, I have a section on how I stay up to date in issue No 2 here: https://magazine.sebastianraschka.com/p/ahead-of-ai-2-transformers-fast-and…
------
You love using PyTorch for Deep Learning but want it a bit more organized, so it's easier to take advantage of more advanced features?

Great news: Unit 5 is finally live! In Unit 5, I'll show you how to train PyTorch models with the Lightning Trainer!

 https://lightning.ai/pages/courses/deep-learning-fundamentals/overview-organizing-your-code-with-pytorch-lightning/…
------
Btw in this unit I am teaching you the exact same approach I use to finetune large language models. 

Using the Trainer with mixed-precision & multi-GPU training lets you train an LLM classifierl in ~3 min* on 50k example target data: https://sebastianraschka.com/blog/2023/pytorch-faster.html…

*if you have 4 GPUs
------
You love using PyTorch for Deep Learning but want it a bit more organized, so it's easier to take advantage of more advanced features?

Great news: Unit 5 is finally live! In Unit 5, I'll show you how to train PyTorch models with the Lightning Trainer!

 https://lightning.ai/pages/courses/deep-learning-fundamentals/overview-organizing-your-code-with-pytorch-lightning/…
------
Some takeaways after reading the GPT-4 paper: https://cdn.openai.com/papers/gpt-4.pdf…

The good:

1)  The model now accepts multimodal inputs: images and text

2) The paper is surprisingly honest: "While less capable than humans in many real-world scenarios...", "GPT-4's capabilities and… Show more
------
You love using PyTorch for Deep Learning but want it a bit more organized, so it's easier to take advantage of more advanced features?

Great news: Unit 5 is finally live! In Unit 5, I'll show you how to train PyTorch models with the Lightning Trainer!

 https://lightning.ai/pages/courses/deep-learning-fundamentals/overview-organizing-your-code-with-pytorch-lightning/…
------
I should note that this is also the foundation for the upcoming ones in the next weeks, where we'll tackle lots of great stuff:

- learning rate schedulers
- mixed precision training
- checkpointing
- multi-GPU training
- neural network debugging

and more!
------
PS: I also cover 
- code reproducibility, 
- logging, 
- and creating custom callbacks 
as a warmup for the next unit on Essential Deep Learning Tips & Tricks using the 
@LightningAI
 Trainer!
------
Machine learning competitions are often a good indicator of what techniques actually work well in practice on new datasets. 
The very comprehensive State of Competitive Machine Learning 2022 report just came out and contained many interesting and https://mlcontests.com/state-of-competitive-machine-learning-2022/…… Show more
------
My two main surprises here were that ViT already halfway caught up with CNNs in vision. And another positive surprise is that k-fold CV is more widely used than I thought. 

Where’s scikit-learn? Is it still relevant? Scikit-learn implements fundamental models, and they are not… Show more
------
Thanks for all the comments on this so far! Very interesting! I am particularly surprised that they are dominated by TF vs PyTorch discussions rather than DL vs gradient boosting for tabular data, or cross-validation vs fixed validation sets.
------
Just see there's also another nice summary thread via 
@ml_contests
 here:
------
 The State of Competitive Machine Learning in 2022 
-  Analysis of 200+ ML competitions and 67 winning solutions from 2022 
-  Winners mostly use a common toolkit - Python, PyData, PyTorch, and a few other key tools.
Full report here: https://mlcontests.com/state-of-competitive-machine-learning-2022?ref=mlctwitter…
 (1/n)
------
Just took the new Lion optimizer (https://arxiv.org/abs/2302.06675) for a spin, and I am positively surprised.

With a bit of tinkering, I got it to perform similar to Adam for finetuning DistilBERT (never had any luck with SGD on that model). 

Code here for ref: https://github.com/rasbt/try-lion-optimizer…
------
I should say that I didn't notice any speedup though. All methods were around 4.3 min on avg with the same hardware setup.
------
Just saw that the final 
@PyTorch
 2.0 release candidate is out now! 

CPU: 
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/test/cpu

CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/test/cu118

ROCm 5.4
pip https://download.pytorch.org/whl/test/rocm5.4/……
------
Ahead of AI #6: TrAIn Differently is out!
https://magazine.sebastianraschka.com/p/ahead-of-ai-6-train-differently?sd=pf…

This issue covers alternative training techniques for large language models and reinforcement learning with human feedback.

Also, I have an extensive section on how to read a research paper!

Happy reading!
------
Now, while RLHF seems to be a very popular way for finetune the latest LLMs, it's of course not the only promising solution moving forward.

In the recent "The Wisdom of Hindsight makes Language Models Better Instruction Followers"(https://arxiv.org/abs/2302.05206), researchers propose a…
------
Why do latest language transformers (LLMs like ChatGPT etc.) use reinforcement learning (RL) for finetuning instead of regular supervised learning (SL)?

There are at least 5 reasons ...
[1/10]
------
We need more nuanced discussions around the risk of open sourcing models. 

Open source brings valuable access, but it is absurd to ignore the fact that it lowers the barriers to entry for both useful use cases and potential misuse.
------
I love Pandas! I've been using it ever since I started doing machine learning more than a decade ago. 

Now that the Pandas 2.0 release candidate is out, I was just taking the new PyArrow backend for a test drive. It's a significant boost over the original https://github.com/rasbt/machine-learning-notes/blob/main/benchmark/pandas-pyarrow/pandas2-pyarrow.ipynb……
------
Pro tip: Instead of using the pyarrow backend explicitely on very DataFrame via 

df = pd.DataFrame(numbers, dtype="float64[pyarrow]")

you can enable it globally via 

pd.options.mode.dtype_backend = "pyarrow"
------
When using automatic mixed-precision training to accelerate model training, there are two common options: float16 and bfloat16 (16-bit "brain" floating points). What's the difference?

Compared to float16, bfloat16 has the same dynamic range as float32 but lower precision.

In my…
------
Productive week! We recorded 5 new units of Deep Learning Fundamentals! And the new Units will start dropping mid-March: https://lightning.ai/pages/courses/deep-learning-fundamentals/…

We'll cover code organization, computer vision, transformers, and performance tricks (mixed precision, multi-GPU paradigms & more!)
------
Are transformers truly more easy to parallelize than recurrent neural networks? Yes and no. For encoder-style models like BERT, this is of course true. But in recent months, transformer has become synonymous to GPT (a decoder-style model). 

Sure, we can parallelize each…
------
Why do latest language transformers (LLMs like ChatGPT etc.) use reinforcement learning (RL) for finetuning instead of regular supervised learning (SL)?

There are at least 5 reasons ...
[1/10]
------
Empirically, RLHF tends to perform better than SL. SL uses a token-level loss (that can be summed or averaged over the text passage), RL is taking the entire text passage, as a whole, into account.
[9/10]
------
Reason 5). It's not either SL or RLHF; InstructGPT & ChatGPT use both! The combination is key. ChatGPT / the InstructGPT paper (https://arxiv.org/abs/2203.02155) first finetunes the model via SL and then further updates it via RL. 
[10/10]
------
Some takeaways after reading the GPT-4 paper: https://cdn.openai.com/papers/gpt-4.pdf…

The good:

1)  The model now accepts multimodal inputs: images and text

2) The paper is surprisingly honest: "While less capable than humans in many real-world scenarios...", "GPT-4's capabilities and… Show more
------
You love using PyTorch for Deep Learning but want it a bit more organized, so it's easier to take advantage of more advanced features?

Great news: Unit 5 is finally live! In Unit 5, I'll show you how to train PyTorch models with the Lightning Trainer!

 https://lightning.ai/pages/courses/deep-learning-fundamentals/overview-organizing-your-code-with-pytorch-lightning/…
------
I should note that this is also the foundation for the upcoming ones in the next weeks, where we'll tackle lots of great stuff:

- learning rate schedulers
- mixed precision training
- checkpointing
- multi-GPU training
- neural network debugging

and more!
------
PS: I also cover 
- code reproducibility, 
- logging, 
- and creating custom callbacks 
as a warmup for the next unit on Essential Deep Learning Tips & Tricks using the 
@LightningAI
 Trainer!
------
Machine learning competitions are often a good indicator of what techniques actually work well in practice on new datasets. 
The very comprehensive State of Competitive Machine Learning 2022 report just came out and contained many interesting and https://mlcontests.com/state-of-competitive-machine-learning-2022/…… Show more
------
My two main surprises here were that ViT already halfway caught up with CNNs in vision. And another positive surprise is that k-fold CV is more widely used than I thought. 

Where’s scikit-learn? Is it still relevant? Scikit-learn implements fundamental models, and they are not… Show more
------
Thanks for all the comments on this so far! Very interesting! I am particularly surprised that they are dominated by TF vs PyTorch discussions rather than DL vs gradient boosting for tabular data, or cross-validation vs fixed validation sets.
------
Just see there's also another nice summary thread via 
@ml_contests
 here:
------
 The State of Competitive Machine Learning in 2022 
-  Analysis of 200+ ML competitions and 67 winning solutions from 2022 
-  Winners mostly use a common toolkit - Python, PyData, PyTorch, and a few other key tools.
Full report here: https://mlcontests.com/state-of-competitive-machine-learning-2022?ref=mlctwitter…
 (1/n)
------
Just took the new Lion optimizer (https://arxiv.org/abs/2302.06675) for a spin, and I am positively surprised.

With a bit of tinkering, I got it to perform similar to Adam for finetuning DistilBERT (never had any luck with SGD on that model). 

Code here for ref: https://github.com/rasbt/try-lion-optimizer…
------
I should say that I didn't notice any speedup though. All methods were around 4.3 min on avg with the same hardware setup.
------
Just saw that the final 
@PyTorch
 2.0 release candidate is out now! 

CPU: 
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/test/cpu

CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/test/cu118

ROCm 5.4
pip https://download.pytorch.org/whl/test/rocm5.4/……
------
Ahead of AI #6: TrAIn Differently is out!
https://magazine.sebastianraschka.com/p/ahead-of-ai-6-train-differently?sd=pf…

This issue covers alternative training techniques for large language models and reinforcement learning with human feedback.

Also, I have an extensive section on how to read a research paper!

Happy reading!
------
Now, while RLHF seems to be a very popular way for finetune the latest LLMs, it's of course not the only promising solution moving forward.

In the recent "The Wisdom of Hindsight makes Language Models Better Instruction Followers"(https://arxiv.org/abs/2302.05206), researchers propose a…
------
Why do latest language transformers (LLMs like ChatGPT etc.) use reinforcement learning (RL) for finetuning instead of regular supervised learning (SL)?

There are at least 5 reasons ...
[1/10]
------
We need more nuanced discussions around the risk of open sourcing models. 

Open source brings valuable access, but it is absurd to ignore the fact that it lowers the barriers to entry for both useful use cases and potential misuse.
------
I love Pandas! I've been using it ever since I started doing machine learning more than a decade ago. 

Now that the Pandas 2.0 release candidate is out, I was just taking the new PyArrow backend for a test drive. It's a significant boost over the original https://github.com/rasbt/machine-learning-notes/blob/main/benchmark/pandas-pyarrow/pandas2-pyarrow.ipynb……
------
Pro tip: Instead of using the pyarrow backend explicitely on very DataFrame via 

df = pd.DataFrame(numbers, dtype="float64[pyarrow]")

you can enable it globally via 

pd.options.mode.dtype_backend = "pyarrow"
------
When using automatic mixed-precision training to accelerate model training, there are two common options: float16 and bfloat16 (16-bit "brain" floating points). What's the difference?

Compared to float16, bfloat16 has the same dynamic range as float32 but lower precision.

In my…
------
Productive week! We recorded 5 new units of Deep Learning Fundamentals! And the new Units will start dropping mid-March: https://lightning.ai/pages/courses/deep-learning-fundamentals/…

We'll cover code organization, computer vision, transformers, and performance tricks (mixed precision, multi-GPU paradigms & more!)
------
Are transformers truly more easy to parallelize than recurrent neural networks? Yes and no. For encoder-style models like BERT, this is of course true. But in recent months, transformer has become synonymous to GPT (a decoder-style model). 

Sure, we can parallelize each…
------
Why do latest language transformers (LLMs like ChatGPT etc.) use reinforcement learning (RL) for finetuning instead of regular supervised learning (SL)?

There are at least 5 reasons ...
[1/10]
------
Empirically, RLHF tends to perform better than SL. SL uses a token-level loss (that can be summed or averaged over the text passage), RL is taking the entire text passage, as a whole, into account.
[9/10]
------
Reason 5). It's not either SL or RLHF; InstructGPT & ChatGPT use both! The combination is key. ChatGPT / the InstructGPT paper (https://arxiv.org/abs/2203.02155) first finetunes the model via SL and then further updates it via RL. 
[10/10]
------
Reason 4). Now, it's not impossible to train the model with SL. In fact, that's been done in the "Learning to Summarize from Human Feedback (2022)" paper. It just doesn't perform that well compared to RL with human feedback.
[8/10]
------
Here we go!

Microsoft introduces a multimodal large language model called Kosmos-1.

Achieves great performance on language understanding, OCR-free NLP, perception-language tasks, visual QA, and more.
------
"Choose the evaluation metric before making modeling choices. Don’t use metrics like AIC or BIC that are based on model assumptions." 
-- wise words from 
@ChristophMolnar
! 
(via https://mindfulmodeler.substack.com/p/the-way-of-model-agnostic-machine…)
------
A question that often comes up when introducing colleagues to the attention mechanism: how are attention scores different from weights in a fully-connected layer?

Conceptually, attention mechanisms allow  transformers to attend to different parts of a sequence or image. On the…
------
*this also extends to convolutional layers. E.g., for simplicity, think of cases where fully-connected and convolutional layers are equivalent:
------
Fun fact: there are not one, but (at least) two ways we convert a fully connected layer into a convolutional layer:

1) making the kernel size similar to the input (left image)
2) stacking 1x1 pixel channels (right image)

(My verbose explanation here: https://youtube.com/watch?v=rqLjZ8k4va8…)
------
Takeaways from reading the "LLaMa: Open and Efficient Foundation Language Models" paper that made big waves yesterday. 
It's a laudable open-source effort making large language models available* for research purposes, but there are also some missed research opportunities here
1/8
------
Just see there's also another nice summary thread via 
@ml_contests
 here:
------
 The State of Competitive Machine Learning in 2022 
-  Analysis of 200+ ML competitions and 67 winning solutions from 2022 
-  Winners mostly use a common toolkit - Python, PyData, PyTorch, and a few other key tools.
Full report here: https://mlcontests.com/state-of-competitive-machine-learning-2022?ref=mlctwitter…
 (1/n)
------
Just took the new Lion optimizer (https://arxiv.org/abs/2302.06675) for a spin, and I am positively surprised.

With a bit of tinkering, I got it to perform similar to Adam for finetuning DistilBERT (never had any luck with SGD on that model). 

Code here for ref: https://github.com/rasbt/try-lion-optimizer…
------
I should say that I didn't notice any speedup though. All methods were around 4.3 min on avg with the same hardware setup.
------
Just saw that the final 
@PyTorch
 2.0 release candidate is out now! 

CPU: 
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/test/cpu

CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/test/cu118

ROCm 5.4
pip https://download.pytorch.org/whl/test/rocm5.4/……
------
Ahead of AI #6: TrAIn Differently is out!
https://magazine.sebastianraschka.com/p/ahead-of-ai-6-train-differently?sd=pf…

This issue covers alternative training techniques for large language models and reinforcement learning with human feedback.

Also, I have an extensive section on how to read a research paper!

Happy reading!
------
Now, while RLHF seems to be a very popular way for finetune the latest LLMs, it's of course not the only promising solution moving forward.

In the recent "The Wisdom of Hindsight makes Language Models Better Instruction Followers"(https://arxiv.org/abs/2302.05206), researchers propose a…
------
Why do latest language transformers (LLMs like ChatGPT etc.) use reinforcement learning (RL) for finetuning instead of regular supervised learning (SL)?

There are at least 5 reasons ...
[1/10]
------
We need more nuanced discussions around the risk of open sourcing models. 

Open source brings valuable access, but it is absurd to ignore the fact that it lowers the barriers to entry for both useful use cases and potential misuse.
------
I love Pandas! I've been using it ever since I started doing machine learning more than a decade ago. 

Now that the Pandas 2.0 release candidate is out, I was just taking the new PyArrow backend for a test drive. It's a significant boost over the original https://github.com/rasbt/machine-learning-notes/blob/main/benchmark/pandas-pyarrow/pandas2-pyarrow.ipynb……
------
Pro tip: Instead of using the pyarrow backend explicitely on very DataFrame via 

df = pd.DataFrame(numbers, dtype="float64[pyarrow]")

you can enable it globally via 

pd.options.mode.dtype_backend = "pyarrow"
------
When using automatic mixed-precision training to accelerate model training, there are two common options: float16 and bfloat16 (16-bit "brain" floating points). What's the difference?

Compared to float16, bfloat16 has the same dynamic range as float32 but lower precision.

In my…
------
Productive week! We recorded 5 new units of Deep Learning Fundamentals! And the new Units will start dropping mid-March: https://lightning.ai/pages/courses/deep-learning-fundamentals/…

We'll cover code organization, computer vision, transformers, and performance tricks (mixed precision, multi-GPU paradigms & more!)
------
Are transformers truly more easy to parallelize than recurrent neural networks? Yes and no. For encoder-style models like BERT, this is of course true. But in recent months, transformer has become synonymous to GPT (a decoder-style model). 

Sure, we can parallelize each…
------
Why do latest language transformers (LLMs like ChatGPT etc.) use reinforcement learning (RL) for finetuning instead of regular supervised learning (SL)?

There are at least 5 reasons ...
[1/10]
------
Empirically, RLHF tends to perform better than SL. SL uses a token-level loss (that can be summed or averaged over the text passage), RL is taking the entire text passage, as a whole, into account.
[9/10]
------
Reason 5). It's not either SL or RLHF; InstructGPT & ChatGPT use both! The combination is key. ChatGPT / the InstructGPT paper (https://arxiv.org/abs/2203.02155) first finetunes the model via SL and then further updates it via RL. 
[10/10]
------
Reason 4). Now, it's not impossible to train the model with SL. In fact, that's been done in the "Learning to Summarize from Human Feedback (2022)" paper. It just doesn't perform that well compared to RL with human feedback.
[8/10]
------
Here we go!

Microsoft introduces a multimodal large language model called Kosmos-1.

Achieves great performance on language understanding, OCR-free NLP, perception-language tasks, visual QA, and more.
------
"Choose the evaluation metric before making modeling choices. Don’t use metrics like AIC or BIC that are based on model assumptions." 
-- wise words from 
@ChristophMolnar
! 
(via https://mindfulmodeler.substack.com/p/the-way-of-model-agnostic-machine…)
------
A question that often comes up when introducing colleagues to the attention mechanism: how are attention scores different from weights in a fully-connected layer?

Conceptually, attention mechanisms allow  transformers to attend to different parts of a sequence or image. On the…
------
*this also extends to convolutional layers. E.g., for simplicity, think of cases where fully-connected and convolutional layers are equivalent:
------
Fun fact: there are not one, but (at least) two ways we convert a fully connected layer into a convolutional layer:

1) making the kernel size similar to the input (left image)
2) stacking 1x1 pixel channels (right image)

(My verbose explanation here: https://youtube.com/watch?v=rqLjZ8k4va8…)
------
Takeaways from reading the "LLaMa: Open and Efficient Foundation Language Models" paper that made big waves yesterday. 
It's a laudable open-source effort making large language models available* for research purposes, but there are also some missed research opportunities here
1/8
------
*Now let's get to the asterisk of the first tweet. The model repo is available under a GNU GPL v3.0 license on GitHub here: https://github.com/facebookresearch/llama….
It contains the code only. The weights are available upon filing a request form. 
7/8
------
While I think it's fair (no pun intended), it should be mentioned that this comes with a pretty hefty restriction: 
"The license prohibits using the models or any data produced by the models for any type of commercial or production purpose."
8/8
------
Ahead of AI #6: TrAIn Differently is out!
https://magazine.sebastianraschka.com/p/ahead-of-ai-6-train-differently?sd=pf…

This issue covers alternative training techniques for large language models and reinforcement learning with human feedback.

Also, I have an extensive section on how to read a research paper!

Happy reading!
------
Now, while RLHF seems to be a very popular way for finetune the latest LLMs, it's of course not the only promising solution moving forward.

In the recent "The Wisdom of Hindsight makes Language Models Better Instruction Followers"(https://arxiv.org/abs/2302.05206), researchers propose a…
------
Why do latest language transformers (LLMs like ChatGPT etc.) use reinforcement learning (RL) for finetuning instead of regular supervised learning (SL)?

There are at least 5 reasons ...
[1/10]
------
We need more nuanced discussions around the risk of open sourcing models. 

Open source brings valuable access, but it is absurd to ignore the fact that it lowers the barriers to entry for both useful use cases and potential misuse.
------
I love Pandas! I've been using it ever since I started doing machine learning more than a decade ago. 

Now that the Pandas 2.0 release candidate is out, I was just taking the new PyArrow backend for a test drive. It's a significant boost over the original https://github.com/rasbt/machine-learning-notes/blob/main/benchmark/pandas-pyarrow/pandas2-pyarrow.ipynb……
------
Pro tip: Instead of using the pyarrow backend explicitely on very DataFrame via 

df = pd.DataFrame(numbers, dtype="float64[pyarrow]")

you can enable it globally via 

pd.options.mode.dtype_backend = "pyarrow"
------
When using automatic mixed-precision training to accelerate model training, there are two common options: float16 and bfloat16 (16-bit "brain" floating points). What's the difference?

Compared to float16, bfloat16 has the same dynamic range as float32 but lower precision.

In my…
------
Productive week! We recorded 5 new units of Deep Learning Fundamentals! And the new Units will start dropping mid-March: https://lightning.ai/pages/courses/deep-learning-fundamentals/…

We'll cover code organization, computer vision, transformers, and performance tricks (mixed precision, multi-GPU paradigms & more!)
------
Are transformers truly more easy to parallelize than recurrent neural networks? Yes and no. For encoder-style models like BERT, this is of course true. But in recent months, transformer has become synonymous to GPT (a decoder-style model). 

Sure, we can parallelize each…
------
Why do latest language transformers (LLMs like ChatGPT etc.) use reinforcement learning (RL) for finetuning instead of regular supervised learning (SL)?

There are at least 5 reasons ...
[1/10]
------
Empirically, RLHF tends to perform better than SL. SL uses a token-level loss (that can be summed or averaged over the text passage), RL is taking the entire text passage, as a whole, into account.
[9/10]
------
Reason 5). It's not either SL or RLHF; InstructGPT & ChatGPT use both! The combination is key. ChatGPT / the InstructGPT paper (https://arxiv.org/abs/2203.02155) first finetunes the model via SL and then further updates it via RL. 
[10/10]
------
Reason 4). Now, it's not impossible to train the model with SL. In fact, that's been done in the "Learning to Summarize from Human Feedback (2022)" paper. It just doesn't perform that well compared to RL with human feedback.
[8/10]
------
Here we go!

Microsoft introduces a multimodal large language model called Kosmos-1.

Achieves great performance on language understanding, OCR-free NLP, perception-language tasks, visual QA, and more.
------
"Choose the evaluation metric before making modeling choices. Don’t use metrics like AIC or BIC that are based on model assumptions." 
-- wise words from 
@ChristophMolnar
! 
(via https://mindfulmodeler.substack.com/p/the-way-of-model-agnostic-machine…)
------
A question that often comes up when introducing colleagues to the attention mechanism: how are attention scores different from weights in a fully-connected layer?

Conceptually, attention mechanisms allow  transformers to attend to different parts of a sequence or image. On the…
------
*this also extends to convolutional layers. E.g., for simplicity, think of cases where fully-connected and convolutional layers are equivalent:
------
Fun fact: there are not one, but (at least) two ways we convert a fully connected layer into a convolutional layer:

1) making the kernel size similar to the input (left image)
2) stacking 1x1 pixel channels (right image)

(My verbose explanation here: https://youtube.com/watch?v=rqLjZ8k4va8…)
------
Takeaways from reading the "LLaMa: Open and Efficient Foundation Language Models" paper that made big waves yesterday. 
It's a laudable open-source effort making large language models available* for research purposes, but there are also some missed research opportunities here
1/8
------
*Now let's get to the asterisk of the first tweet. The model repo is available under a GNU GPL v3.0 license on GitHub here: https://github.com/facebookresearch/llama….
It contains the code only. The weights are available upon filing a request form. 
7/8
------
While I think it's fair (no pun intended), it should be mentioned that this comes with a pretty hefty restriction: 
"The license prohibits using the models or any data produced by the models for any type of commercial or production purpose."
8/8
------
Moreover, from reading the research paper, it's not clear what the architecture looks like exactly. The referenced "Attention is all you need" architecture is an encoder-decoder architecture. GPT-3 that they compare themselves to is a decoder-only architecture.
6/8
------
In papers, we typically report confidence intervals around the prediction accuracy.

In praxis, we use conformal predictions to capture the uncertainty of the model prediction. For classifiers, this is a set of labels such that the set contains labels with a given confidence.
------
For more info about different methods to construct confidence intervals for machine learning classifiers, I have an article here: https://sebastianraschka.com/blog/2022/confidence-intervals-for-ml.html…

For more info about conformal predictions, check out this treasure trove by 
@predict_addict
------
When people read my confidence intervals for ML article, I often get the feedback to use conformal predictions. 

I don't disagree, but I think that's just a different use case or goal.
------
Very intuitive article on self-attention and how to code it ⁦
@rasbt
⁩
------
Wrote a new blog post outlining techniques for improving the training performance of your PyTorch model without compromising its accuracy.
By changing only a few lines of code, we are going from 22.63 minutes to 3.15 minutes when finetuning BERT:
------
When using automatic mixed-precision training to accelerate model training, there are two common options: float16 and bfloat16 (16-bit "brain" floating points). What's the difference?

Compared to float16, bfloat16 has the same dynamic range as float32 but lower precision.

In my…
------
Productive week! We recorded 5 new units of Deep Learning Fundamentals! And the new Units will start dropping mid-March: https://lightning.ai/pages/courses/deep-learning-fundamentals/…

We'll cover code organization, computer vision, transformers, and performance tricks (mixed precision, multi-GPU paradigms & more!)
------
Are transformers truly more easy to parallelize than recurrent neural networks? Yes and no. For encoder-style models like BERT, this is of course true. But in recent months, transformer has become synonymous to GPT (a decoder-style model). 

Sure, we can parallelize each…
------
Why do latest language transformers (LLMs like ChatGPT etc.) use reinforcement learning (RL) for finetuning instead of regular supervised learning (SL)?

There are at least 5 reasons ...
[1/10]
------
Empirically, RLHF tends to perform better than SL. SL uses a token-level loss (that can be summed or averaged over the text passage), RL is taking the entire text passage, as a whole, into account.
[9/10]
------
Reason 5). It's not either SL or RLHF; InstructGPT & ChatGPT use both! The combination is key. ChatGPT / the InstructGPT paper (https://arxiv.org/abs/2203.02155) first finetunes the model via SL and then further updates it via RL. 
[10/10]
------
Reason 4). Now, it's not impossible to train the model with SL. In fact, that's been done in the "Learning to Summarize from Human Feedback (2022)" paper. It just doesn't perform that well compared to RL with human feedback.
[8/10]
------
Here we go!

Microsoft introduces a multimodal large language model called Kosmos-1.

Achieves great performance on language understanding, OCR-free NLP, perception-language tasks, visual QA, and more.
------
"Choose the evaluation metric before making modeling choices. Don’t use metrics like AIC or BIC that are based on model assumptions." 
-- wise words from 
@ChristophMolnar
! 
(via https://mindfulmodeler.substack.com/p/the-way-of-model-agnostic-machine…)
------
A question that often comes up when introducing colleagues to the attention mechanism: how are attention scores different from weights in a fully-connected layer?

Conceptually, attention mechanisms allow  transformers to attend to different parts of a sequence or image. On the…
------
*this also extends to convolutional layers. E.g., for simplicity, think of cases where fully-connected and convolutional layers are equivalent:
------
Fun fact: there are not one, but (at least) two ways we convert a fully connected layer into a convolutional layer:

1) making the kernel size similar to the input (left image)
2) stacking 1x1 pixel channels (right image)

(My verbose explanation here: https://youtube.com/watch?v=rqLjZ8k4va8…)
------
Takeaways from reading the "LLaMa: Open and Efficient Foundation Language Models" paper that made big waves yesterday. 
It's a laudable open-source effort making large language models available* for research purposes, but there are also some missed research opportunities here
1/8
------
*Now let's get to the asterisk of the first tweet. The model repo is available under a GNU GPL v3.0 license on GitHub here: https://github.com/facebookresearch/llama….
It contains the code only. The weights are available upon filing a request form. 
7/8
------
While I think it's fair (no pun intended), it should be mentioned that this comes with a pretty hefty restriction: 
"The license prohibits using the models or any data produced by the models for any type of commercial or production purpose."
8/8
------
Moreover, from reading the research paper, it's not clear what the architecture looks like exactly. The referenced "Attention is all you need" architecture is an encoder-decoder architecture. GPT-3 that they compare themselves to is a decoder-only architecture.
6/8
------
In papers, we typically report confidence intervals around the prediction accuracy.

In praxis, we use conformal predictions to capture the uncertainty of the model prediction. For classifiers, this is a set of labels such that the set contains labels with a given confidence.
------
For more info about different methods to construct confidence intervals for machine learning classifiers, I have an article here: https://sebastianraschka.com/blog/2022/confidence-intervals-for-ml.html…

For more info about conformal predictions, check out this treasure trove by 
@predict_addict
------
When people read my confidence intervals for ML article, I often get the feedback to use conformal predictions. 

I don't disagree, but I think that's just a different use case or goal.
------
Very intuitive article on self-attention and how to code it ⁦
@rasbt
⁩
------
Wrote a new blog post outlining techniques for improving the training performance of your PyTorch model without compromising its accuracy.
By changing only a few lines of code, we are going from 22.63 minutes to 3.15 minutes when finetuning BERT:
------
And that's 2 min (an 11.5x performance boost) if you have 8 GPUs to spare 
------
Adding these two new books to my collection - 1. "#Mathematics for #MachineLearning" by Mark Peter Deisenroth + A. Aldo Faisal 
@AnalogAldo
 + Cheng Soon Ong 
@ChengSoonOng
 and 2. "#Python Machine Learning" by Sebastian Raschka 
@rasbt
 + Vahid Mirjalili
------
The Pandas 2.0 release candidate is out! https://pandas.pydata.org/docs/dev/whatsnew/v2.0.0.html…
Pandas is one of those libs since I started using Python as a grad student. I've been using it ever since. 
Sure, there's polars now, but for most contexts, pandas is a sufficient go-to for my interactive data prep.
------
Can deep transformers be trained without skip connections nor normalisation layers?

Our ICLR 2023 paper shows you how, using wide NN signal propagation ideas. We hope this can potentially pave the way to more efficient deep LLMs! (1/9)

Paper: https://arxiv.org/abs/2302.10322
------
When deciding upon an appropriate value of *k* for k-fold cross-validation, we are often guided by computational performance and conventions. 

However, it's worthwhile to define k based on the purpose and context when we are using k-fold cross-validation. 

1) If we care mostly…
------
Empirically, RLHF tends to perform better than SL. SL uses a token-level loss (that can be summed or averaged over the text passage), RL is taking the entire text passage, as a whole, into account.
[9/10]
------
Reason 5). It's not either SL or RLHF; InstructGPT & ChatGPT use both! The combination is key. ChatGPT / the InstructGPT paper (https://arxiv.org/abs/2203.02155) first finetunes the model via SL and then further updates it via RL. 
[10/10]
------
Reason 4). Now, it's not impossible to train the model with SL. In fact, that's been done in the "Learning to Summarize from Human Feedback (2022)" paper. It just doesn't perform that well compared to RL with human feedback.
[8/10]
------
Here we go!

Microsoft introduces a multimodal large language model called Kosmos-1.

Achieves great performance on language understanding, OCR-free NLP, perception-language tasks, visual QA, and more.
------
"Choose the evaluation metric before making modeling choices. Don’t use metrics like AIC or BIC that are based on model assumptions." 
-- wise words from 
@ChristophMolnar
! 
(via https://mindfulmodeler.substack.com/p/the-way-of-model-agnostic-machine…)
------
A question that often comes up when introducing colleagues to the attention mechanism: how are attention scores different from weights in a fully-connected layer?

Conceptually, attention mechanisms allow  transformers to attend to different parts of a sequence or image. On the…
------
*this also extends to convolutional layers. E.g., for simplicity, think of cases where fully-connected and convolutional layers are equivalent:
------
Fun fact: there are not one, but (at least) two ways we convert a fully connected layer into a convolutional layer:

1) making the kernel size similar to the input (left image)
2) stacking 1x1 pixel channels (right image)

(My verbose explanation here: https://youtube.com/watch?v=rqLjZ8k4va8…)
------
Takeaways from reading the "LLaMa: Open and Efficient Foundation Language Models" paper that made big waves yesterday. 
It's a laudable open-source effort making large language models available* for research purposes, but there are also some missed research opportunities here
1/8
------
*Now let's get to the asterisk of the first tweet. The model repo is available under a GNU GPL v3.0 license on GitHub here: https://github.com/facebookresearch/llama….
It contains the code only. The weights are available upon filing a request form. 
7/8
------
While I think it's fair (no pun intended), it should be mentioned that this comes with a pretty hefty restriction: 
"The license prohibits using the models or any data produced by the models for any type of commercial or production purpose."
8/8
------
Moreover, from reading the research paper, it's not clear what the architecture looks like exactly. The referenced "Attention is all you need" architecture is an encoder-decoder architecture. GPT-3 that they compare themselves to is a decoder-only architecture.
6/8
------
In papers, we typically report confidence intervals around the prediction accuracy.

In praxis, we use conformal predictions to capture the uncertainty of the model prediction. For classifiers, this is a set of labels such that the set contains labels with a given confidence.
------
For more info about different methods to construct confidence intervals for machine learning classifiers, I have an article here: https://sebastianraschka.com/blog/2022/confidence-intervals-for-ml.html…

For more info about conformal predictions, check out this treasure trove by 
@predict_addict
------
When people read my confidence intervals for ML article, I often get the feedback to use conformal predictions. 

I don't disagree, but I think that's just a different use case or goal.
------
Very intuitive article on self-attention and how to code it ⁦
@rasbt
⁩
------
Wrote a new blog post outlining techniques for improving the training performance of your PyTorch model without compromising its accuracy.
By changing only a few lines of code, we are going from 22.63 minutes to 3.15 minutes when finetuning BERT:
------
And that's 2 min (an 11.5x performance boost) if you have 8 GPUs to spare 
------
Adding these two new books to my collection - 1. "#Mathematics for #MachineLearning" by Mark Peter Deisenroth + A. Aldo Faisal 
@AnalogAldo
 + Cheng Soon Ong 
@ChengSoonOng
 and 2. "#Python Machine Learning" by Sebastian Raschka 
@rasbt
 + Vahid Mirjalili
------
The Pandas 2.0 release candidate is out! https://pandas.pydata.org/docs/dev/whatsnew/v2.0.0.html…
Pandas is one of those libs since I started using Python as a grad student. I've been using it ever since. 
Sure, there's polars now, but for most contexts, pandas is a sufficient go-to for my interactive data prep.
------
Can deep transformers be trained without skip connections nor normalisation layers?

Our ICLR 2023 paper shows you how, using wide NN signal propagation ideas. We hope this can potentially pave the way to more efficient deep LLMs! (1/9)

Paper: https://arxiv.org/abs/2302.10322
------
When deciding upon an appropriate value of *k* for k-fold cross-validation, we are often guided by computational performance and conventions. 

However, it's worthwhile to define k based on the purpose and context when we are using k-fold cross-validation. 

1) If we care mostly…
------
Thank you! This is the gist of it, see also the blog https://windowsontheory.org/2023/02/21/provable-copyright-protection-for-generative-models/…

Point is that the combined model can use shared parts of distribution but would not output anything too similar to a training point since it’s guaranteed to be close to model that didn’t see it .
------
Whether it's okay to train generative models on copyrighted material is still up for debate. However, beyond the fair-use discussion, there's this related issue whether the *generated outputs* violate copyrights. This is an issue because generative models https://arxiv.org/abs/2302.10870…
------
https://arxiv.org/abs/2301.11108
A paper on the mathematics of diffusion models that explains the diffusion SDEs --- both forward and backward --- assuming only familiarity with Gaussians. It also gives some original non-variational likelihood formulas.
------
"Choose the evaluation metric before making modeling choices. Don’t use metrics like AIC or BIC that are based on model assumptions." 
-- wise words from 
@ChristophMolnar
! 
(via https://mindfulmodeler.substack.com/p/the-way-of-model-agnostic-machine…)
------
A question that often comes up when introducing colleagues to the attention mechanism: how are attention scores different from weights in a fully-connected layer?

Conceptually, attention mechanisms allow  transformers to attend to different parts of a sequence or image. On the…
------
*this also extends to convolutional layers. E.g., for simplicity, think of cases where fully-connected and convolutional layers are equivalent:
------
Fun fact: there are not one, but (at least) two ways we convert a fully connected layer into a convolutional layer:

1) making the kernel size similar to the input (left image)
2) stacking 1x1 pixel channels (right image)

(My verbose explanation here: https://youtube.com/watch?v=rqLjZ8k4va8…)
------
Takeaways from reading the "LLaMa: Open and Efficient Foundation Language Models" paper that made big waves yesterday. 
It's a laudable open-source effort making large language models available* for research purposes, but there are also some missed research opportunities here
1/8
------
*Now let's get to the asterisk of the first tweet. The model repo is available under a GNU GPL v3.0 license on GitHub here: https://github.com/facebookresearch/llama….
It contains the code only. The weights are available upon filing a request form. 
7/8
------
While I think it's fair (no pun intended), it should be mentioned that this comes with a pretty hefty restriction: 
"The license prohibits using the models or any data produced by the models for any type of commercial or production purpose."
8/8
------
Moreover, from reading the research paper, it's not clear what the architecture looks like exactly. The referenced "Attention is all you need" architecture is an encoder-decoder architecture. GPT-3 that they compare themselves to is a decoder-only architecture.
6/8
------
In papers, we typically report confidence intervals around the prediction accuracy.

In praxis, we use conformal predictions to capture the uncertainty of the model prediction. For classifiers, this is a set of labels such that the set contains labels with a given confidence.
------
For more info about different methods to construct confidence intervals for machine learning classifiers, I have an article here: https://sebastianraschka.com/blog/2022/confidence-intervals-for-ml.html…

For more info about conformal predictions, check out this treasure trove by 
@predict_addict
------
When people read my confidence intervals for ML article, I often get the feedback to use conformal predictions. 

I don't disagree, but I think that's just a different use case or goal.
------
Very intuitive article on self-attention and how to code it ⁦
@rasbt
⁩
------
Wrote a new blog post outlining techniques for improving the training performance of your PyTorch model without compromising its accuracy.
By changing only a few lines of code, we are going from 22.63 minutes to 3.15 minutes when finetuning BERT:
------
And that's 2 min (an 11.5x performance boost) if you have 8 GPUs to spare 
------
Adding these two new books to my collection - 1. "#Mathematics for #MachineLearning" by Mark Peter Deisenroth + A. Aldo Faisal 
@AnalogAldo
 + Cheng Soon Ong 
@ChengSoonOng
 and 2. "#Python Machine Learning" by Sebastian Raschka 
@rasbt
 + Vahid Mirjalili
------
The Pandas 2.0 release candidate is out! https://pandas.pydata.org/docs/dev/whatsnew/v2.0.0.html…
Pandas is one of those libs since I started using Python as a grad student. I've been using it ever since. 
Sure, there's polars now, but for most contexts, pandas is a sufficient go-to for my interactive data prep.
------
Can deep transformers be trained without skip connections nor normalisation layers?

Our ICLR 2023 paper shows you how, using wide NN signal propagation ideas. We hope this can potentially pave the way to more efficient deep LLMs! (1/9)

Paper: https://arxiv.org/abs/2302.10322
------
When deciding upon an appropriate value of *k* for k-fold cross-validation, we are often guided by computational performance and conventions. 

However, it's worthwhile to define k based on the purpose and context when we are using k-fold cross-validation. 

1) If we care mostly…
------
Thank you! This is the gist of it, see also the blog https://windowsontheory.org/2023/02/21/provable-copyright-protection-for-generative-models/…

Point is that the combined model can use shared parts of distribution but would not output anything too similar to a training point since it’s guaranteed to be close to model that didn’t see it .
------
Whether it's okay to train generative models on copyrighted material is still up for debate. However, beyond the fair-use discussion, there's this related issue whether the *generated outputs* violate copyrights. This is an issue because generative models https://arxiv.org/abs/2302.10870…
------
https://arxiv.org/abs/2301.11108
A paper on the mathematics of diffusion models that explains the diffusion SDEs --- both forward and backward --- assuming only familiarity with Gaussians. It also gives some original non-variational likelihood formulas.
------
"MarioGPT: Open-Ended Text2Level Generation through Large Language Models" 
This looks like a pretty fun and creative project!
The best part, it's based on a distilled GPT-3 model and can be trained on a single GPU.
------
In my excitement, I totally forgot to include the Arxiv link: https://arxiv.org/abs/2302.05981v1…

(And I meant a distilled version of GPT-2, which is ~80M parameters. You can run it on a 2080Ti.)
------
If you want to give it a try, I added a self-contained project template for PyTorch models here: https://github.com/rasbt/machine-learning-notes/blob/main/templates/modern-early-stop-with-checkpointing/checkpointing.ipynb…
------
And I'd argue it's always a good idea to consider adopting early stopping by default.
If you are using @PyTorch, that's super easy to implement, so no excuses   twitter.com/vipul_1011/sta…
------
A ~100 pages survey on pretrained foundation models: from BERT to ChatGPT. 

Has a nice structure for reviewing pretrained models in addition to challenges and open problems. 

https://arxiv.org/abs/2302.09419
------
In recent years, Transformers made other methods for text classification and generation obsolete (bag-of-words, 1D CNNs, RNNs, ...). 
Text is essentially sequence data, so let's address the elephant in the room: "Are Transformers Effective for Time Series https://arxiv.org/abs/2205.13504…
------
*Now let's get to the asterisk of the first tweet. The model repo is available under a GNU GPL v3.0 license on GitHub here: https://github.com/facebookresearch/llama….
It contains the code only. The weights are available upon filing a request form. 
7/8
------
While I think it's fair (no pun intended), it should be mentioned that this comes with a pretty hefty restriction: 
"The license prohibits using the models or any data produced by the models for any type of commercial or production purpose."
8/8
------
Moreover, from reading the research paper, it's not clear what the architecture looks like exactly. The referenced "Attention is all you need" architecture is an encoder-decoder architecture. GPT-3 that they compare themselves to is a decoder-only architecture.
6/8
------
In papers, we typically report confidence intervals around the prediction accuracy.

In praxis, we use conformal predictions to capture the uncertainty of the model prediction. For classifiers, this is a set of labels such that the set contains labels with a given confidence.
------
For more info about different methods to construct confidence intervals for machine learning classifiers, I have an article here: https://sebastianraschka.com/blog/2022/confidence-intervals-for-ml.html…

For more info about conformal predictions, check out this treasure trove by 
@predict_addict
------
When people read my confidence intervals for ML article, I often get the feedback to use conformal predictions. 

I don't disagree, but I think that's just a different use case or goal.
------
Very intuitive article on self-attention and how to code it ⁦
@rasbt
⁩
------
Wrote a new blog post outlining techniques for improving the training performance of your PyTorch model without compromising its accuracy.
By changing only a few lines of code, we are going from 22.63 minutes to 3.15 minutes when finetuning BERT:
------
And that's 2 min (an 11.5x performance boost) if you have 8 GPUs to spare 
------
Adding these two new books to my collection - 1. "#Mathematics for #MachineLearning" by Mark Peter Deisenroth + A. Aldo Faisal 
@AnalogAldo
 + Cheng Soon Ong 
@ChengSoonOng
 and 2. "#Python Machine Learning" by Sebastian Raschka 
@rasbt
 + Vahid Mirjalili
------
The Pandas 2.0 release candidate is out! https://pandas.pydata.org/docs/dev/whatsnew/v2.0.0.html…
Pandas is one of those libs since I started using Python as a grad student. I've been using it ever since. 
Sure, there's polars now, but for most contexts, pandas is a sufficient go-to for my interactive data prep.
------
Can deep transformers be trained without skip connections nor normalisation layers?

Our ICLR 2023 paper shows you how, using wide NN signal propagation ideas. We hope this can potentially pave the way to more efficient deep LLMs! (1/9)

Paper: https://arxiv.org/abs/2302.10322
------
When deciding upon an appropriate value of *k* for k-fold cross-validation, we are often guided by computational performance and conventions. 

However, it's worthwhile to define k based on the purpose and context when we are using k-fold cross-validation. 

1) If we care mostly…
------
Thank you! This is the gist of it, see also the blog https://windowsontheory.org/2023/02/21/provable-copyright-protection-for-generative-models/…

Point is that the combined model can use shared parts of distribution but would not output anything too similar to a training point since it’s guaranteed to be close to model that didn’t see it .
------
Whether it's okay to train generative models on copyrighted material is still up for debate. However, beyond the fair-use discussion, there's this related issue whether the *generated outputs* violate copyrights. This is an issue because generative models https://arxiv.org/abs/2302.10870…
------
https://arxiv.org/abs/2301.11108
A paper on the mathematics of diffusion models that explains the diffusion SDEs --- both forward and backward --- assuming only familiarity with Gaussians. It also gives some original non-variational likelihood formulas.
------
"MarioGPT: Open-Ended Text2Level Generation through Large Language Models" 
This looks like a pretty fun and creative project!
The best part, it's based on a distilled GPT-3 model and can be trained on a single GPU.
------
In my excitement, I totally forgot to include the Arxiv link: https://arxiv.org/abs/2302.05981v1…

(And I meant a distilled version of GPT-2, which is ~80M parameters. You can run it on a 2080Ti.)
------
If you want to give it a try, I added a self-contained project template for PyTorch models here: https://github.com/rasbt/machine-learning-notes/blob/main/templates/modern-early-stop-with-checkpointing/checkpointing.ipynb…
------
And I'd argue it's always a good idea to consider adopting early stopping by default.
If you are using @PyTorch, that's super easy to implement, so no excuses   twitter.com/vipul_1011/sta…
------
A ~100 pages survey on pretrained foundation models: from BERT to ChatGPT. 

Has a nice structure for reviewing pretrained models in addition to challenges and open problems. 

https://arxiv.org/abs/2302.09419
------
In recent years, Transformers made other methods for text classification and generation obsolete (bag-of-words, 1D CNNs, RNNs, ...). 
Text is essentially sequence data, so let's address the elephant in the room: "Are Transformers Effective for Time Series https://arxiv.org/abs/2205.13504…
------
LLMs are clearly inspired by JRPGs.
------
Why do LLM services insist on "typing" their responses? Are they trying to replicate the slow, pixelated, nostalgic experience of dial-up internet revealing an image line by line? ...Yet another way in which LLMs are like "blurry JPEGs."
------
Productive weekend! Just added 4 new Q&A's!

- Multi-GPU Training Paradigms
- The Distributional Hypothesis 
- "Self"-Attention
- Training & Test Set Discordance

And "Machine Learning Q and AI" just crossed the 50% milestone! 

PS: the Multi-GPU section https://leanpub.com/machine-learning-q-and-ai/……
------
Here are some resources to understand RLHF (reinforcement learning with human feedback)

(1) "Asynchronous Methods for Deep Reinforcement Learning" (2016, https://arxiv.org/abs/1602.01783). Introduces policy gradients methods for deep learning-based RL as https://arxiv.org/abs/2203.02155…
------
Also just added them the LLM reading list here
------
In papers, we typically report confidence intervals around the prediction accuracy.

In praxis, we use conformal predictions to capture the uncertainty of the model prediction. For classifiers, this is a set of labels such that the set contains labels with a given confidence.
------
For more info about different methods to construct confidence intervals for machine learning classifiers, I have an article here: https://sebastianraschka.com/blog/2022/confidence-intervals-for-ml.html…

For more info about conformal predictions, check out this treasure trove by 
@predict_addict
------
When people read my confidence intervals for ML article, I often get the feedback to use conformal predictions. 

I don't disagree, but I think that's just a different use case or goal.
------
Very intuitive article on self-attention and how to code it ⁦
@rasbt
⁩
------
Wrote a new blog post outlining techniques for improving the training performance of your PyTorch model without compromising its accuracy.
By changing only a few lines of code, we are going from 22.63 minutes to 3.15 minutes when finetuning BERT:
------
And that's 2 min (an 11.5x performance boost) if you have 8 GPUs to spare 
------
Adding these two new books to my collection - 1. "#Mathematics for #MachineLearning" by Mark Peter Deisenroth + A. Aldo Faisal 
@AnalogAldo
 + Cheng Soon Ong 
@ChengSoonOng
 and 2. "#Python Machine Learning" by Sebastian Raschka 
@rasbt
 + Vahid Mirjalili
------
The Pandas 2.0 release candidate is out! https://pandas.pydata.org/docs/dev/whatsnew/v2.0.0.html…
Pandas is one of those libs since I started using Python as a grad student. I've been using it ever since. 
Sure, there's polars now, but for most contexts, pandas is a sufficient go-to for my interactive data prep.
------
Can deep transformers be trained without skip connections nor normalisation layers?

Our ICLR 2023 paper shows you how, using wide NN signal propagation ideas. We hope this can potentially pave the way to more efficient deep LLMs! (1/9)

Paper: https://arxiv.org/abs/2302.10322
------
When deciding upon an appropriate value of *k* for k-fold cross-validation, we are often guided by computational performance and conventions. 

However, it's worthwhile to define k based on the purpose and context when we are using k-fold cross-validation. 

1) If we care mostly…
------
Thank you! This is the gist of it, see also the blog https://windowsontheory.org/2023/02/21/provable-copyright-protection-for-generative-models/…

Point is that the combined model can use shared parts of distribution but would not output anything too similar to a training point since it’s guaranteed to be close to model that didn’t see it .
------
Whether it's okay to train generative models on copyrighted material is still up for debate. However, beyond the fair-use discussion, there's this related issue whether the *generated outputs* violate copyrights. This is an issue because generative models https://arxiv.org/abs/2302.10870…
------
https://arxiv.org/abs/2301.11108
A paper on the mathematics of diffusion models that explains the diffusion SDEs --- both forward and backward --- assuming only familiarity with Gaussians. It also gives some original non-variational likelihood formulas.
------
"MarioGPT: Open-Ended Text2Level Generation through Large Language Models" 
This looks like a pretty fun and creative project!
The best part, it's based on a distilled GPT-3 model and can be trained on a single GPU.
------
In my excitement, I totally forgot to include the Arxiv link: https://arxiv.org/abs/2302.05981v1…

(And I meant a distilled version of GPT-2, which is ~80M parameters. You can run it on a 2080Ti.)
------
If you want to give it a try, I added a self-contained project template for PyTorch models here: https://github.com/rasbt/machine-learning-notes/blob/main/templates/modern-early-stop-with-checkpointing/checkpointing.ipynb…
------
And I'd argue it's always a good idea to consider adopting early stopping by default.
If you are using @PyTorch, that's super easy to implement, so no excuses   twitter.com/vipul_1011/sta…
------
A ~100 pages survey on pretrained foundation models: from BERT to ChatGPT. 

Has a nice structure for reviewing pretrained models in addition to challenges and open problems. 

https://arxiv.org/abs/2302.09419
------
In recent years, Transformers made other methods for text classification and generation obsolete (bag-of-words, 1D CNNs, RNNs, ...). 
Text is essentially sequence data, so let's address the elephant in the room: "Are Transformers Effective for Time Series https://arxiv.org/abs/2205.13504…
------
LLMs are clearly inspired by JRPGs.
------
Why do LLM services insist on "typing" their responses? Are they trying to replicate the slow, pixelated, nostalgic experience of dial-up internet revealing an image line by line? ...Yet another way in which LLMs are like "blurry JPEGs."
------
Productive weekend! Just added 4 new Q&A's!

- Multi-GPU Training Paradigms
- The Distributional Hypothesis 
- "Self"-Attention
- Training & Test Set Discordance

And "Machine Learning Q and AI" just crossed the 50% milestone! 

PS: the Multi-GPU section https://leanpub.com/machine-learning-q-and-ai/……
------
Here are some resources to understand RLHF (reinforcement learning with human feedback)

(1) "Asynchronous Methods for Deep Reinforcement Learning" (2016, https://arxiv.org/abs/1602.01783). Introduces policy gradients methods for deep learning-based RL as https://arxiv.org/abs/2203.02155…
------
Also just added them the LLM reading list here
------
RLHF (reinforcement learning with human feedback) may not fix the current issues of LLMs, but it's the best we have -- e.g. compare current-gen to previous-gen LLMs. 
There'll be more of that in the future, incl creative ways to apply this to LLMs in other domains (AlphaFold 3?)
------
Is RLHF sufficient for aligning LLMs with human values?
My answer: no.
------
Happy to see researchers questioning whether RLHF is sufficient for “alignment”. I like this perspective: https://researchgate.net/publication/228764857_That_special_something_Dennett_on_the_making_of_minds_and_selves… of @danieldennett. But ethical questions abound as it involves ego, TOM, morality, compassion, empathy…
------
GPT in 60 Lines of NumPy -- a simple yet complete technical introduction to the GPT as an educational tool. 

https://jaykmody.com/blog/gpt-from-scratch/…

I love this! It reminds me why coding in Python can be so much fun!
------
If your GPU supports mixed precision training, it's a good idea to enable it.

Fine-tuning a BERT model, mixed-precision can 4x training speed.

Shared the code here if you want to take a look: https://github.com/rasbt/deeplearning-models/tree/master/pytorch-lightning_ipynb/transformer/distilbert-finetuning-ii-amp…
------
And I'd argue it's always a good idea to consider adopting early stopping by default.
If you are using 
@PyTorch
, that's super easy to implement, so no excuses 
------
This is really interesting. Performing pruning after full training doesn't help much twitter.com/rasbt/status/1…
------
The above example uses the 
@LightningAI
 Trainer for PyTorch where model checkpointing comes basically for free. 
I have a full notebook example fine-tuning an LLM (BERT) here: https://github.com/rasbt/deeplearning-models/blob/master/pytorch-lightning_ipynb/transformer/distilbert-finetuning-ii.ipynb…
------
And that's 2 min (an 11.5x performance boost) if you have 8 GPUs to spare 
------
Adding these two new books to my collection - 1. "#Mathematics for #MachineLearning" by Mark Peter Deisenroth + A. Aldo Faisal 
@AnalogAldo
 + Cheng Soon Ong 
@ChengSoonOng
 and 2. "#Python Machine Learning" by Sebastian Raschka 
@rasbt
 + Vahid Mirjalili
------
The Pandas 2.0 release candidate is out! https://pandas.pydata.org/docs/dev/whatsnew/v2.0.0.html…
Pandas is one of those libs since I started using Python as a grad student. I've been using it ever since. 
Sure, there's polars now, but for most contexts, pandas is a sufficient go-to for my interactive data prep.
------
Can deep transformers be trained without skip connections nor normalisation layers?

Our ICLR 2023 paper shows you how, using wide NN signal propagation ideas. We hope this can potentially pave the way to more efficient deep LLMs! (1/9)

Paper: https://arxiv.org/abs/2302.10322
------
When deciding upon an appropriate value of *k* for k-fold cross-validation, we are often guided by computational performance and conventions. 

However, it's worthwhile to define k based on the purpose and context when we are using k-fold cross-validation. 

1) If we care mostly…
------
Thank you! This is the gist of it, see also the blog https://windowsontheory.org/2023/02/21/provable-copyright-protection-for-generative-models/…

Point is that the combined model can use shared parts of distribution but would not output anything too similar to a training point since it’s guaranteed to be close to model that didn’t see it .
------
Whether it's okay to train generative models on copyrighted material is still up for debate. However, beyond the fair-use discussion, there's this related issue whether the *generated outputs* violate copyrights. This is an issue because generative models https://arxiv.org/abs/2302.10870…
------
https://arxiv.org/abs/2301.11108
A paper on the mathematics of diffusion models that explains the diffusion SDEs --- both forward and backward --- assuming only familiarity with Gaussians. It also gives some original non-variational likelihood formulas.
------
"MarioGPT: Open-Ended Text2Level Generation through Large Language Models" 
This looks like a pretty fun and creative project!
The best part, it's based on a distilled GPT-3 model and can be trained on a single GPU.
------
In my excitement, I totally forgot to include the Arxiv link: https://arxiv.org/abs/2302.05981v1…

(And I meant a distilled version of GPT-2, which is ~80M parameters. You can run it on a 2080Ti.)
------
If you want to give it a try, I added a self-contained project template for PyTorch models here: https://github.com/rasbt/machine-learning-notes/blob/main/templates/modern-early-stop-with-checkpointing/checkpointing.ipynb…
------
And I'd argue it's always a good idea to consider adopting early stopping by default.
If you are using @PyTorch, that's super easy to implement, so no excuses   twitter.com/vipul_1011/sta…
------
A ~100 pages survey on pretrained foundation models: from BERT to ChatGPT. 

Has a nice structure for reviewing pretrained models in addition to challenges and open problems. 

https://arxiv.org/abs/2302.09419
------
In recent years, Transformers made other methods for text classification and generation obsolete (bag-of-words, 1D CNNs, RNNs, ...). 
Text is essentially sequence data, so let's address the elephant in the room: "Are Transformers Effective for Time Series https://arxiv.org/abs/2205.13504…
------
LLMs are clearly inspired by JRPGs.
------
Why do LLM services insist on "typing" their responses? Are they trying to replicate the slow, pixelated, nostalgic experience of dial-up internet revealing an image line by line? ...Yet another way in which LLMs are like "blurry JPEGs."
------
Productive weekend! Just added 4 new Q&A's!

- Multi-GPU Training Paradigms
- The Distributional Hypothesis 
- "Self"-Attention
- Training & Test Set Discordance

And "Machine Learning Q and AI" just crossed the 50% milestone! 

PS: the Multi-GPU section https://leanpub.com/machine-learning-q-and-ai/……
------
Here are some resources to understand RLHF (reinforcement learning with human feedback)

(1) "Asynchronous Methods for Deep Reinforcement Learning" (2016, https://arxiv.org/abs/1602.01783). Introduces policy gradients methods for deep learning-based RL as https://arxiv.org/abs/2203.02155…
------
Also just added them the LLM reading list here
------
RLHF (reinforcement learning with human feedback) may not fix the current issues of LLMs, but it's the best we have -- e.g. compare current-gen to previous-gen LLMs. 
There'll be more of that in the future, incl creative ways to apply this to LLMs in other domains (AlphaFold 3?)
------
Is RLHF sufficient for aligning LLMs with human values?
My answer: no.
------
Happy to see researchers questioning whether RLHF is sufficient for “alignment”. I like this perspective: https://researchgate.net/publication/228764857_That_special_something_Dennett_on_the_making_of_minds_and_selves… of @danieldennett. But ethical questions abound as it involves ego, TOM, morality, compassion, empathy…
------
GPT in 60 Lines of NumPy -- a simple yet complete technical introduction to the GPT as an educational tool. 

https://jaykmody.com/blog/gpt-from-scratch/…

I love this! It reminds me why coding in Python can be so much fun!
------
If your GPU supports mixed precision training, it's a good idea to enable it.

Fine-tuning a BERT model, mixed-precision can 4x training speed.

Shared the code here if you want to take a look: https://github.com/rasbt/deeplearning-models/tree/master/pytorch-lightning_ipynb/transformer/distilbert-finetuning-ii-amp…
------
And I'd argue it's always a good idea to consider adopting early stopping by default.
If you are using 
@PyTorch
, that's super easy to implement, so no excuses 
------
This is really interesting. Performing pruning after full training doesn't help much twitter.com/rasbt/status/1…
------
The above example uses the 
@LightningAI
 Trainer for PyTorch where model checkpointing comes basically for free. 
I have a full notebook example fine-tuning an LLM (BERT) here: https://github.com/rasbt/deeplearning-models/blob/master/pytorch-lightning_ipynb/transformer/distilbert-finetuning-ii.ipynb…
------
"A Survey on Efficient Training of Transformers" 2023 (https://arxiv.org/abs/2302.01107)
 
Summarized some of the interesting takeaways below. 
(Note that alternatives to scaled-dot product self-attention are notably absent -- no one uses these, still?)

1/6
------
Large batch training (which I recently discussed in another Twitter thread, https://twitter.com/rasbt/status/1617544195220312066?s=20…) is one of the prevailing ways to accelerate training.

5/6
------
Regarding choosing a good batch size:
Choosing a batch size to be as large as your hardware permits (up to a point) seems to be a good recommendation.
Compiled some resources via the figure below.
------
Incremental learning can be used both ways. Instead of the traditional approach of progressively stacking layers and initializing a larger model from a smaller one, the reverse direction (layer dropping) seems to be very promising for transformers: https://arxiv.org/abs/2010.13369

6/6
------
Thank you! This is the gist of it, see also the blog https://windowsontheory.org/2023/02/21/provable-copyright-protection-for-generative-models/…

Point is that the combined model can use shared parts of distribution but would not output anything too similar to a training point since it’s guaranteed to be close to model that didn’t see it .
------
Whether it's okay to train generative models on copyrighted material is still up for debate. However, beyond the fair-use discussion, there's this related issue whether the *generated outputs* violate copyrights. This is an issue because generative models https://arxiv.org/abs/2302.10870…
------
https://arxiv.org/abs/2301.11108
A paper on the mathematics of diffusion models that explains the diffusion SDEs --- both forward and backward --- assuming only familiarity with Gaussians. It also gives some original non-variational likelihood formulas.
------
"MarioGPT: Open-Ended Text2Level Generation through Large Language Models" 
This looks like a pretty fun and creative project!
The best part, it's based on a distilled GPT-3 model and can be trained on a single GPU.
------
In my excitement, I totally forgot to include the Arxiv link: https://arxiv.org/abs/2302.05981v1…

(And I meant a distilled version of GPT-2, which is ~80M parameters. You can run it on a 2080Ti.)
------
If you want to give it a try, I added a self-contained project template for PyTorch models here: https://github.com/rasbt/machine-learning-notes/blob/main/templates/modern-early-stop-with-checkpointing/checkpointing.ipynb…
------
And I'd argue it's always a good idea to consider adopting early stopping by default.
If you are using @PyTorch, that's super easy to implement, so no excuses   twitter.com/vipul_1011/sta…
------
A ~100 pages survey on pretrained foundation models: from BERT to ChatGPT. 

Has a nice structure for reviewing pretrained models in addition to challenges and open problems. 

https://arxiv.org/abs/2302.09419
------
In recent years, Transformers made other methods for text classification and generation obsolete (bag-of-words, 1D CNNs, RNNs, ...). 
Text is essentially sequence data, so let's address the elephant in the room: "Are Transformers Effective for Time Series https://arxiv.org/abs/2205.13504…
------
LLMs are clearly inspired by JRPGs.
------
Why do LLM services insist on "typing" their responses? Are they trying to replicate the slow, pixelated, nostalgic experience of dial-up internet revealing an image line by line? ...Yet another way in which LLMs are like "blurry JPEGs."
------
Productive weekend! Just added 4 new Q&A's!

- Multi-GPU Training Paradigms
- The Distributional Hypothesis 
- "Self"-Attention
- Training & Test Set Discordance

And "Machine Learning Q and AI" just crossed the 50% milestone! 

PS: the Multi-GPU section https://leanpub.com/machine-learning-q-and-ai/……
------
Here are some resources to understand RLHF (reinforcement learning with human feedback)

(1) "Asynchronous Methods for Deep Reinforcement Learning" (2016, https://arxiv.org/abs/1602.01783). Introduces policy gradients methods for deep learning-based RL as https://arxiv.org/abs/2203.02155…
------
Also just added them the LLM reading list here
------
RLHF (reinforcement learning with human feedback) may not fix the current issues of LLMs, but it's the best we have -- e.g. compare current-gen to previous-gen LLMs. 
There'll be more of that in the future, incl creative ways to apply this to LLMs in other domains (AlphaFold 3?)
------
Is RLHF sufficient for aligning LLMs with human values?
My answer: no.
------
Happy to see researchers questioning whether RLHF is sufficient for “alignment”. I like this perspective: https://researchgate.net/publication/228764857_That_special_something_Dennett_on_the_making_of_minds_and_selves… of @danieldennett. But ethical questions abound as it involves ego, TOM, morality, compassion, empathy…
------
GPT in 60 Lines of NumPy -- a simple yet complete technical introduction to the GPT as an educational tool. 

https://jaykmody.com/blog/gpt-from-scratch/…

I love this! It reminds me why coding in Python can be so much fun!
------
If your GPU supports mixed precision training, it's a good idea to enable it.

Fine-tuning a BERT model, mixed-precision can 4x training speed.

Shared the code here if you want to take a look: https://github.com/rasbt/deeplearning-models/tree/master/pytorch-lightning_ipynb/transformer/distilbert-finetuning-ii-amp…
------
And I'd argue it's always a good idea to consider adopting early stopping by default.
If you are using 
@PyTorch
, that's super easy to implement, so no excuses 
------
This is really interesting. Performing pruning after full training doesn't help much twitter.com/rasbt/status/1…
------
The above example uses the 
@LightningAI
 Trainer for PyTorch where model checkpointing comes basically for free. 
I have a full notebook example fine-tuning an LLM (BERT) here: https://github.com/rasbt/deeplearning-models/blob/master/pytorch-lightning_ipynb/transformer/distilbert-finetuning-ii.ipynb…
------
"A Survey on Efficient Training of Transformers" 2023 (https://arxiv.org/abs/2302.01107)
 
Summarized some of the interesting takeaways below. 
(Note that alternatives to scaled-dot product self-attention are notably absent -- no one uses these, still?)

1/6
------
Large batch training (which I recently discussed in another Twitter thread, https://twitter.com/rasbt/status/1617544195220312066?s=20…) is one of the prevailing ways to accelerate training.

5/6
------
Regarding choosing a good batch size:
Choosing a batch size to be as large as your hardware permits (up to a point) seems to be a good recommendation.
Compiled some resources via the figure below.
------
Incremental learning can be used both ways. Instead of the traditional approach of progressively stacking layers and initializing a larger model from a smaller one, the reverse direction (layer dropping) seems to be very promising for transformers: https://arxiv.org/abs/2010.13369

6/6
------
Increasing the number of parameters and overparameterizing transformers improves both generalization and convergence. Inspired by the lottery ticket hypothesis, an efficient training strategy is to train a large model via early stopping and then prune it.

4/6
------
What is your favorite matplotlib configuration setting for beautiful scientific charts? Links welcome to open source or examples of charts you love.
------
Regarding Tensor Parallelism: how does that work, how can we just split a matrix or tensor across GPUs?

Like always, it goes back to the fundamental concepts of linear algebra 
------
Training deep neural nets on multiple GPUs has become increasingly common in recent years.
Dividing the workload allows for larger and more complex models to be trained more quickly.

I made a little cheatsheet summarizing the different approaches:
------
Theoretically, we could also use block multiplication, but I don't think this is commonly implemented or used for tensor parallelism (yet)
------
Training deep neural nets on multiple GPUs has become increasingly common in recent years.
Dividing the workload allows for larger and more complex models to be trained more quickly.

I made a little cheatsheet summarizing the different approaches:
------
In my excitement, I totally forgot to include the Arxiv link: https://arxiv.org/abs/2302.05981v1…

(And I meant a distilled version of GPT-2, which is ~80M parameters. You can run it on a 2080Ti.)
------
If you want to give it a try, I added a self-contained project template for PyTorch models here: https://github.com/rasbt/machine-learning-notes/blob/main/templates/modern-early-stop-with-checkpointing/checkpointing.ipynb…
------
And I'd argue it's always a good idea to consider adopting early stopping by default.
If you are using @PyTorch, that's super easy to implement, so no excuses   twitter.com/vipul_1011/sta…
------
A ~100 pages survey on pretrained foundation models: from BERT to ChatGPT. 

Has a nice structure for reviewing pretrained models in addition to challenges and open problems. 

https://arxiv.org/abs/2302.09419
------
In recent years, Transformers made other methods for text classification and generation obsolete (bag-of-words, 1D CNNs, RNNs, ...). 
Text is essentially sequence data, so let's address the elephant in the room: "Are Transformers Effective for Time Series https://arxiv.org/abs/2205.13504…
------
LLMs are clearly inspired by JRPGs.
------
Why do LLM services insist on "typing" their responses? Are they trying to replicate the slow, pixelated, nostalgic experience of dial-up internet revealing an image line by line? ...Yet another way in which LLMs are like "blurry JPEGs."
------
Productive weekend! Just added 4 new Q&A's!

- Multi-GPU Training Paradigms
- The Distributional Hypothesis 
- "Self"-Attention
- Training & Test Set Discordance

And "Machine Learning Q and AI" just crossed the 50% milestone! 

PS: the Multi-GPU section https://leanpub.com/machine-learning-q-and-ai/……
------
Here are some resources to understand RLHF (reinforcement learning with human feedback)

(1) "Asynchronous Methods for Deep Reinforcement Learning" (2016, https://arxiv.org/abs/1602.01783). Introduces policy gradients methods for deep learning-based RL as https://arxiv.org/abs/2203.02155…
------
Also just added them the LLM reading list here
------
RLHF (reinforcement learning with human feedback) may not fix the current issues of LLMs, but it's the best we have -- e.g. compare current-gen to previous-gen LLMs. 
There'll be more of that in the future, incl creative ways to apply this to LLMs in other domains (AlphaFold 3?)
------
Is RLHF sufficient for aligning LLMs with human values?
My answer: no.
------
Happy to see researchers questioning whether RLHF is sufficient for “alignment”. I like this perspective: https://researchgate.net/publication/228764857_That_special_something_Dennett_on_the_making_of_minds_and_selves… of @danieldennett. But ethical questions abound as it involves ego, TOM, morality, compassion, empathy…
------
GPT in 60 Lines of NumPy -- a simple yet complete technical introduction to the GPT as an educational tool. 

https://jaykmody.com/blog/gpt-from-scratch/…

I love this! It reminds me why coding in Python can be so much fun!
------
If your GPU supports mixed precision training, it's a good idea to enable it.

Fine-tuning a BERT model, mixed-precision can 4x training speed.

Shared the code here if you want to take a look: https://github.com/rasbt/deeplearning-models/tree/master/pytorch-lightning_ipynb/transformer/distilbert-finetuning-ii-amp…
------
And I'd argue it's always a good idea to consider adopting early stopping by default.
If you are using 
@PyTorch
, that's super easy to implement, so no excuses 
------
This is really interesting. Performing pruning after full training doesn't help much twitter.com/rasbt/status/1…
------
The above example uses the 
@LightningAI
 Trainer for PyTorch where model checkpointing comes basically for free. 
I have a full notebook example fine-tuning an LLM (BERT) here: https://github.com/rasbt/deeplearning-models/blob/master/pytorch-lightning_ipynb/transformer/distilbert-finetuning-ii.ipynb…
------
"A Survey on Efficient Training of Transformers" 2023 (https://arxiv.org/abs/2302.01107)
 
Summarized some of the interesting takeaways below. 
(Note that alternatives to scaled-dot product self-attention are notably absent -- no one uses these, still?)

1/6
------
Large batch training (which I recently discussed in another Twitter thread, https://twitter.com/rasbt/status/1617544195220312066?s=20…) is one of the prevailing ways to accelerate training.

5/6
------
Regarding choosing a good batch size:
Choosing a batch size to be as large as your hardware permits (up to a point) seems to be a good recommendation.
Compiled some resources via the figure below.
------
Incremental learning can be used both ways. Instead of the traditional approach of progressively stacking layers and initializing a larger model from a smaller one, the reverse direction (layer dropping) seems to be very promising for transformers: https://arxiv.org/abs/2010.13369

6/6
------
Increasing the number of parameters and overparameterizing transformers improves both generalization and convergence. Inspired by the lottery ticket hypothesis, an efficient training strategy is to train a large model via early stopping and then prune it.

4/6
------
What is your favorite matplotlib configuration setting for beautiful scientific charts? Links welcome to open source or examples of charts you love.
------
Regarding Tensor Parallelism: how does that work, how can we just split a matrix or tensor across GPUs?

Like always, it goes back to the fundamental concepts of linear algebra 
------
Training deep neural nets on multiple GPUs has become increasingly common in recent years.
Dividing the workload allows for larger and more complex models to be trained more quickly.

I made a little cheatsheet summarizing the different approaches:
------
Theoretically, we could also use block multiplication, but I don't think this is commonly implemented or used for tensor parallelism (yet)
------
Training deep neural nets on multiple GPUs has become increasingly common in recent years.
Dividing the workload allows for larger and more complex models to be trained more quickly.

I made a little cheatsheet summarizing the different approaches:
------
or, for fine-tuning if 5) is not enough, consider DeepSpeed again:
Trainer(…, strategy=“deepspeed_stage_3") 
Trainer(…, strategy="deepspeed_stage_3_offload")

[4/5]
------
Here are some additional resources (and I will also have a Lightning Trainer intro in Unit 5 of my DL Fundamentals course, dropping next week hopefully):

Multi-GPU training intro: https://pytorch-lightning.readthedocs.io/en/latest/accelerators/gpu_basic.html…

More advanced discussion: https://pytorch-lightning.readthedocs.io/en/latest/accelerators/gpu_intermediate.html…

[5/5]
------
Training a convolutional neural network to 94% accuracy on CIFAR-10 in 8 seconds? 
Yes, I can confirm, it works!

Source code: https://github.com/tysam-code/hlb-CIFAR10…
------
I should mention this is on a single A100, same hardware as suggested in the repo.
------
Not everyone likes using LLMs as writing aids. That's okay. 
But a ban on LLMs as writing aids would be similar to banning calculators as calculation aids -- because calculators can be misused by individuals engaging in financial crimes.
------
The significance of deep learning in the natural and physical sciences is often underappreciated.
Despite the media buzz surrounding conversational chatbots & consumer-facing AI apps, the real impact of AI may lie in its applications in the natural and physical sciences.
------
This week, I have been working on including reader quizzes for the remaining Q & A's -- adding two new questions for each.

I tried to make them challenging, and I may probably end up using some of them as #machinelearning interview questions :P

https://leanpub.com/machine-learning-q-and-ai/…
------
I realized that the textbooks & courses that were most impactful for me all incorporated varying levels of challenging exercises & quizzes to check my understanding. 

So, I've begun adding reader quizzes ...
Have fun & thanks for supporting this work!

https://leanpub.com/machine-learning-q-and-ai/…
------
LLMs are clearly inspired by JRPGs.
------
Why do LLM services insist on "typing" their responses? Are they trying to replicate the slow, pixelated, nostalgic experience of dial-up internet revealing an image line by line? ...Yet another way in which LLMs are like "blurry JPEGs."
------
Productive weekend! Just added 4 new Q&A's!

- Multi-GPU Training Paradigms
- The Distributional Hypothesis 
- "Self"-Attention
- Training & Test Set Discordance

And "Machine Learning Q and AI" just crossed the 50% milestone! 

PS: the Multi-GPU section https://leanpub.com/machine-learning-q-and-ai/……
------
Here are some resources to understand RLHF (reinforcement learning with human feedback)

(1) "Asynchronous Methods for Deep Reinforcement Learning" (2016, https://arxiv.org/abs/1602.01783). Introduces policy gradients methods for deep learning-based RL as https://arxiv.org/abs/2203.02155…
------
Also just added them the LLM reading list here
------
RLHF (reinforcement learning with human feedback) may not fix the current issues of LLMs, but it's the best we have -- e.g. compare current-gen to previous-gen LLMs. 
There'll be more of that in the future, incl creative ways to apply this to LLMs in other domains (AlphaFold 3?)
------
Is RLHF sufficient for aligning LLMs with human values?
My answer: no.
------
Happy to see researchers questioning whether RLHF is sufficient for “alignment”. I like this perspective: https://researchgate.net/publication/228764857_That_special_something_Dennett_on_the_making_of_minds_and_selves… of @danieldennett. But ethical questions abound as it involves ego, TOM, morality, compassion, empathy…
------
GPT in 60 Lines of NumPy -- a simple yet complete technical introduction to the GPT as an educational tool. 

https://jaykmody.com/blog/gpt-from-scratch/…

I love this! It reminds me why coding in Python can be so much fun!
------
If your GPU supports mixed precision training, it's a good idea to enable it.

Fine-tuning a BERT model, mixed-precision can 4x training speed.

Shared the code here if you want to take a look: https://github.com/rasbt/deeplearning-models/tree/master/pytorch-lightning_ipynb/transformer/distilbert-finetuning-ii-amp…
------
And I'd argue it's always a good idea to consider adopting early stopping by default.
If you are using 
@PyTorch
, that's super easy to implement, so no excuses 
------
This is really interesting. Performing pruning after full training doesn't help much twitter.com/rasbt/status/1…
------
The above example uses the 
@LightningAI
 Trainer for PyTorch where model checkpointing comes basically for free. 
I have a full notebook example fine-tuning an LLM (BERT) here: https://github.com/rasbt/deeplearning-models/blob/master/pytorch-lightning_ipynb/transformer/distilbert-finetuning-ii.ipynb…
------
"A Survey on Efficient Training of Transformers" 2023 (https://arxiv.org/abs/2302.01107)
 
Summarized some of the interesting takeaways below. 
(Note that alternatives to scaled-dot product self-attention are notably absent -- no one uses these, still?)

1/6
------
Large batch training (which I recently discussed in another Twitter thread, https://twitter.com/rasbt/status/1617544195220312066?s=20…) is one of the prevailing ways to accelerate training.

5/6
------
Regarding choosing a good batch size:
Choosing a batch size to be as large as your hardware permits (up to a point) seems to be a good recommendation.
Compiled some resources via the figure below.
------
Incremental learning can be used both ways. Instead of the traditional approach of progressively stacking layers and initializing a larger model from a smaller one, the reverse direction (layer dropping) seems to be very promising for transformers: https://arxiv.org/abs/2010.13369

6/6
------
Increasing the number of parameters and overparameterizing transformers improves both generalization and convergence. Inspired by the lottery ticket hypothesis, an efficient training strategy is to train a large model via early stopping and then prune it.

4/6
------
What is your favorite matplotlib configuration setting for beautiful scientific charts? Links welcome to open source or examples of charts you love.
------
Regarding Tensor Parallelism: how does that work, how can we just split a matrix or tensor across GPUs?

Like always, it goes back to the fundamental concepts of linear algebra 
------
Training deep neural nets on multiple GPUs has become increasingly common in recent years.
Dividing the workload allows for larger and more complex models to be trained more quickly.

I made a little cheatsheet summarizing the different approaches:
------
Theoretically, we could also use block multiplication, but I don't think this is commonly implemented or used for tensor parallelism (yet)
------
Training deep neural nets on multiple GPUs has become increasingly common in recent years.
Dividing the workload allows for larger and more complex models to be trained more quickly.

I made a little cheatsheet summarizing the different approaches:
------
or, for fine-tuning if 5) is not enough, consider DeepSpeed again:
Trainer(…, strategy=“deepspeed_stage_3") 
Trainer(…, strategy="deepspeed_stage_3_offload")

[4/5]
------
Here are some additional resources (and I will also have a Lightning Trainer intro in Unit 5 of my DL Fundamentals course, dropping next week hopefully):

Multi-GPU training intro: https://pytorch-lightning.readthedocs.io/en/latest/accelerators/gpu_basic.html…

More advanced discussion: https://pytorch-lightning.readthedocs.io/en/latest/accelerators/gpu_intermediate.html…

[5/5]
------
Training a convolutional neural network to 94% accuracy on CIFAR-10 in 8 seconds? 
Yes, I can confirm, it works!

Source code: https://github.com/tysam-code/hlb-CIFAR10…
------
I should mention this is on a single A100, same hardware as suggested in the repo.
------
Not everyone likes using LLMs as writing aids. That's okay. 
But a ban on LLMs as writing aids would be similar to banning calculators as calculation aids -- because calculators can be misused by individuals engaging in financial crimes.
------
The significance of deep learning in the natural and physical sciences is often underappreciated.
Despite the media buzz surrounding conversational chatbots & consumer-facing AI apps, the real impact of AI may lie in its applications in the natural and physical sciences.
------
This week, I have been working on including reader quizzes for the remaining Q & A's -- adding two new questions for each.

I tried to make them challenging, and I may probably end up using some of them as #machinelearning interview questions :P

https://leanpub.com/machine-learning-q-and-ai/…
------
I realized that the textbooks & courses that were most impactful for me all incorporated varying levels of challenging exercises & quizzes to check my understanding. 

So, I've begun adding reader quizzes ...
Have fun & thanks for supporting this work!

https://leanpub.com/machine-learning-q-and-ai/…
------
Somewhat inspired by the recent paper recommendation by 
@Machine01776819
 , "When Deep Learners Change Their Mind: Learning Dynamics for Active Learning"
------
Also just added them the LLM reading list here
------
RLHF (reinforcement learning with human feedback) may not fix the current issues of LLMs, but it's the best we have -- e.g. compare current-gen to previous-gen LLMs. 
There'll be more of that in the future, incl creative ways to apply this to LLMs in other domains (AlphaFold 3?)
------
Is RLHF sufficient for aligning LLMs with human values?
My answer: no.
------
Happy to see researchers questioning whether RLHF is sufficient for “alignment”. I like this perspective: https://researchgate.net/publication/228764857_That_special_something_Dennett_on_the_making_of_minds_and_selves… of @danieldennett. But ethical questions abound as it involves ego, TOM, morality, compassion, empathy…
------
GPT in 60 Lines of NumPy -- a simple yet complete technical introduction to the GPT as an educational tool. 

https://jaykmody.com/blog/gpt-from-scratch/…

I love this! It reminds me why coding in Python can be so much fun!
------
If your GPU supports mixed precision training, it's a good idea to enable it.

Fine-tuning a BERT model, mixed-precision can 4x training speed.

Shared the code here if you want to take a look: https://github.com/rasbt/deeplearning-models/tree/master/pytorch-lightning_ipynb/transformer/distilbert-finetuning-ii-amp…
------
And I'd argue it's always a good idea to consider adopting early stopping by default.
If you are using 
@PyTorch
, that's super easy to implement, so no excuses 
------
This is really interesting. Performing pruning after full training doesn't help much twitter.com/rasbt/status/1…
------
The above example uses the 
@LightningAI
 Trainer for PyTorch where model checkpointing comes basically for free. 
I have a full notebook example fine-tuning an LLM (BERT) here: https://github.com/rasbt/deeplearning-models/blob/master/pytorch-lightning_ipynb/transformer/distilbert-finetuning-ii.ipynb…
------
"A Survey on Efficient Training of Transformers" 2023 (https://arxiv.org/abs/2302.01107)
 
Summarized some of the interesting takeaways below. 
(Note that alternatives to scaled-dot product self-attention are notably absent -- no one uses these, still?)

1/6
------
Large batch training (which I recently discussed in another Twitter thread, https://twitter.com/rasbt/status/1617544195220312066?s=20…) is one of the prevailing ways to accelerate training.

5/6
------
Regarding choosing a good batch size:
Choosing a batch size to be as large as your hardware permits (up to a point) seems to be a good recommendation.
Compiled some resources via the figure below.
------
Incremental learning can be used both ways. Instead of the traditional approach of progressively stacking layers and initializing a larger model from a smaller one, the reverse direction (layer dropping) seems to be very promising for transformers: https://arxiv.org/abs/2010.13369

6/6
------
What is your favorite matplotlib configuration setting for beautiful scientific charts? Links welcome to open source or examples of charts you love.
------
Regarding Tensor Parallelism: how does that work, how can we just split a matrix or tensor across GPUs?

Like always, it goes back to the fundamental concepts of linear algebra 
------
Training deep neural nets on multiple GPUs has become increasingly common in recent years.
Dividing the workload allows for larger and more complex models to be trained more quickly.

I made a little cheatsheet summarizing the different approaches:
------
Theoretically, we could also use block multiplication, but I don't think this is commonly implemented or used for tensor parallelism (yet)
------
Training deep neural nets on multiple GPUs has become increasingly common in recent years.
Dividing the workload allows for larger and more complex models to be trained more quickly.

I made a little cheatsheet summarizing the different approaches:
------
or, for fine-tuning if 5) is not enough, consider DeepSpeed again:
Trainer(…, strategy=“deepspeed_stage_3") 
Trainer(…, strategy="deepspeed_stage_3_offload")

[4/5]
------
Here are some additional resources (and I will also have a Lightning Trainer intro in Unit 5 of my DL Fundamentals course, dropping next week hopefully):

Multi-GPU training intro: https://pytorch-lightning.readthedocs.io/en/latest/accelerators/gpu_basic.html…

More advanced discussion: https://pytorch-lightning.readthedocs.io/en/latest/accelerators/gpu_intermediate.html…

[5/5]
------
Training a convolutional neural network to 94% accuracy on CIFAR-10 in 8 seconds? 
Yes, I can confirm, it works!

Source code: https://github.com/tysam-code/hlb-CIFAR10…
------
I should mention this is on a single A100, same hardware as suggested in the repo.
------
Not everyone likes using LLMs as writing aids. That's okay. 
But a ban on LLMs as writing aids would be similar to banning calculators as calculation aids -- because calculators can be misused by individuals engaging in financial crimes.
------
The significance of deep learning in the natural and physical sciences is often underappreciated.
Despite the media buzz surrounding conversational chatbots & consumer-facing AI apps, the real impact of AI may lie in its applications in the natural and physical sciences.
------
This week, I have been working on including reader quizzes for the remaining Q & A's -- adding two new questions for each.

I tried to make them challenging, and I may probably end up using some of them as #machinelearning interview questions :P

https://leanpub.com/machine-learning-q-and-ai/…
------
I realized that the textbooks & courses that were most impactful for me all incorporated varying levels of challenging exercises & quizzes to check my understanding. 

So, I've begun adding reader quizzes ...
Have fun & thanks for supporting this work!

https://leanpub.com/machine-learning-q-and-ai/…
------
Somewhat inspired by the recent paper recommendation by 
@Machine01776819
 , "When Deep Learners Change Their Mind: Learning Dynamics for Active Learning"
------
Are LLMs making it easier to write good essays? I’d say NO, they are just shifting the goalpost.

LLMs will help us to write "correctly". But crafting an original and engaging essay that someone wants to read will become more challenging in the future.

https://technologyreview.com/2023/02/07/1067928/why-detecting-ai-generated-text-is-so-difficult-and-what-to-do-about-it/…
------
Ty for the all the positive feedback on the "coding self-attention from scratch" article!

Lots of questions about cross-attention. The beautiful thing about blogs is that it is easy to update and extend!

I hope you like the new sections and figures.
https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html…
------
Since self-attention is now everywhere, it's important to understand how it works.
And there is no better and more fun way than coding it from scratch!
My new article on "Understanding the Self-Attention Mechanism of Large Language Models From Scratch"
 https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html…
------
And now back to writing Machine Learning Q and AI 
------
As a follow-up to my post yesterday, some were wondering why it's called *self*-attention.

That's because it's an attention mechanism for all the elements of the same set.

In contrast, the original attention for RNNs is applied between the encoder and the decoder embeddings.
------
Since self-attention is now everywhere, it's important to understand how it works.
And there is no better and more fun way than coding it from scratch!
My new article on "Understanding the Self-Attention Mechanism of Large Language Models From Scratch"
 https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html…
------
With the original attention mechanism, I mean the RNN attention mechanism developed by 
@kchonyc
 et al. 2014 in
------
Since self-attention is now everywhere, it's important to understand how it works.
And there is no better and more fun way than coding it from scratch!
My new article on "Understanding the Self-Attention Mechanism of Large Language Models From Scratch"

------
Want to improve the inferencing speed of your PyTorch models? Join us at our webinar in 2 hours (10 am CT).

We'll give a broad overview of techniques, and I'll be doing a code demo & load testing example on stable diffusion
------
The above example uses the 
@LightningAI
 Trainer for PyTorch where model checkpointing comes basically for free. 
I have a full notebook example fine-tuning an LLM (BERT) here: https://github.com/rasbt/deeplearning-models/blob/master/pytorch-lightning_ipynb/transformer/distilbert-finetuning-ii.ipynb…
------
"A Survey on Efficient Training of Transformers" 2023 (https://arxiv.org/abs/2302.01107)
 
Summarized some of the interesting takeaways below. 
(Note that alternatives to scaled-dot product self-attention are notably absent -- no one uses these, still?)

1/6
------
Large batch training (which I recently discussed in another Twitter thread, https://twitter.com/rasbt/status/1617544195220312066?s=20…) is one of the prevailing ways to accelerate training.

5/6
------
Regarding choosing a good batch size:
Choosing a batch size to be as large as your hardware permits (up to a point) seems to be a good recommendation.
Compiled some resources via the figure below.
------
Incremental learning can be used both ways. Instead of the traditional approach of progressively stacking layers and initializing a larger model from a smaller one, the reverse direction (layer dropping) seems to be very promising for transformers: https://arxiv.org/abs/2010.13369

6/6
------
What is your favorite matplotlib configuration setting for beautiful scientific charts? Links welcome to open source or examples of charts you love.
------
Regarding Tensor Parallelism: how does that work, how can we just split a matrix or tensor across GPUs?

Like always, it goes back to the fundamental concepts of linear algebra 
------
Training deep neural nets on multiple GPUs has become increasingly common in recent years.
Dividing the workload allows for larger and more complex models to be trained more quickly.

I made a little cheatsheet summarizing the different approaches:
------
Theoretically, we could also use block multiplication, but I don't think this is commonly implemented or used for tensor parallelism (yet)
------
Training deep neural nets on multiple GPUs has become increasingly common in recent years.
Dividing the workload allows for larger and more complex models to be trained more quickly.

I made a little cheatsheet summarizing the different approaches:
------
or, for fine-tuning if 5) is not enough, consider DeepSpeed again:
Trainer(…, strategy=“deepspeed_stage_3") 
Trainer(…, strategy="deepspeed_stage_3_offload")

[4/5]
------
Here are some additional resources (and I will also have a Lightning Trainer intro in Unit 5 of my DL Fundamentals course, dropping next week hopefully):

Multi-GPU training intro: https://pytorch-lightning.readthedocs.io/en/latest/accelerators/gpu_basic.html…

More advanced discussion: https://pytorch-lightning.readthedocs.io/en/latest/accelerators/gpu_intermediate.html…

[5/5]
------
Training a convolutional neural network to 94% accuracy on CIFAR-10 in 8 seconds? 
Yes, I can confirm, it works!

Source code: https://github.com/tysam-code/hlb-CIFAR10…
------
I should mention this is on a single A100, same hardware as suggested in the repo.
------
Not everyone likes using LLMs as writing aids. That's okay. 
But a ban on LLMs as writing aids would be similar to banning calculators as calculation aids -- because calculators can be misused by individuals engaging in financial crimes.
------
The significance of deep learning in the natural and physical sciences is often underappreciated.
Despite the media buzz surrounding conversational chatbots & consumer-facing AI apps, the real impact of AI may lie in its applications in the natural and physical sciences.
------
This week, I have been working on including reader quizzes for the remaining Q & A's -- adding two new questions for each.

I tried to make them challenging, and I may probably end up using some of them as #machinelearning interview questions :P

https://leanpub.com/machine-learning-q-and-ai/…
------
I realized that the textbooks & courses that were most impactful for me all incorporated varying levels of challenging exercises & quizzes to check my understanding. 

So, I've begun adding reader quizzes ...
Have fun & thanks for supporting this work!

https://leanpub.com/machine-learning-q-and-ai/…
------
Somewhat inspired by the recent paper recommendation by 
@Machine01776819
 , "When Deep Learners Change Their Mind: Learning Dynamics for Active Learning"
------
Are LLMs making it easier to write good essays? I’d say NO, they are just shifting the goalpost.

LLMs will help us to write "correctly". But crafting an original and engaging essay that someone wants to read will become more challenging in the future.

https://technologyreview.com/2023/02/07/1067928/why-detecting-ai-generated-text-is-so-difficult-and-what-to-do-about-it/…
------
Ty for the all the positive feedback on the "coding self-attention from scratch" article!

Lots of questions about cross-attention. The beautiful thing about blogs is that it is easy to update and extend!

I hope you like the new sections and figures.
https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html…
------
Since self-attention is now everywhere, it's important to understand how it works.
And there is no better and more fun way than coding it from scratch!
My new article on "Understanding the Self-Attention Mechanism of Large Language Models From Scratch"
 https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html…
------
And now back to writing Machine Learning Q and AI 
------
As a follow-up to my post yesterday, some were wondering why it's called *self*-attention.

That's because it's an attention mechanism for all the elements of the same set.

In contrast, the original attention for RNNs is applied between the encoder and the decoder embeddings.
------
Since self-attention is now everywhere, it's important to understand how it works.
And there is no better and more fun way than coding it from scratch!
My new article on "Understanding the Self-Attention Mechanism of Large Language Models From Scratch"
 https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html…
------
With the original attention mechanism, I mean the RNN attention mechanism developed by 
@kchonyc
 et al. 2014 in
------
Since self-attention is now everywhere, it's important to understand how it works.
And there is no better and more fun way than coding it from scratch!
My new article on "Understanding the Self-Attention Mechanism of Large Language Models From Scratch"

------
Want to improve the inferencing speed of your PyTorch models? Join us at our webinar in 2 hours (10 am CT).

We'll give a broad overview of techniques, and I'll be doing a code demo & load testing example on stable diffusion
------
@chaton_thomas
, 
@_neilbhatt
, and I will of course also be happy to answer any questions you may have!
------
Whoa, my coworkers are having too much fun! 

Haha, I presume this is the first comedy series centered around machine learning & AI (if we don't count the last season of Silicon Valley)
------
Introducing Boltus: The God of AI 

Binge  all four episodes in this !

Let Boltus teach you how to deploy diffusion models at scale  https://bit.ly/3YvdWE2

(1/4)
------
Since transformers have such a big impact on everyone's research agenda, I wanted to flesh out a short reading list for machine learning researchers and practitioners getting started with large language models:
------
If you're starting a reading group on Large Language Models (LLMs), what is one research paper you will want added to the reading list? 

Researchers: Feel free to recommend your own paper too!
------
I've watched every lesson from this course. Fantastic work from 
@rasbt
!

More lessons are still coming, so today it's a great time to start watching it.

Sebastian, quick note:

The 4th question on Quiz 1.6, Part 1 gives away the answer in the text.
------
FREE course on learning Deep Learning using a modern Open-Source stack:

Deep Learning Fundamentals.

Here is the link: https://lightning.ai/pages/courses/deep-learning-fundamentals/…
------
Regarding Tensor Parallelism: how does that work, how can we just split a matrix or tensor across GPUs?

Like always, it goes back to the fundamental concepts of linear algebra 
------
Training deep neural nets on multiple GPUs has become increasingly common in recent years.
Dividing the workload allows for larger and more complex models to be trained more quickly.

I made a little cheatsheet summarizing the different approaches:
------
Theoretically, we could also use block multiplication, but I don't think this is commonly implemented or used for tensor parallelism (yet)
------
Training deep neural nets on multiple GPUs has become increasingly common in recent years.
Dividing the workload allows for larger and more complex models to be trained more quickly.

I made a little cheatsheet summarizing the different approaches:
------
or, for fine-tuning if 5) is not enough, consider DeepSpeed again:
Trainer(…, strategy=“deepspeed_stage_3") 
Trainer(…, strategy="deepspeed_stage_3_offload")

[4/5]
------
Here are some additional resources (and I will also have a Lightning Trainer intro in Unit 5 of my DL Fundamentals course, dropping next week hopefully):

Multi-GPU training intro: https://pytorch-lightning.readthedocs.io/en/latest/accelerators/gpu_basic.html…

More advanced discussion: https://pytorch-lightning.readthedocs.io/en/latest/accelerators/gpu_intermediate.html…

[5/5]
------
Training a convolutional neural network to 94% accuracy on CIFAR-10 in 8 seconds? 
Yes, I can confirm, it works!

Source code: https://github.com/tysam-code/hlb-CIFAR10…
------
I should mention this is on a single A100, same hardware as suggested in the repo.
------
Not everyone likes using LLMs as writing aids. That's okay. 
But a ban on LLMs as writing aids would be similar to banning calculators as calculation aids -- because calculators can be misused by individuals engaging in financial crimes.
------
The significance of deep learning in the natural and physical sciences is often underappreciated.
Despite the media buzz surrounding conversational chatbots & consumer-facing AI apps, the real impact of AI may lie in its applications in the natural and physical sciences.
------
This week, I have been working on including reader quizzes for the remaining Q & A's -- adding two new questions for each.

I tried to make them challenging, and I may probably end up using some of them as #machinelearning interview questions :P

https://leanpub.com/machine-learning-q-and-ai/…
------
I realized that the textbooks & courses that were most impactful for me all incorporated varying levels of challenging exercises & quizzes to check my understanding. 

So, I've begun adding reader quizzes ...
Have fun & thanks for supporting this work!

https://leanpub.com/machine-learning-q-and-ai/…
------
Somewhat inspired by the recent paper recommendation by 
@Machine01776819
 , "When Deep Learners Change Their Mind: Learning Dynamics for Active Learning"
------
Are LLMs making it easier to write good essays? I’d say NO, they are just shifting the goalpost.

LLMs will help us to write "correctly". But crafting an original and engaging essay that someone wants to read will become more challenging in the future.

https://technologyreview.com/2023/02/07/1067928/why-detecting-ai-generated-text-is-so-difficult-and-what-to-do-about-it/…
------
Ty for the all the positive feedback on the "coding self-attention from scratch" article!

Lots of questions about cross-attention. The beautiful thing about blogs is that it is easy to update and extend!

I hope you like the new sections and figures.
https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html…
------
Since self-attention is now everywhere, it's important to understand how it works.
And there is no better and more fun way than coding it from scratch!
My new article on "Understanding the Self-Attention Mechanism of Large Language Models From Scratch"
 https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html…
------
And now back to writing Machine Learning Q and AI 
------
As a follow-up to my post yesterday, some were wondering why it's called *self*-attention.

That's because it's an attention mechanism for all the elements of the same set.

In contrast, the original attention for RNNs is applied between the encoder and the decoder embeddings.
------
Since self-attention is now everywhere, it's important to understand how it works.
And there is no better and more fun way than coding it from scratch!
My new article on "Understanding the Self-Attention Mechanism of Large Language Models From Scratch"
 https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html…
------
With the original attention mechanism, I mean the RNN attention mechanism developed by 
@kchonyc
 et al. 2014 in
------
Since self-attention is now everywhere, it's important to understand how it works.
And there is no better and more fun way than coding it from scratch!
My new article on "Understanding the Self-Attention Mechanism of Large Language Models From Scratch"

------
Want to improve the inferencing speed of your PyTorch models? Join us at our webinar in 2 hours (10 am CT).

We'll give a broad overview of techniques, and I'll be doing a code demo & load testing example on stable diffusion
------
@chaton_thomas
, 
@_neilbhatt
, and I will of course also be happy to answer any questions you may have!
------
Whoa, my coworkers are having too much fun! 

Haha, I presume this is the first comedy series centered around machine learning & AI (if we don't count the last season of Silicon Valley)
------
Introducing Boltus: The God of AI 

Binge  all four episodes in this !

Let Boltus teach you how to deploy diffusion models at scale  https://bit.ly/3YvdWE2

(1/4)
------
Since transformers have such a big impact on everyone's research agenda, I wanted to flesh out a short reading list for machine learning researchers and practitioners getting started with large language models:
------
If you're starting a reading group on Large Language Models (LLMs), what is one research paper you will want added to the reading list? 

Researchers: Feel free to recommend your own paper too!
------
I've watched every lesson from this course. Fantastic work from 
@rasbt
!

More lessons are still coming, so today it's a great time to start watching it.

Sebastian, quick note:

The 4th question on Quiz 1.6, Part 1 gives away the answer in the text.
------
FREE course on learning Deep Learning using a modern Open-Source stack:

Deep Learning Fundamentals.

Here is the link: https://lightning.ai/pages/courses/deep-learning-fundamentals/…
------
Ahead of AI #5: RevAIval of Ideas is out!

A particularly exciting one since a lot has happened this month!

This issue covers
• Self-supervised learning for CNNs
• Benefits of pretraining LLMs from scratch
• Open source highlights

 https://magazine.sebastianraschka.com/p/ahead-of-ai-5-revaival-of-ideas…

Happy Monday!
------
Tired: train a large language model (LLM) to generate text in human languages

Wired: train an LLM to generate proteins sequences, the language of life

In this recent paper, researchers trained a decoder-style LLM to generate functional proteins:
https://nature.com/articles/s41587-022-01618-2…
------
The impressive thing here is that the researchers didn't just train an LLM on amino acid strings. They went ahead, synthesized full-length genes, and expressed these proteins for real. The artificial proteins exhibited the same level of functionality as natural proteins.
------
All you need is a 13" inch MacBook Air. 

When I was a student, I wrote
- MLxtend
- My first two books
- My blog
- PhD Thesis
- ...

all on that little laptop.
And guess what I am using right now ... 

(ok ok, connected to a external monitor for posture reasons)
------
What’s stopping you from coding like this
------
Here are some additional resources (and I will also have a Lightning Trainer intro in Unit 5 of my DL Fundamentals course, dropping next week hopefully):

Multi-GPU training intro: https://pytorch-lightning.readthedocs.io/en/latest/accelerators/gpu_basic.html…

More advanced discussion: https://pytorch-lightning.readthedocs.io/en/latest/accelerators/gpu_intermediate.html…

[5/5]
------
Training a convolutional neural network to 94% accuracy on CIFAR-10 in 8 seconds? 
Yes, I can confirm, it works!

Source code: https://github.com/tysam-code/hlb-CIFAR10…
------
I should mention this is on a single A100, same hardware as suggested in the repo.
------
Not everyone likes using LLMs as writing aids. That's okay. 
But a ban on LLMs as writing aids would be similar to banning calculators as calculation aids -- because calculators can be misused by individuals engaging in financial crimes.
------
The significance of deep learning in the natural and physical sciences is often underappreciated.
Despite the media buzz surrounding conversational chatbots & consumer-facing AI apps, the real impact of AI may lie in its applications in the natural and physical sciences.
------
This week, I have been working on including reader quizzes for the remaining Q & A's -- adding two new questions for each.

I tried to make them challenging, and I may probably end up using some of them as #machinelearning interview questions :P

https://leanpub.com/machine-learning-q-and-ai/…
------
I realized that the textbooks & courses that were most impactful for me all incorporated varying levels of challenging exercises & quizzes to check my understanding. 

So, I've begun adding reader quizzes ...
Have fun & thanks for supporting this work!

https://leanpub.com/machine-learning-q-and-ai/…
------
Somewhat inspired by the recent paper recommendation by 
@Machine01776819
 , "When Deep Learners Change Their Mind: Learning Dynamics for Active Learning"
------
Are LLMs making it easier to write good essays? I’d say NO, they are just shifting the goalpost.

LLMs will help us to write "correctly". But crafting an original and engaging essay that someone wants to read will become more challenging in the future.

https://technologyreview.com/2023/02/07/1067928/why-detecting-ai-generated-text-is-so-difficult-and-what-to-do-about-it/…
------
Ty for the all the positive feedback on the "coding self-attention from scratch" article!

Lots of questions about cross-attention. The beautiful thing about blogs is that it is easy to update and extend!

I hope you like the new sections and figures.
https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html…
------
Since self-attention is now everywhere, it's important to understand how it works.
And there is no better and more fun way than coding it from scratch!
My new article on "Understanding the Self-Attention Mechanism of Large Language Models From Scratch"
 https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html…
------
And now back to writing Machine Learning Q and AI 
------
As a follow-up to my post yesterday, some were wondering why it's called *self*-attention.

That's because it's an attention mechanism for all the elements of the same set.

In contrast, the original attention for RNNs is applied between the encoder and the decoder embeddings.
------
Since self-attention is now everywhere, it's important to understand how it works.
And there is no better and more fun way than coding it from scratch!
My new article on "Understanding the Self-Attention Mechanism of Large Language Models From Scratch"
 https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html…
------
With the original attention mechanism, I mean the RNN attention mechanism developed by 
@kchonyc
 et al. 2014 in
------
Since self-attention is now everywhere, it's important to understand how it works.
And there is no better and more fun way than coding it from scratch!
My new article on "Understanding the Self-Attention Mechanism of Large Language Models From Scratch"

------
Want to improve the inferencing speed of your PyTorch models? Join us at our webinar in 2 hours (10 am CT).

We'll give a broad overview of techniques, and I'll be doing a code demo & load testing example on stable diffusion
------
@chaton_thomas
, 
@_neilbhatt
, and I will of course also be happy to answer any questions you may have!
------
Whoa, my coworkers are having too much fun! 

Haha, I presume this is the first comedy series centered around machine learning & AI (if we don't count the last season of Silicon Valley)
------
Introducing Boltus: The God of AI 

Binge  all four episodes in this !

Let Boltus teach you how to deploy diffusion models at scale  https://bit.ly/3YvdWE2

(1/4)
------
Since transformers have such a big impact on everyone's research agenda, I wanted to flesh out a short reading list for machine learning researchers and practitioners getting started with large language models:
------
If you're starting a reading group on Large Language Models (LLMs), what is one research paper you will want added to the reading list? 

Researchers: Feel free to recommend your own paper too!
------
I've watched every lesson from this course. Fantastic work from 
@rasbt
!

More lessons are still coming, so today it's a great time to start watching it.

Sebastian, quick note:

The 4th question on Quiz 1.6, Part 1 gives away the answer in the text.
------
FREE course on learning Deep Learning using a modern Open-Source stack:

Deep Learning Fundamentals.

Here is the link: https://lightning.ai/pages/courses/deep-learning-fundamentals/…
------
Ahead of AI #5: RevAIval of Ideas is out!

A particularly exciting one since a lot has happened this month!

This issue covers
• Self-supervised learning for CNNs
• Benefits of pretraining LLMs from scratch
• Open source highlights

 https://magazine.sebastianraschka.com/p/ahead-of-ai-5-revaival-of-ideas…

Happy Monday!
------
Tired: train a large language model (LLM) to generate text in human languages

Wired: train an LLM to generate proteins sequences, the language of life

In this recent paper, researchers trained a decoder-style LLM to generate functional proteins:
https://nature.com/articles/s41587-022-01618-2…
------
The impressive thing here is that the researchers didn't just train an LLM on amino acid strings. They went ahead, synthesized full-length genes, and expressed these proteins for real. The artificial proteins exhibited the same level of functionality as natural proteins.
------
All you need is a 13" inch MacBook Air. 

When I was a student, I wrote
- MLxtend
- My first two books
- My blog
- PhD Thesis
- ...

all on that little laptop.
And guess what I am using right now ... 

(ok ok, connected to a external monitor for posture reasons)
------
What’s stopping you from coding like this
------
If you are working on some hobby projects this weekend and find that submitting PRs has become a slog due to the slow CI pipelines, it's maybe worth giving Ruff a try. Ruff is an extremely fast Python linter written in Rust and can be installed via pip: 

https://github.com/charliermarsh/ruff…
------
I realized that the textbooks & courses that were most impactful for me all incorporated varying levels of challenging exercises & quizzes to check my understanding. 

So, I've begun adding reader quizzes ...
Have fun & thanks for supporting this work!
------
The most dramatic optimization to nanoGPT so far (~25% speedup) is to simply increase vocab size from 50257 to 50304 (nearest multiple of 64). This calculates added useless dimensions but goes down a different kernel path with much higher occupancy. Careful with your Powers of 2.
------
Somewhat inspired by the recent paper recommendation by 
@Machine01776819
 , "When Deep Learners Change Their Mind: Learning Dynamics for Active Learning"
------
Are LLMs making it easier to write good essays? I’d say NO, they are just shifting the goalpost.

LLMs will help us to write "correctly". But crafting an original and engaging essay that someone wants to read will become more challenging in the future.

https://technologyreview.com/2023/02/07/1067928/why-detecting-ai-generated-text-is-so-difficult-and-what-to-do-about-it/…
------
Ty for the all the positive feedback on the "coding self-attention from scratch" article!

Lots of questions about cross-attention. The beautiful thing about blogs is that it is easy to update and extend!

I hope you like the new sections and figures.
https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html…
------
Since self-attention is now everywhere, it's important to understand how it works.
And there is no better and more fun way than coding it from scratch!
My new article on "Understanding the Self-Attention Mechanism of Large Language Models From Scratch"
 https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html…
------
And now back to writing Machine Learning Q and AI 
------
As a follow-up to my post yesterday, some were wondering why it's called *self*-attention.

That's because it's an attention mechanism for all the elements of the same set.

In contrast, the original attention for RNNs is applied between the encoder and the decoder embeddings.
------
Since self-attention is now everywhere, it's important to understand how it works.
And there is no better and more fun way than coding it from scratch!
My new article on "Understanding the Self-Attention Mechanism of Large Language Models From Scratch"
 https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html…
------
With the original attention mechanism, I mean the RNN attention mechanism developed by 
@kchonyc
 et al. 2014 in
------
Since self-attention is now everywhere, it's important to understand how it works.
And there is no better and more fun way than coding it from scratch!
My new article on "Understanding the Self-Attention Mechanism of Large Language Models From Scratch"

------
Want to improve the inferencing speed of your PyTorch models? Join us at our webinar in 2 hours (10 am CT).

We'll give a broad overview of techniques, and I'll be doing a code demo & load testing example on stable diffusion
------
@chaton_thomas
, 
@_neilbhatt
, and I will of course also be happy to answer any questions you may have!
------
Whoa, my coworkers are having too much fun! 

Haha, I presume this is the first comedy series centered around machine learning & AI (if we don't count the last season of Silicon Valley)
------
Introducing Boltus: The God of AI 

Binge  all four episodes in this !

Let Boltus teach you how to deploy diffusion models at scale  https://bit.ly/3YvdWE2

(1/4)
------
Since transformers have such a big impact on everyone's research agenda, I wanted to flesh out a short reading list for machine learning researchers and practitioners getting started with large language models:
------
If you're starting a reading group on Large Language Models (LLMs), what is one research paper you will want added to the reading list? 

Researchers: Feel free to recommend your own paper too!
------
I've watched every lesson from this course. Fantastic work from 
@rasbt
!

More lessons are still coming, so today it's a great time to start watching it.

Sebastian, quick note:

The 4th question on Quiz 1.6, Part 1 gives away the answer in the text.
------
FREE course on learning Deep Learning using a modern Open-Source stack:

Deep Learning Fundamentals.

Here is the link: https://lightning.ai/pages/courses/deep-learning-fundamentals/…
------
Ahead of AI #5: RevAIval of Ideas is out!

A particularly exciting one since a lot has happened this month!

This issue covers
• Self-supervised learning for CNNs
• Benefits of pretraining LLMs from scratch
• Open source highlights

 https://magazine.sebastianraschka.com/p/ahead-of-ai-5-revaival-of-ideas…

Happy Monday!
------
Tired: train a large language model (LLM) to generate text in human languages

Wired: train an LLM to generate proteins sequences, the language of life

In this recent paper, researchers trained a decoder-style LLM to generate functional proteins:
https://nature.com/articles/s41587-022-01618-2…
------
The impressive thing here is that the researchers didn't just train an LLM on amino acid strings. They went ahead, synthesized full-length genes, and expressed these proteins for real. The artificial proteins exhibited the same level of functionality as natural proteins.
------
All you need is a 13" inch MacBook Air. 

When I was a student, I wrote
- MLxtend
- My first two books
- My blog
- PhD Thesis
- ...

all on that little laptop.
And guess what I am using right now ... 

(ok ok, connected to a external monitor for posture reasons)
------
What’s stopping you from coding like this
------
If you are working on some hobby projects this weekend and find that submitting PRs has become a slog due to the slow CI pipelines, it's maybe worth giving Ruff a try. Ruff is an extremely fast Python linter written in Rust and can be installed via pip: 

https://github.com/charliermarsh/ruff…
------
I realized that the textbooks & courses that were most impactful for me all incorporated varying levels of challenging exercises & quizzes to check my understanding. 

So, I've begun adding reader quizzes ...
Have fun & thanks for supporting this work!
------
The most dramatic optimization to nanoGPT so far (~25% speedup) is to simply increase vocab size from 50257 to 50304 (nearest multiple of 64). This calculates added useless dimensions but goes down a different kernel path with much higher occupancy. Careful with your Powers of 2.
------
After putting together a lecture on multi-GPU training paradigms, I thought it might be a good idea to catch up with the recent “Cramming: Training a Language Model on a Single GPU in One Day” paper (https://arxiv.org/abs/2212.14034).

An interesting read with lots of insights!
1/8
------
Btw. a triangular one-cycle learning rate schedules work best.

And dropout was not needed during pretraining due to the large training dataset and 1 epoch training schedule.
7/8
------
Finally, you may wonder why do we need to pretrain LLMs yourself anyway given that pretrained models are available off-the-shelf? Research/study purposes, or you may want to adapt them to new language or domains (think protein or DNA sequences).
8/8
------
LLMs definitely win the headlines this month, again.
------
*Forgot one:

According to report by Semafor, OpenAI is training a ChatGPT model to execute mundane coding tasks -- the headline reads "to replace software engineers".

https://watcher.guru/news/openai-is-training-chatgpt-to-eventually-replace-software-engineers…
------
ML engineers may not worry about overpopulation on MARS atm, but MARS is still worth thinking about in ML contexts:

1. Maintainability (versioning, docs, reproducibility)
2. Adaptability (upd w/o interrupts)
3. Reliability (adversarial attacks)
4. Scalability (hardware scaling)
------
Adding CSI:
- Cost
- Scalability
- Interpretability
------
It will be interesting to see how things play out for competing companies who are pro/con AI-generated content.

(1) 
- Getty Images bans AI and sues
- Shutterstock adds AI and compensates

(2) 
- Science Magazine bans AI content 
- Springer Nature permits authors to use AI
------
As a follow-up to my post yesterday, some were wondering why it's called *self*-attention.

That's because it's an attention mechanism for all the elements of the same set.

In contrast, the original attention for RNNs is applied between the encoder and the decoder embeddings.
------
Since self-attention is now everywhere, it's important to understand how it works.
And there is no better and more fun way than coding it from scratch!
My new article on "Understanding the Self-Attention Mechanism of Large Language Models From Scratch"
 https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html…
------
With the original attention mechanism, I mean the RNN attention mechanism developed by 
@kchonyc
 et al. 2014 in
------
Since self-attention is now everywhere, it's important to understand how it works.
And there is no better and more fun way than coding it from scratch!
My new article on "Understanding the Self-Attention Mechanism of Large Language Models From Scratch"

------
Want to improve the inferencing speed of your PyTorch models? Join us at our webinar in 2 hours (10 am CT).

We'll give a broad overview of techniques, and I'll be doing a code demo & load testing example on stable diffusion
------
@chaton_thomas
, 
@_neilbhatt
, and I will of course also be happy to answer any questions you may have!
------
Whoa, my coworkers are having too much fun! 

Haha, I presume this is the first comedy series centered around machine learning & AI (if we don't count the last season of Silicon Valley)
------
Introducing Boltus: The God of AI 

Binge  all four episodes in this !

Let Boltus teach you how to deploy diffusion models at scale  https://bit.ly/3YvdWE2

(1/4)
------
Since transformers have such a big impact on everyone's research agenda, I wanted to flesh out a short reading list for machine learning researchers and practitioners getting started with large language models:
------
If you're starting a reading group on Large Language Models (LLMs), what is one research paper you will want added to the reading list? 

Researchers: Feel free to recommend your own paper too!
------
I've watched every lesson from this course. Fantastic work from 
@rasbt
!

More lessons are still coming, so today it's a great time to start watching it.

Sebastian, quick note:

The 4th question on Quiz 1.6, Part 1 gives away the answer in the text.
------
FREE course on learning Deep Learning using a modern Open-Source stack:

Deep Learning Fundamentals.

Here is the link: https://lightning.ai/pages/courses/deep-learning-fundamentals/…
------
Ahead of AI #5: RevAIval of Ideas is out!

A particularly exciting one since a lot has happened this month!

This issue covers
• Self-supervised learning for CNNs
• Benefits of pretraining LLMs from scratch
• Open source highlights

 https://magazine.sebastianraschka.com/p/ahead-of-ai-5-revaival-of-ideas…

Happy Monday!
------
Tired: train a large language model (LLM) to generate text in human languages

Wired: train an LLM to generate proteins sequences, the language of life

In this recent paper, researchers trained a decoder-style LLM to generate functional proteins:
https://nature.com/articles/s41587-022-01618-2…
------
The impressive thing here is that the researchers didn't just train an LLM on amino acid strings. They went ahead, synthesized full-length genes, and expressed these proteins for real. The artificial proteins exhibited the same level of functionality as natural proteins.
------
All you need is a 13" inch MacBook Air. 

When I was a student, I wrote
- MLxtend
- My first two books
- My blog
- PhD Thesis
- ...

all on that little laptop.
And guess what I am using right now ... 

(ok ok, connected to a external monitor for posture reasons)
------
What’s stopping you from coding like this
------
If you are working on some hobby projects this weekend and find that submitting PRs has become a slog due to the slow CI pipelines, it's maybe worth giving Ruff a try. Ruff is an extremely fast Python linter written in Rust and can be installed via pip: 

https://github.com/charliermarsh/ruff…
------
I realized that the textbooks & courses that were most impactful for me all incorporated varying levels of challenging exercises & quizzes to check my understanding. 

So, I've begun adding reader quizzes ...
Have fun & thanks for supporting this work!
------
The most dramatic optimization to nanoGPT so far (~25% speedup) is to simply increase vocab size from 50257 to 50304 (nearest multiple of 64). This calculates added useless dimensions but goes down a different kernel path with much higher occupancy. Careful with your Powers of 2.
------
After putting together a lecture on multi-GPU training paradigms, I thought it might be a good idea to catch up with the recent “Cramming: Training a Language Model on a Single GPU in One Day” paper (https://arxiv.org/abs/2212.14034).

An interesting read with lots of insights!
1/8
------
Btw. a triangular one-cycle learning rate schedules work best.

And dropout was not needed during pretraining due to the large training dataset and 1 epoch training schedule.
7/8
------
Finally, you may wonder why do we need to pretrain LLMs yourself anyway given that pretrained models are available off-the-shelf? Research/study purposes, or you may want to adapt them to new language or domains (think protein or DNA sequences).
8/8
------
LLMs definitely win the headlines this month, again.
------
*Forgot one:

According to report by Semafor, OpenAI is training a ChatGPT model to execute mundane coding tasks -- the headline reads "to replace software engineers".

https://watcher.guru/news/openai-is-training-chatgpt-to-eventually-replace-software-engineers…
------
ML engineers may not worry about overpopulation on MARS atm, but MARS is still worth thinking about in ML contexts:

1. Maintainability (versioning, docs, reproducibility)
2. Adaptability (upd w/o interrupts)
3. Reliability (adversarial attacks)
4. Scalability (hardware scaling)
------
Adding CSI:
- Cost
- Scalability
- Interpretability
------
It will be interesting to see how things play out for competing companies who are pro/con AI-generated content.

(1) 
- Getty Images bans AI and sues
- Shutterstock adds AI and compensates

(2) 
- Science Magazine bans AI content 
- Springer Nature permits authors to use AI
------
Sources 2/3:

Springer Nature "says it has no problem with AI being used to help write research — as long as its use is properly disclosed."
------
Sources 3/3 

Science Magazine's policies state that "text generated by ChatGPT (or any other AI tools) cannot be used in the work, nor can figures, images, or graphics be the products of such tools"
------
Thanks to 
@rasbt
 for chatting with me this morning about the challenges of 
@OpenAI
's new AI Text Classifier and other similar tools. Hmm...what would Shakespeare think?
------
Top AI Newsletter #AI Feb, 2023. 

1. The Road to AI We Can Trust (
@GaryMarcus
)

2. Ahead of A.I. (
@rasbt
)

3. Ben Bites (
@bentossell
)

4. The Neuron  (
@nonmayorpete
 )

5. AI Breakfast (
@AiBreakfast
)

6. A.I. Supremacy (
@AISupremacyNews
)

7. Bot Eat Brain (
@AnthonyCastrio
) 
------
What are the different approaches for detecting content generated by LLMs such as ChatGPT? And how do they work and differ?

Let's discuss
(1) The AI Classifier by OpenAI
(2) DetectGPT
(3) GPTZero
(4) Watermarking

1/5
------
@chaton_thomas
, 
@_neilbhatt
, and I will of course also be happy to answer any questions you may have!
------
Whoa, my coworkers are having too much fun! 

Haha, I presume this is the first comedy series centered around machine learning & AI (if we don't count the last season of Silicon Valley)
------
Introducing Boltus: The God of AI 

Binge  all four episodes in this !

Let Boltus teach you how to deploy diffusion models at scale  https://bit.ly/3YvdWE2

(1/4)
------
Since transformers have such a big impact on everyone's research agenda, I wanted to flesh out a short reading list for machine learning researchers and practitioners getting started with large language models:
------
If you're starting a reading group on Large Language Models (LLMs), what is one research paper you will want added to the reading list? 

Researchers: Feel free to recommend your own paper too!
------
I've watched every lesson from this course. Fantastic work from 
@rasbt
!

More lessons are still coming, so today it's a great time to start watching it.

Sebastian, quick note:

The 4th question on Quiz 1.6, Part 1 gives away the answer in the text.
------
FREE course on learning Deep Learning using a modern Open-Source stack:

Deep Learning Fundamentals.

Here is the link: https://lightning.ai/pages/courses/deep-learning-fundamentals/…
------
Ahead of AI #5: RevAIval of Ideas is out!

A particularly exciting one since a lot has happened this month!

This issue covers
• Self-supervised learning for CNNs
• Benefits of pretraining LLMs from scratch
• Open source highlights

 https://magazine.sebastianraschka.com/p/ahead-of-ai-5-revaival-of-ideas…

Happy Monday!
------
Tired: train a large language model (LLM) to generate text in human languages

Wired: train an LLM to generate proteins sequences, the language of life

In this recent paper, researchers trained a decoder-style LLM to generate functional proteins:
https://nature.com/articles/s41587-022-01618-2…
------
The impressive thing here is that the researchers didn't just train an LLM on amino acid strings. They went ahead, synthesized full-length genes, and expressed these proteins for real. The artificial proteins exhibited the same level of functionality as natural proteins.
------
All you need is a 13" inch MacBook Air. 

When I was a student, I wrote
- MLxtend
- My first two books
- My blog
- PhD Thesis
- ...

all on that little laptop.
And guess what I am using right now ... 

(ok ok, connected to a external monitor for posture reasons)
------
What’s stopping you from coding like this
------
If you are working on some hobby projects this weekend and find that submitting PRs has become a slog due to the slow CI pipelines, it's maybe worth giving Ruff a try. Ruff is an extremely fast Python linter written in Rust and can be installed via pip: 

https://github.com/charliermarsh/ruff…
------
I realized that the textbooks & courses that were most impactful for me all incorporated varying levels of challenging exercises & quizzes to check my understanding. 

So, I've begun adding reader quizzes ...
Have fun & thanks for supporting this work!
------
The most dramatic optimization to nanoGPT so far (~25% speedup) is to simply increase vocab size from 50257 to 50304 (nearest multiple of 64). This calculates added useless dimensions but goes down a different kernel path with much higher occupancy. Careful with your Powers of 2.
------
After putting together a lecture on multi-GPU training paradigms, I thought it might be a good idea to catch up with the recent “Cramming: Training a Language Model on a Single GPU in One Day” paper (https://arxiv.org/abs/2212.14034).

An interesting read with lots of insights!
1/8
------
Btw. a triangular one-cycle learning rate schedules work best.

And dropout was not needed during pretraining due to the large training dataset and 1 epoch training schedule.
7/8
------
Finally, you may wonder why do we need to pretrain LLMs yourself anyway given that pretrained models are available off-the-shelf? Research/study purposes, or you may want to adapt them to new language or domains (think protein or DNA sequences).
8/8
------
LLMs definitely win the headlines this month, again.
------
*Forgot one:

According to report by Semafor, OpenAI is training a ChatGPT model to execute mundane coding tasks -- the headline reads "to replace software engineers".

https://watcher.guru/news/openai-is-training-chatgpt-to-eventually-replace-software-engineers…
------
ML engineers may not worry about overpopulation on MARS atm, but MARS is still worth thinking about in ML contexts:

1. Maintainability (versioning, docs, reproducibility)
2. Adaptability (upd w/o interrupts)
3. Reliability (adversarial attacks)
4. Scalability (hardware scaling)
------
Adding CSI:
- Cost
- Scalability
- Interpretability
------
It will be interesting to see how things play out for competing companies who are pro/con AI-generated content.

(1) 
- Getty Images bans AI and sues
- Shutterstock adds AI and compensates

(2) 
- Science Magazine bans AI content 
- Springer Nature permits authors to use AI
------
Sources 2/3:

Springer Nature "says it has no problem with AI being used to help write research — as long as its use is properly disclosed."
------
Sources 3/3 

Science Magazine's policies state that "text generated by ChatGPT (or any other AI tools) cannot be used in the work, nor can figures, images, or graphics be the products of such tools"
------
Thanks to 
@rasbt
 for chatting with me this morning about the challenges of 
@OpenAI
's new AI Text Classifier and other similar tools. Hmm...what would Shakespeare think?
------
Top AI Newsletter #AI Feb, 2023. 

1. The Road to AI We Can Trust (
@GaryMarcus
)

2. Ahead of A.I. (
@rasbt
)

3. Ben Bites (
@bentossell
)

4. The Neuron  (
@nonmayorpete
 )

5. AI Breakfast (
@AiBreakfast
)

6. A.I. Supremacy (
@AISupremacyNews
)

7. Bot Eat Brain (
@AnthonyCastrio
) 
------
What are the different approaches for detecting content generated by LLMs such as ChatGPT? And how do they work and differ?

Let's discuss
(1) The AI Classifier by OpenAI
(2) DetectGPT
(3) GPTZero
(4) Watermarking

1/5
------
Here is a slightly longer version I typed up before truncating it for the Twitter thread: https://sebastianraschka.com/blog/2023/detect-ai.html…
------
(4) Watermarking. The idea is to lower the probas of certain words so that they are less likely being used by the LLMs using an "avoid list".

Limitations: Requires an LLM that has been modified with this avoid list. If the avoid list is known, one can modify AI-generated text.
------
[5/5] Sources:

(1) OpenAI AI classifier: https://platform.openai.com/ai-text-classifier…

(2) Detect GPT: https://arxiv.org/abs/2301.11305v1…

(3) GPTZero: https://gptzero.substack.com/p/gptzero-update-v1…

(4) Watermarking: https://arxiv.org/abs/2301.10226v2…
------
I've watched every lesson from this course. Fantastic work from 
@rasbt
!

More lessons are still coming, so today it's a great time to start watching it.

Sebastian, quick note:

The 4th question on Quiz 1.6, Part 1 gives away the answer in the text.
------
FREE course on learning Deep Learning using a modern Open-Source stack:

Deep Learning Fundamentals.

Here is the link: https://lightning.ai/pages/courses/deep-learning-fundamentals/…
------
Ahead of AI #5: RevAIval of Ideas is out!

A particularly exciting one since a lot has happened this month!

This issue covers
• Self-supervised learning for CNNs
• Benefits of pretraining LLMs from scratch
• Open source highlights

 https://magazine.sebastianraschka.com/p/ahead-of-ai-5-revaival-of-ideas…

Happy Monday!
------
Tired: train a large language model (LLM) to generate text in human languages

Wired: train an LLM to generate proteins sequences, the language of life

In this recent paper, researchers trained a decoder-style LLM to generate functional proteins:
https://nature.com/articles/s41587-022-01618-2…
------
The impressive thing here is that the researchers didn't just train an LLM on amino acid strings. They went ahead, synthesized full-length genes, and expressed these proteins for real. The artificial proteins exhibited the same level of functionality as natural proteins.
------
All you need is a 13" inch MacBook Air. 

When I was a student, I wrote
- MLxtend
- My first two books
- My blog
- PhD Thesis
- ...

all on that little laptop.
And guess what I am using right now ... 

(ok ok, connected to a external monitor for posture reasons)
------
What’s stopping you from coding like this
------
If you are working on some hobby projects this weekend and find that submitting PRs has become a slog due to the slow CI pipelines, it's maybe worth giving Ruff a try. Ruff is an extremely fast Python linter written in Rust and can be installed via pip: 

https://github.com/charliermarsh/ruff…
------
I realized that the textbooks & courses that were most impactful for me all incorporated varying levels of challenging exercises & quizzes to check my understanding. 

So, I've begun adding reader quizzes ...
Have fun & thanks for supporting this work!
------
The most dramatic optimization to nanoGPT so far (~25% speedup) is to simply increase vocab size from 50257 to 50304 (nearest multiple of 64). This calculates added useless dimensions but goes down a different kernel path with much higher occupancy. Careful with your Powers of 2.
------
After putting together a lecture on multi-GPU training paradigms, I thought it might be a good idea to catch up with the recent “Cramming: Training a Language Model on a Single GPU in One Day” paper (https://arxiv.org/abs/2212.14034).

An interesting read with lots of insights!
1/8
------
Btw. a triangular one-cycle learning rate schedules work best.

And dropout was not needed during pretraining due to the large training dataset and 1 epoch training schedule.
7/8
------
Finally, you may wonder why do we need to pretrain LLMs yourself anyway given that pretrained models are available off-the-shelf? Research/study purposes, or you may want to adapt them to new language or domains (think protein or DNA sequences).
8/8
------
LLMs definitely win the headlines this month, again.
------
*Forgot one:

According to report by Semafor, OpenAI is training a ChatGPT model to execute mundane coding tasks -- the headline reads "to replace software engineers".

https://watcher.guru/news/openai-is-training-chatgpt-to-eventually-replace-software-engineers…
------
It will be interesting to see how things play out for competing companies who are pro/con AI-generated content.

(1) 
- Getty Images bans AI and sues
- Shutterstock adds AI and compensates

(2) 
- Science Magazine bans AI content 
- Springer Nature permits authors to use AI
------
Sources 2/3:

Springer Nature "says it has no problem with AI being used to help write research — as long as its use is properly disclosed."
------
Sources 3/3 

Science Magazine's policies state that "text generated by ChatGPT (or any other AI tools) cannot be used in the work, nor can figures, images, or graphics be the products of such tools"
------
Thanks to 
@rasbt
 for chatting with me this morning about the challenges of 
@OpenAI
's new AI Text Classifier and other similar tools. Hmm...what would Shakespeare think?
------
Top AI Newsletter #AI Feb, 2023. 

1. The Road to AI We Can Trust (
@GaryMarcus
)

2. Ahead of A.I. (
@rasbt
)

3. Ben Bites (
@bentossell
)

4. The Neuron  (
@nonmayorpete
 )

5. AI Breakfast (
@AiBreakfast
)

6. A.I. Supremacy (
@AISupremacyNews
)

7. Bot Eat Brain (
@AnthonyCastrio
) 
------
What are the different approaches for detecting content generated by LLMs such as ChatGPT? And how do they work and differ?

Let's discuss
(1) The AI Classifier by OpenAI
(2) DetectGPT
(3) GPTZero
(4) Watermarking

1/5
------
Here is a slightly longer version I typed up before truncating it for the Twitter thread: https://sebastianraschka.com/blog/2023/detect-ai.html…
------
(4) Watermarking. The idea is to lower the probas of certain words so that they are less likely being used by the LLMs using an "avoid list".

Limitations: Requires an LLM that has been modified with this avoid list. If the avoid list is known, one can modify AI-generated text.
------
[5/5] Sources:

(1) OpenAI AI classifier: https://platform.openai.com/ai-text-classifier…

(2) Detect GPT: https://arxiv.org/abs/2301.11305v1…

(3) GPTZero: https://gptzero.substack.com/p/gptzero-update-v1…

(4) Watermarking: https://arxiv.org/abs/2301.10226v2…
------
(3) GPTZero computes perplexity values (related to log-probas of the texts). GPTZero assumes the lower perplexity are more likely generated by an AI.

Limitations: see DetectGPT above. Furthermore, GPTZero only approximates the perplexity values by using a linear model.
------
OpenAI just launched the "AI Text Classifier" to identify texts generated by AI. 
Tried it, and IT DOES NOT WORK.
https://platform.openai.com/ai-text-classifier…

Using my Python ML book published in 2015:

1) 
@randal_olson
's foreword: unclear
2) my preface: possibly AI
3) paragraph from Ch1: likely AI
------
Plot twist: after ChatGPT making your homework easier, it's now harder than ever before. 

You now have to rephrase your own words several times until they don't look AI-generated anymore before you can submit.
------
If you deploy a model like this, pls share a confusioon matrix.
Models like this can cause real-world harm due to educators adopting this for grading. So let's add some transparency about False Positives and False Negatives.
------
Hypothesis: can't feed anything created before 2022, because it was all training data.
------
First page from Shakespeare's Macbeth. What!?
------
I mean, this is a funny example, but I already feel bad for students who might get penalized for their essays in the future because of this.
------
Had a fun chat with 
@prateekvjoshi
 on the Infinite Machine Learning podcast 

We chatted about frameworks for writing books, the current state of AI, trends, and our favorite AI use-cases 

Apple Podcasts: https://podcasts.apple.com/us/podcast/state-of-play-in-ai-research-sebastian-raschka/id1615142314?i=1000597288104…

Spotify: https://open.spotify.com/episode/18VfoM9O6xjv9ksb80N11w…
------
We have @rasbt on the Infinite ML pod today. We talk about the state of play in AI research. 

We cover a range of topics including:

- Framework for writing great books 

- Current state of AI research

- Protein structure

- Killer apps

- AI infrastructure

- Generative AI
------
"We’ve upgraded the ChatGPT model with improved factuality and mathematical capabilities." (https://help.openai.com/en/articles/6825453-chatgpt-release-notes…)

Close!
------
BREAKING!! And update to ChatGPT just launched.

https://help.openai.com/en/articles/6825453-chatgpt-release-notes…
------
All you need is a 13" inch MacBook Air. 

When I was a student, I wrote
- MLxtend
- My first two books
- My blog
- PhD Thesis
- ...

all on that little laptop.
And guess what I am using right now ... 

(ok ok, connected to a external monitor for posture reasons)
------
What’s stopping you from coding like this
------
If you are working on some hobby projects this weekend and find that submitting PRs has become a slog due to the slow CI pipelines, it's maybe worth giving Ruff a try. Ruff is an extremely fast Python linter written in Rust and can be installed via pip: 

https://github.com/charliermarsh/ruff…
------
I realized that the textbooks & courses that were most impactful for me all incorporated varying levels of challenging exercises & quizzes to check my understanding. 

So, I've begun adding reader quizzes ...
Have fun & thanks for supporting this work!
------
The most dramatic optimization to nanoGPT so far (~25% speedup) is to simply increase vocab size from 50257 to 50304 (nearest multiple of 64). This calculates added useless dimensions but goes down a different kernel path with much higher occupancy. Careful with your Powers of 2.
------
After putting together a lecture on multi-GPU training paradigms, I thought it might be a good idea to catch up with the recent “Cramming: Training a Language Model on a Single GPU in One Day” paper (https://arxiv.org/abs/2212.14034).

An interesting read with lots of insights!
1/8
------
Btw. a triangular one-cycle learning rate schedules work best.

And dropout was not needed during pretraining due to the large training dataset and 1 epoch training schedule.
7/8
------
Finally, you may wonder why do we need to pretrain LLMs yourself anyway given that pretrained models are available off-the-shelf? Research/study purposes, or you may want to adapt them to new language or domains (think protein or DNA sequences).
8/8
------
LLMs definitely win the headlines this month, again.
------
*Forgot one:

According to report by Semafor, OpenAI is training a ChatGPT model to execute mundane coding tasks -- the headline reads "to replace software engineers".

https://watcher.guru/news/openai-is-training-chatgpt-to-eventually-replace-software-engineers…
------
It will be interesting to see how things play out for competing companies who are pro/con AI-generated content.

(1) 
- Getty Images bans AI and sues
- Shutterstock adds AI and compensates

(2) 
- Science Magazine bans AI content 
- Springer Nature permits authors to use AI
------
Sources 2/3:

Springer Nature "says it has no problem with AI being used to help write research — as long as its use is properly disclosed."
------
Sources 3/3 

Science Magazine's policies state that "text generated by ChatGPT (or any other AI tools) cannot be used in the work, nor can figures, images, or graphics be the products of such tools"
------
Thanks to 
@rasbt
 for chatting with me this morning about the challenges of 
@OpenAI
's new AI Text Classifier and other similar tools. Hmm...what would Shakespeare think?
------
Top AI Newsletter #AI Feb, 2023. 

1. The Road to AI We Can Trust (
@GaryMarcus
)

2. Ahead of A.I. (
@rasbt
)

3. Ben Bites (
@bentossell
)

4. The Neuron  (
@nonmayorpete
 )

5. AI Breakfast (
@AiBreakfast
)

6. A.I. Supremacy (
@AISupremacyNews
)

7. Bot Eat Brain (
@AnthonyCastrio
) 
------
What are the different approaches for detecting content generated by LLMs such as ChatGPT? And how do they work and differ?

Let's discuss
(1) The AI Classifier by OpenAI
(2) DetectGPT
(3) GPTZero
(4) Watermarking

1/5
------
Here is a slightly longer version I typed up before truncating it for the Twitter thread: https://sebastianraschka.com/blog/2023/detect-ai.html…
------
(4) Watermarking. The idea is to lower the probas of certain words so that they are less likely being used by the LLMs using an "avoid list".

Limitations: Requires an LLM that has been modified with this avoid list. If the avoid list is known, one can modify AI-generated text.
------
[5/5] Sources:

(1) OpenAI AI classifier: https://platform.openai.com/ai-text-classifier…

(2) Detect GPT: https://arxiv.org/abs/2301.11305v1…

(3) GPTZero: https://gptzero.substack.com/p/gptzero-update-v1…

(4) Watermarking: https://arxiv.org/abs/2301.10226v2…
------
(3) GPTZero computes perplexity values (related to log-probas of the texts). GPTZero assumes the lower perplexity are more likely generated by an AI.

Limitations: see DetectGPT above. Furthermore, GPTZero only approximates the perplexity values by using a linear model.
------
OpenAI just launched the "AI Text Classifier" to identify texts generated by AI. 
Tried it, and IT DOES NOT WORK.
https://platform.openai.com/ai-text-classifier…

Using my Python ML book published in 2015:

1) 
@randal_olson
's foreword: unclear
2) my preface: possibly AI
3) paragraph from Ch1: likely AI
------
Plot twist: after ChatGPT making your homework easier, it's now harder than ever before. 

You now have to rephrase your own words several times until they don't look AI-generated anymore before you can submit.
------
If you deploy a model like this, pls share a confusioon matrix.
Models like this can cause real-world harm due to educators adopting this for grading. So let's add some transparency about False Positives and False Negatives.
------
Hypothesis: can't feed anything created before 2022, because it was all training data.
------
First page from Shakespeare's Macbeth. What!?
------
I mean, this is a funny example, but I already feel bad for students who might get penalized for their essays in the future because of this.
------
Had a fun chat with 
@prateekvjoshi
 on the Infinite Machine Learning podcast 

We chatted about frameworks for writing books, the current state of AI, trends, and our favorite AI use-cases 

Apple Podcasts: https://podcasts.apple.com/us/podcast/state-of-play-in-ai-research-sebastian-raschka/id1615142314?i=1000597288104…

Spotify: https://open.spotify.com/episode/18VfoM9O6xjv9ksb80N11w…
------
We have @rasbt on the Infinite ML pod today. We talk about the state of play in AI research. 

We cover a range of topics including:

- Framework for writing great books 

- Current state of AI research

- Protein structure

- Killer apps

- AI infrastructure

- Generative AI
------
"We’ve upgraded the ChatGPT model with improved factuality and mathematical capabilities." (https://help.openai.com/en/articles/6825453-chatgpt-release-notes…)

Close!
------
BREAKING!! And update to ChatGPT just launched.

https://help.openai.com/en/articles/6825453-chatgpt-release-notes…
------
"We’ve upgraded the ChatGPT model with improved factuality and mathematical capabilities." (https://help.openai.com/en/articles/6825453-chatgpt-release-notes…)

Close!
------
BREAKING!! And update to ChatGPT just launched.

https://help.openai.com/en/articles/6825453-chatgpt-release-notes…
------
By all means, use chatGPT to improve your writing. It's great for that. But math is not its strength. That's not how neural networks work.
------
Great write up! We studied a similar problem too. We looked at when and what augmentations help for various classification models. Link to the paper -
------
Excited to be giving a live webinar next week where we talk more about how we 3x-ed the inference speed of Stable Diffusion in PyTorch.

Do you have any questions you’d like us to answer & chat about? Happy to include those!!
------
Went down the rabbit hole of comparing *automatic* image augmentation methods in 
@pytorch
.
TrivialAugment -- the simplest solution -- seems to be the clear winner, boosting the test set accuracy on CIFAR-10 by 15%!

A more detailed write-up in my blog:
------
Btw. a triangular one-cycle learning rate schedules work best.

And dropout was not needed during pretraining due to the large training dataset and 1 epoch training schedule.
7/8
------
Finally, you may wonder why do we need to pretrain LLMs yourself anyway given that pretrained models are available off-the-shelf? Research/study purposes, or you may want to adapt them to new language or domains (think protein or DNA sequences).
8/8
------
LLMs definitely win the headlines this month, again.
------
*Forgot one:

According to report by Semafor, OpenAI is training a ChatGPT model to execute mundane coding tasks -- the headline reads "to replace software engineers".

https://watcher.guru/news/openai-is-training-chatgpt-to-eventually-replace-software-engineers…
------
It will be interesting to see how things play out for competing companies who are pro/con AI-generated content.

(1) 
- Getty Images bans AI and sues
- Shutterstock adds AI and compensates

(2) 
- Science Magazine bans AI content 
- Springer Nature permits authors to use AI
------
Sources 2/3:

Springer Nature "says it has no problem with AI being used to help write research — as long as its use is properly disclosed."
------
Sources 3/3 

Science Magazine's policies state that "text generated by ChatGPT (or any other AI tools) cannot be used in the work, nor can figures, images, or graphics be the products of such tools"
------
Thanks to 
@rasbt
 for chatting with me this morning about the challenges of 
@OpenAI
's new AI Text Classifier and other similar tools. Hmm...what would Shakespeare think?
------
Top AI Newsletter #AI Feb, 2023. 

1. The Road to AI We Can Trust (
@GaryMarcus
)

2. Ahead of A.I. (
@rasbt
)

3. Ben Bites (
@bentossell
)

4. The Neuron  (
@nonmayorpete
 )

5. AI Breakfast (
@AiBreakfast
)

6. A.I. Supremacy (
@AISupremacyNews
)

7. Bot Eat Brain (
@AnthonyCastrio
) 
------
What are the different approaches for detecting content generated by LLMs such as ChatGPT? And how do they work and differ?

Let's discuss
(1) The AI Classifier by OpenAI
(2) DetectGPT
(3) GPTZero
(4) Watermarking

1/5
------
Here is a slightly longer version I typed up before truncating it for the Twitter thread: https://sebastianraschka.com/blog/2023/detect-ai.html…
------
(4) Watermarking. The idea is to lower the probas of certain words so that they are less likely being used by the LLMs using an "avoid list".

Limitations: Requires an LLM that has been modified with this avoid list. If the avoid list is known, one can modify AI-generated text.
------
[5/5] Sources:

(1) OpenAI AI classifier: https://platform.openai.com/ai-text-classifier…

(2) Detect GPT: https://arxiv.org/abs/2301.11305v1…

(3) GPTZero: https://gptzero.substack.com/p/gptzero-update-v1…

(4) Watermarking: https://arxiv.org/abs/2301.10226v2…
------
(3) GPTZero computes perplexity values (related to log-probas of the texts). GPTZero assumes the lower perplexity are more likely generated by an AI.

Limitations: see DetectGPT above. Furthermore, GPTZero only approximates the perplexity values by using a linear model.
------
OpenAI just launched the "AI Text Classifier" to identify texts generated by AI. 
Tried it, and IT DOES NOT WORK.
https://platform.openai.com/ai-text-classifier…

Using my Python ML book published in 2015:

1) 
@randal_olson
's foreword: unclear
2) my preface: possibly AI
3) paragraph from Ch1: likely AI
------
Plot twist: after ChatGPT making your homework easier, it's now harder than ever before. 

You now have to rephrase your own words several times until they don't look AI-generated anymore before you can submit.
------
If you deploy a model like this, pls share a confusioon matrix.
Models like this can cause real-world harm due to educators adopting this for grading. So let's add some transparency about False Positives and False Negatives.
------
Hypothesis: can't feed anything created before 2022, because it was all training data.
------
First page from Shakespeare's Macbeth. What!?
------
I mean, this is a funny example, but I already feel bad for students who might get penalized for their essays in the future because of this.
------
Had a fun chat with 
@prateekvjoshi
 on the Infinite Machine Learning podcast 

We chatted about frameworks for writing books, the current state of AI, trends, and our favorite AI use-cases 

Apple Podcasts: https://podcasts.apple.com/us/podcast/state-of-play-in-ai-research-sebastian-raschka/id1615142314?i=1000597288104…

Spotify: https://open.spotify.com/episode/18VfoM9O6xjv9ksb80N11w…
------
We have @rasbt on the Infinite ML pod today. We talk about the state of play in AI research. 

We cover a range of topics including:

- Framework for writing great books 

- Current state of AI research

- Protein structure

- Killer apps

- AI infrastructure

- Generative AI
------
"We’ve upgraded the ChatGPT model with improved factuality and mathematical capabilities." (https://help.openai.com/en/articles/6825453-chatgpt-release-notes…)

Close!
------
BREAKING!! And update to ChatGPT just launched.

https://help.openai.com/en/articles/6825453-chatgpt-release-notes…
------
"We’ve upgraded the ChatGPT model with improved factuality and mathematical capabilities." (https://help.openai.com/en/articles/6825453-chatgpt-release-notes…)

Close!
------
BREAKING!! And update to ChatGPT just launched.

https://help.openai.com/en/articles/6825453-chatgpt-release-notes…
------
By all means, use chatGPT to improve your writing. It's great for that. But math is not its strength. That's not how neural networks work.
------
Great write up! We studied a similar problem too. We looked at when and what augmentations help for various classification models. Link to the paper -
------
Excited to be giving a live webinar next week where we talk more about how we 3x-ed the inference speed of Stable Diffusion in PyTorch.

Do you have any questions you’d like us to answer & chat about? Happy to include those!!
------
Went down the rabbit hole of comparing *automatic* image augmentation methods in 
@pytorch
.
TrivialAugment -- the simplest solution -- seems to be the clear winner, boosting the test set accuracy on CIFAR-10 by 15%!

A more detailed write-up in my blog:
------
Quite the productive weekend! Lots of new content added to Machine Learning Q and AI:  https://leanpub.com/machine-learning-q-and-ai/…

Q21. Stateless vs Stateful Training
Q27. Proper Metrics
Q30. Limited Labeled Data
------
Once in a while, I stumble upon some nice little feature in the 
@PyTorch
  library that I haven't seen before. 

Have you tried AutoAugment, yet?

Just gave it a try: takes a few epochs, but that's a 10% accuracy improvement in this simple example case
https://github.com/rasbt/deeplearning-models/tree/master/pytorch-lightning_ipynb/data-augmentation/autoaugment…
------
Or even better: TrivialAugment! 

(Notebook here if you want to give it a try: https://github.com/rasbt/deeplearning-models/tree/master/pytorch-lightning_ipynb/data-augmentation/autoaugment…)

Thanks for the suggestion 
@SamuelMullr
!
------
Sources 3/3 

Science Magazine's policies state that "text generated by ChatGPT (or any other AI tools) cannot be used in the work, nor can figures, images, or graphics be the products of such tools"
------
Thanks to 
@rasbt
 for chatting with me this morning about the challenges of 
@OpenAI
's new AI Text Classifier and other similar tools. Hmm...what would Shakespeare think?
------
Top AI Newsletter #AI Feb, 2023. 

1. The Road to AI We Can Trust (
@GaryMarcus
)

2. Ahead of A.I. (
@rasbt
)

3. Ben Bites (
@bentossell
)

4. The Neuron  (
@nonmayorpete
 )

5. AI Breakfast (
@AiBreakfast
)

6. A.I. Supremacy (
@AISupremacyNews
)

7. Bot Eat Brain (
@AnthonyCastrio
) 
------
What are the different approaches for detecting content generated by LLMs such as ChatGPT? And how do they work and differ?

Let's discuss
(1) The AI Classifier by OpenAI
(2) DetectGPT
(3) GPTZero
(4) Watermarking

1/5
------
Here is a slightly longer version I typed up before truncating it for the Twitter thread: https://sebastianraschka.com/blog/2023/detect-ai.html…
------
(4) Watermarking. The idea is to lower the probas of certain words so that they are less likely being used by the LLMs using an "avoid list".

Limitations: Requires an LLM that has been modified with this avoid list. If the avoid list is known, one can modify AI-generated text.
------
[5/5] Sources:

(1) OpenAI AI classifier: https://platform.openai.com/ai-text-classifier…

(2) Detect GPT: https://arxiv.org/abs/2301.11305v1…

(3) GPTZero: https://gptzero.substack.com/p/gptzero-update-v1…

(4) Watermarking: https://arxiv.org/abs/2301.10226v2…
------
(3) GPTZero computes perplexity values (related to log-probas of the texts). GPTZero assumes the lower perplexity are more likely generated by an AI.

Limitations: see DetectGPT above. Furthermore, GPTZero only approximates the perplexity values by using a linear model.
------
OpenAI just launched the "AI Text Classifier" to identify texts generated by AI. 
Tried it, and IT DOES NOT WORK.
https://platform.openai.com/ai-text-classifier…

Using my Python ML book published in 2015:

1) 
@randal_olson
's foreword: unclear
2) my preface: possibly AI
3) paragraph from Ch1: likely AI
------
Plot twist: after ChatGPT making your homework easier, it's now harder than ever before. 

You now have to rephrase your own words several times until they don't look AI-generated anymore before you can submit.
------
If you deploy a model like this, pls share a confusioon matrix.
Models like this can cause real-world harm due to educators adopting this for grading. So let's add some transparency about False Positives and False Negatives.
------
Hypothesis: can't feed anything created before 2022, because it was all training data.
------
First page from Shakespeare's Macbeth. What!?
------
I mean, this is a funny example, but I already feel bad for students who might get penalized for their essays in the future because of this.
------
Had a fun chat with 
@prateekvjoshi
 on the Infinite Machine Learning podcast 

We chatted about frameworks for writing books, the current state of AI, trends, and our favorite AI use-cases 

Apple Podcasts: https://podcasts.apple.com/us/podcast/state-of-play-in-ai-research-sebastian-raschka/id1615142314?i=1000597288104…

Spotify: https://open.spotify.com/episode/18VfoM9O6xjv9ksb80N11w…
------
We have @rasbt on the Infinite ML pod today. We talk about the state of play in AI research. 

We cover a range of topics including:

- Framework for writing great books 

- Current state of AI research

- Protein structure

- Killer apps

- AI infrastructure

- Generative AI
------
"We’ve upgraded the ChatGPT model with improved factuality and mathematical capabilities." (https://help.openai.com/en/articles/6825453-chatgpt-release-notes…)

Close!
------
BREAKING!! And update to ChatGPT just launched.

https://help.openai.com/en/articles/6825453-chatgpt-release-notes…
------
"We’ve upgraded the ChatGPT model with improved factuality and mathematical capabilities." (https://help.openai.com/en/articles/6825453-chatgpt-release-notes…)

Close!
------
BREAKING!! And update to ChatGPT just launched.

https://help.openai.com/en/articles/6825453-chatgpt-release-notes…
------
By all means, use chatGPT to improve your writing. It's great for that. But math is not its strength. That's not how neural networks work.
------
Great write up! We studied a similar problem too. We looked at when and what augmentations help for various classification models. Link to the paper -
------
Excited to be giving a live webinar next week where we talk more about how we 3x-ed the inference speed of Stable Diffusion in PyTorch.

Do you have any questions you’d like us to answer & chat about? Happy to include those!!
------
Went down the rabbit hole of comparing *automatic* image augmentation methods in 
@pytorch
.
TrivialAugment -- the simplest solution -- seems to be the clear winner, boosting the test set accuracy on CIFAR-10 by 15%!

A more detailed write-up in my blog:
------
Quite the productive weekend! Lots of new content added to Machine Learning Q and AI:  https://leanpub.com/machine-learning-q-and-ai/…

Q21. Stateless vs Stateful Training
Q27. Proper Metrics
Q30. Limited Labeled Data
------
Once in a while, I stumble upon some nice little feature in the 
@PyTorch
  library that I haven't seen before. 

Have you tried AutoAugment, yet?

Just gave it a try: takes a few epochs, but that's a 10% accuracy improvement in this simple example case
https://github.com/rasbt/deeplearning-models/tree/master/pytorch-lightning_ipynb/data-augmentation/autoaugment…
------
Or even better: TrivialAugment! 

(Notebook here if you want to give it a try: https://github.com/rasbt/deeplearning-models/tree/master/pytorch-lightning_ipynb/data-augmentation/autoaugment…)

Thanks for the suggestion 
@SamuelMullr
!
------
Whoa, just looking up my book on Amazon and saw it's now #1 in Natural language processing. (https://amazon.com/Machine-Learning-PyTorch-Scikit-Learn-learning/dp/1801819319/ref=tmm_pap_swatch_0…)

Yeah, it's not an NLP book, but there is an extensive transformer chapter that was a lot of work. Glad it's so well received!
------
AutoAugment paper:
https://arxiv.org/abs/1805.09501

transforms.AutoAugment docs:
https://pytorch.org/vision/main/generated/torchvision.transforms.AutoAugment.html…
------
The whimsical elements on GitHub always bring a sense of delight.
------
I outlined 9 different machine learning strategies, but which one to use? 
An attempt to bring a bit of structure into this:
------
Derivatives of regular supervised learning when you don't have enough labeled data:

1.  Transfer learning
2.  Self-supervised learning
3.  Semi-supervised learning
4.  Few-shot learning
5.  Meta-learning
6.  Active learning
7.  Weakly supervised learning
8.  Multi-task learning
------
Totally missed that one node had 3 arrows above. Just cleaned that up:
------
[5/5] Sources:

(1) OpenAI AI classifier: https://platform.openai.com/ai-text-classifier…

(2) Detect GPT: https://arxiv.org/abs/2301.11305v1…

(3) GPTZero: https://gptzero.substack.com/p/gptzero-update-v1…

(4) Watermarking: https://arxiv.org/abs/2301.10226v2…
------
(3) GPTZero computes perplexity values (related to log-probas of the texts). GPTZero assumes the lower perplexity are more likely generated by an AI.

Limitations: see DetectGPT above. Furthermore, GPTZero only approximates the perplexity values by using a linear model.
------
OpenAI just launched the "AI Text Classifier" to identify texts generated by AI. 
Tried it, and IT DOES NOT WORK.
https://platform.openai.com/ai-text-classifier…

Using my Python ML book published in 2015:

1) 
@randal_olson
's foreword: unclear
2) my preface: possibly AI
3) paragraph from Ch1: likely AI
------
Plot twist: after ChatGPT making your homework easier, it's now harder than ever before. 

You now have to rephrase your own words several times until they don't look AI-generated anymore before you can submit.
------
If you deploy a model like this, pls share a confusioon matrix.
Models like this can cause real-world harm due to educators adopting this for grading. So let's add some transparency about False Positives and False Negatives.
------
Hypothesis: can't feed anything created before 2022, because it was all training data.
------
First page from Shakespeare's Macbeth. What!?
------
I mean, this is a funny example, but I already feel bad for students who might get penalized for their essays in the future because of this.
------
Had a fun chat with 
@prateekvjoshi
 on the Infinite Machine Learning podcast 

We chatted about frameworks for writing books, the current state of AI, trends, and our favorite AI use-cases 

Apple Podcasts: https://podcasts.apple.com/us/podcast/state-of-play-in-ai-research-sebastian-raschka/id1615142314?i=1000597288104…

Spotify: https://open.spotify.com/episode/18VfoM9O6xjv9ksb80N11w…
------
We have @rasbt on the Infinite ML pod today. We talk about the state of play in AI research. 

We cover a range of topics including:

- Framework for writing great books 

- Current state of AI research

- Protein structure

- Killer apps

- AI infrastructure

- Generative AI
------
"We’ve upgraded the ChatGPT model with improved factuality and mathematical capabilities." (https://help.openai.com/en/articles/6825453-chatgpt-release-notes…)

Close!
------
BREAKING!! And update to ChatGPT just launched.

https://help.openai.com/en/articles/6825453-chatgpt-release-notes…
------
"We’ve upgraded the ChatGPT model with improved factuality and mathematical capabilities." (https://help.openai.com/en/articles/6825453-chatgpt-release-notes…)

Close!
------
BREAKING!! And update to ChatGPT just launched.

https://help.openai.com/en/articles/6825453-chatgpt-release-notes…
------
By all means, use chatGPT to improve your writing. It's great for that. But math is not its strength. That's not how neural networks work.
------
Great write up! We studied a similar problem too. We looked at when and what augmentations help for various classification models. Link to the paper -
------
Excited to be giving a live webinar next week where we talk more about how we 3x-ed the inference speed of Stable Diffusion in PyTorch.

Do you have any questions you’d like us to answer & chat about? Happy to include those!!
------
Went down the rabbit hole of comparing *automatic* image augmentation methods in 
@pytorch
.
TrivialAugment -- the simplest solution -- seems to be the clear winner, boosting the test set accuracy on CIFAR-10 by 15%!

A more detailed write-up in my blog:
------
Quite the productive weekend! Lots of new content added to Machine Learning Q and AI:  https://leanpub.com/machine-learning-q-and-ai/…

Q21. Stateless vs Stateful Training
Q27. Proper Metrics
Q30. Limited Labeled Data
------
Once in a while, I stumble upon some nice little feature in the 
@PyTorch
  library that I haven't seen before. 

Have you tried AutoAugment, yet?

Just gave it a try: takes a few epochs, but that's a 10% accuracy improvement in this simple example case
https://github.com/rasbt/deeplearning-models/tree/master/pytorch-lightning_ipynb/data-augmentation/autoaugment…
------
Or even better: TrivialAugment! 

(Notebook here if you want to give it a try: https://github.com/rasbt/deeplearning-models/tree/master/pytorch-lightning_ipynb/data-augmentation/autoaugment…)

Thanks for the suggestion 
@SamuelMullr
!
------
Whoa, just looking up my book on Amazon and saw it's now #1 in Natural language processing. (https://amazon.com/Machine-Learning-PyTorch-Scikit-Learn-learning/dp/1801819319/ref=tmm_pap_swatch_0…)

Yeah, it's not an NLP book, but there is an extensive transformer chapter that was a lot of work. Glad it's so well received!
------
AutoAugment paper:
https://arxiv.org/abs/1805.09501

transforms.AutoAugment docs:
https://pytorch.org/vision/main/generated/torchvision.transforms.AutoAugment.html…
------
The whimsical elements on GitHub always bring a sense of delight.
------
I outlined 9 different machine learning strategies, but which one to use? 
An attempt to bring a bit of structure into this:
------
Derivatives of regular supervised learning when you don't have enough labeled data:

1.  Transfer learning
2.  Self-supervised learning
3.  Semi-supervised learning
4.  Few-shot learning
5.  Meta-learning
6.  Active learning
7.  Weakly supervised learning
8.  Multi-task learning
------
Totally missed that one node had 3 arrows above. Just cleaned that up:
------
*  The black boxes are not terminal nodes but arch back to "Evaluate model performance" (arrows omitted to reduce clutter)

** Techniques such as data augmentation, multi-task, and multi-modal learning could be combined with all these strategies
------
ChatGPT is not a free university.

It’s an amazing tool that can increase your productivity, but “teaching” is not its strength.

If you want to learn coding, copywriting, finances, or any other topic, there are great educators out there.

ChatGPT is not one of them.
------
Had a fun podcast recording & chat with

@prateekvjoshi
 yesterday!

Realized on my morning walk that I totally forgot my currently fav AI use case:
Transcribing my voice recordings!

Earlier this year, I got a simple voice recorder that I use to take notes when driving/walking.
------
Here's a link to the code if you want to give it a try: https://gist.github.com/rasbt/4bfd783328171972d7aaf77e83489c28…

And if you prefer a nice and polished UI, we recently launched the Echo App 
@LightningAI
 on top of Whisper earlier this year: https://lightning.ai/echo
------
I mean, this is a funny example, but I already feel bad for students who might get penalized for their essays in the future because of this.
------
Had a fun chat with 
@prateekvjoshi
 on the Infinite Machine Learning podcast 

We chatted about frameworks for writing books, the current state of AI, trends, and our favorite AI use-cases 

Apple Podcasts: https://podcasts.apple.com/us/podcast/state-of-play-in-ai-research-sebastian-raschka/id1615142314?i=1000597288104…

Spotify: https://open.spotify.com/episode/18VfoM9O6xjv9ksb80N11w…
------
We have @rasbt on the Infinite ML pod today. We talk about the state of play in AI research. 

We cover a range of topics including:

- Framework for writing great books 

- Current state of AI research

- Protein structure

- Killer apps

- AI infrastructure

- Generative AI
------
"We’ve upgraded the ChatGPT model with improved factuality and mathematical capabilities." (https://help.openai.com/en/articles/6825453-chatgpt-release-notes…)

Close!
------
BREAKING!! And update to ChatGPT just launched.

https://help.openai.com/en/articles/6825453-chatgpt-release-notes…
------
"We’ve upgraded the ChatGPT model with improved factuality and mathematical capabilities." (https://help.openai.com/en/articles/6825453-chatgpt-release-notes…)

Close!
------
BREAKING!! And update to ChatGPT just launched.

https://help.openai.com/en/articles/6825453-chatgpt-release-notes…
------
By all means, use chatGPT to improve your writing. It's great for that. But math is not its strength. That's not how neural networks work.
------
Great write up! We studied a similar problem too. We looked at when and what augmentations help for various classification models. Link to the paper -
------
Excited to be giving a live webinar next week where we talk more about how we 3x-ed the inference speed of Stable Diffusion in PyTorch.

Do you have any questions you’d like us to answer & chat about? Happy to include those!!
------
Went down the rabbit hole of comparing *automatic* image augmentation methods in 
@pytorch
.
TrivialAugment -- the simplest solution -- seems to be the clear winner, boosting the test set accuracy on CIFAR-10 by 15%!

A more detailed write-up in my blog:
------
Quite the productive weekend! Lots of new content added to Machine Learning Q and AI:  https://leanpub.com/machine-learning-q-and-ai/…

Q21. Stateless vs Stateful Training
Q27. Proper Metrics
Q30. Limited Labeled Data
------
Once in a while, I stumble upon some nice little feature in the 
@PyTorch
  library that I haven't seen before. 

Have you tried AutoAugment, yet?

Just gave it a try: takes a few epochs, but that's a 10% accuracy improvement in this simple example case
https://github.com/rasbt/deeplearning-models/tree/master/pytorch-lightning_ipynb/data-augmentation/autoaugment…
------
Or even better: TrivialAugment! 

(Notebook here if you want to give it a try: https://github.com/rasbt/deeplearning-models/tree/master/pytorch-lightning_ipynb/data-augmentation/autoaugment…)

Thanks for the suggestion 
@SamuelMullr
!
------
Whoa, just looking up my book on Amazon and saw it's now #1 in Natural language processing. (https://amazon.com/Machine-Learning-PyTorch-Scikit-Learn-learning/dp/1801819319/ref=tmm_pap_swatch_0…)

Yeah, it's not an NLP book, but there is an extensive transformer chapter that was a lot of work. Glad it's so well received!
------
AutoAugment paper:
https://arxiv.org/abs/1805.09501

transforms.AutoAugment docs:
https://pytorch.org/vision/main/generated/torchvision.transforms.AutoAugment.html…
------
The whimsical elements on GitHub always bring a sense of delight.
------
I outlined 9 different machine learning strategies, but which one to use? 
An attempt to bring a bit of structure into this:
------
Derivatives of regular supervised learning when you don't have enough labeled data:

1.  Transfer learning
2.  Self-supervised learning
3.  Semi-supervised learning
4.  Few-shot learning
5.  Meta-learning
6.  Active learning
7.  Weakly supervised learning
8.  Multi-task learning
------
Totally missed that one node had 3 arrows above. Just cleaned that up:
------
*  The black boxes are not terminal nodes but arch back to "Evaluate model performance" (arrows omitted to reduce clutter)

** Techniques such as data augmentation, multi-task, and multi-modal learning could be combined with all these strategies
------
ChatGPT is not a free university.

It’s an amazing tool that can increase your productivity, but “teaching” is not its strength.

If you want to learn coding, copywriting, finances, or any other topic, there are great educators out there.

ChatGPT is not one of them.
------
Had a fun podcast recording & chat with

@prateekvjoshi
 yesterday!

Realized on my morning walk that I totally forgot my currently fav AI use case:
Transcribing my voice recordings!

Earlier this year, I got a simple voice recorder that I use to take notes when driving/walking.
------
Here's a link to the code if you want to give it a try: https://gist.github.com/rasbt/4bfd783328171972d7aaf77e83489c28…

And if you prefer a nice and polished UI, we recently launched the Echo App 
@LightningAI
 on top of Whisper earlier this year: https://lightning.ai/echo
------
! I spend a good chunk of time each week reading books and articles to extend my Machine Learning flashcard decks in Anki.

On the one hand, it's quite rewarding, on the other hand, that's where I wouldn't mind some AI assistance.
------
My dream GPT product is a tool that can generate flash cards (like anki) from books/articles I'm reading so I can quiz myself
------
We made it three times faster to serve Stable Diffusion 

Here’s how we did it ...

(1/6)
------
One weird trick .
Someone suggested on can decrease time to convergence by "warmstarting" / overfitting to a small number of batches first (like during debugging).

Wanted to debunk that, and yeah, it doesn't work . 

Hm, crowdsourcing this suggestion: Anyone done that before?
------
It's the beginning of the semester, so some of you might be looking for interesting machine learning datasets for teaching or class projects.

Put together some resources here:
https://sebastianraschka.com/blog/2021/ml-dl-datasets.html…

(Haven't updated it in a few months -- is there's anything worthwhile to add?)
------
Wow, I just saw that "Machine Learning Q and AI" (https://leanpub.com/machine-learning-q-and-ai…) jumped to #2 in the "machine learning" and "AI" Bestseller lists on 
@leanpub
  
(just behind 
@burkov
's excellent 100 pg ML book)

Thx for all the support & early feedback, everyone. Really appreciate it! 
------
"We’ve upgraded the ChatGPT model with improved factuality and mathematical capabilities." (https://help.openai.com/en/articles/6825453-chatgpt-release-notes…)

Close!
------
BREAKING!! And update to ChatGPT just launched.

https://help.openai.com/en/articles/6825453-chatgpt-release-notes…
------
By all means, use chatGPT to improve your writing. It's great for that. But math is not its strength. That's not how neural networks work.
------
Great write up! We studied a similar problem too. We looked at when and what augmentations help for various classification models. Link to the paper -
------
Excited to be giving a live webinar next week where we talk more about how we 3x-ed the inference speed of Stable Diffusion in PyTorch.

Do you have any questions you’d like us to answer & chat about? Happy to include those!!
------
Went down the rabbit hole of comparing *automatic* image augmentation methods in 
@pytorch
.
TrivialAugment -- the simplest solution -- seems to be the clear winner, boosting the test set accuracy on CIFAR-10 by 15%!

A more detailed write-up in my blog:
------
Quite the productive weekend! Lots of new content added to Machine Learning Q and AI:  https://leanpub.com/machine-learning-q-and-ai/…

Q21. Stateless vs Stateful Training
Q27. Proper Metrics
Q30. Limited Labeled Data
------
Once in a while, I stumble upon some nice little feature in the 
@PyTorch
  library that I haven't seen before. 

Have you tried AutoAugment, yet?

Just gave it a try: takes a few epochs, but that's a 10% accuracy improvement in this simple example case
https://github.com/rasbt/deeplearning-models/tree/master/pytorch-lightning_ipynb/data-augmentation/autoaugment…
------
Or even better: TrivialAugment! 

(Notebook here if you want to give it a try: https://github.com/rasbt/deeplearning-models/tree/master/pytorch-lightning_ipynb/data-augmentation/autoaugment…)

Thanks for the suggestion 
@SamuelMullr
!
------
Whoa, just looking up my book on Amazon and saw it's now #1 in Natural language processing. (https://amazon.com/Machine-Learning-PyTorch-Scikit-Learn-learning/dp/1801819319/ref=tmm_pap_swatch_0…)

Yeah, it's not an NLP book, but there is an extensive transformer chapter that was a lot of work. Glad it's so well received!
------
AutoAugment paper:
https://arxiv.org/abs/1805.09501

transforms.AutoAugment docs:
https://pytorch.org/vision/main/generated/torchvision.transforms.AutoAugment.html…
------
The whimsical elements on GitHub always bring a sense of delight.
------
I outlined 9 different machine learning strategies, but which one to use? 
An attempt to bring a bit of structure into this:
------
Derivatives of regular supervised learning when you don't have enough labeled data:

1.  Transfer learning
2.  Self-supervised learning
3.  Semi-supervised learning
4.  Few-shot learning
5.  Meta-learning
6.  Active learning
7.  Weakly supervised learning
8.  Multi-task learning
------
Totally missed that one node had 3 arrows above. Just cleaned that up:
------
*  The black boxes are not terminal nodes but arch back to "Evaluate model performance" (arrows omitted to reduce clutter)

** Techniques such as data augmentation, multi-task, and multi-modal learning could be combined with all these strategies
------
ChatGPT is not a free university.

It’s an amazing tool that can increase your productivity, but “teaching” is not its strength.

If you want to learn coding, copywriting, finances, or any other topic, there are great educators out there.

ChatGPT is not one of them.
------
Had a fun podcast recording & chat with

@prateekvjoshi
 yesterday!

Realized on my morning walk that I totally forgot my currently fav AI use case:
Transcribing my voice recordings!

Earlier this year, I got a simple voice recorder that I use to take notes when driving/walking.
------
Here's a link to the code if you want to give it a try: https://gist.github.com/rasbt/4bfd783328171972d7aaf77e83489c28…

And if you prefer a nice and polished UI, we recently launched the Echo App 
@LightningAI
 on top of Whisper earlier this year: https://lightning.ai/echo
------
! I spend a good chunk of time each week reading books and articles to extend my Machine Learning flashcard decks in Anki.

On the one hand, it's quite rewarding, on the other hand, that's where I wouldn't mind some AI assistance.
------
My dream GPT product is a tool that can generate flash cards (like anki) from books/articles I'm reading so I can quiz myself
------
We made it three times faster to serve Stable Diffusion 

Here’s how we did it ...

(1/6)
------
One weird trick .
Someone suggested on can decrease time to convergence by "warmstarting" / overfitting to a small number of batches first (like during debugging).

Wanted to debunk that, and yeah, it doesn't work . 

Hm, crowdsourcing this suggestion: Anyone done that before?
------
It's the beginning of the semester, so some of you might be looking for interesting machine learning datasets for teaching or class projects.

Put together some resources here:
https://sebastianraschka.com/blog/2021/ml-dl-datasets.html…

(Haven't updated it in a few months -- is there's anything worthwhile to add?)
------
Wow, I just saw that "Machine Learning Q and AI" (https://leanpub.com/machine-learning-q-and-ai…) jumped to #2 in the "machine learning" and "AI" Bestseller lists on 
@leanpub
  
(just behind 
@burkov
's excellent 100 pg ML book)

Thx for all the support & early feedback, everyone. Really appreciate it! 
------
Whoa 
------
What's one machine learning topic that you wish was taught better in courses/books?
------
Btw if you are taking my Deep Learning Fundamentals course (https://lightning.ai/pages/courses/deep-learning-fundamentals/…), how do you feel about the difficulty of the quizzes so far?

Happy to adjust!
------
Happy to share that Unit 4 of my free Deep Learning Fundamentals class is now live!

It covers multilayer neural nets and important design considerations (using nonlinear activation functions, and small random weight initialization).

 https://lightning.ai/pages/courses/deep-learning-fundamentals/training-multilayer-neural-networks-overview/logistic-regression-for-multiple-classes-part-1-5/…

Happy Learning!
------
Went down the rabbit hole of comparing *automatic* image augmentation methods in 
@pytorch
.
TrivialAugment -- the simplest solution -- seems to be the clear winner, boosting the test set accuracy on CIFAR-10 by 15%!

A more detailed write-up in my blog:
------
Quite the productive weekend! Lots of new content added to Machine Learning Q and AI:  https://leanpub.com/machine-learning-q-and-ai/…

Q21. Stateless vs Stateful Training
Q27. Proper Metrics
Q30. Limited Labeled Data
------
Once in a while, I stumble upon some nice little feature in the 
@PyTorch
  library that I haven't seen before. 

Have you tried AutoAugment, yet?

Just gave it a try: takes a few epochs, but that's a 10% accuracy improvement in this simple example case
https://github.com/rasbt/deeplearning-models/tree/master/pytorch-lightning_ipynb/data-augmentation/autoaugment…
------
Or even better: TrivialAugment! 

(Notebook here if you want to give it a try: https://github.com/rasbt/deeplearning-models/tree/master/pytorch-lightning_ipynb/data-augmentation/autoaugment…)

Thanks for the suggestion 
@SamuelMullr
!
------
Whoa, just looking up my book on Amazon and saw it's now #1 in Natural language processing. (https://amazon.com/Machine-Learning-PyTorch-Scikit-Learn-learning/dp/1801819319/ref=tmm_pap_swatch_0…)

Yeah, it's not an NLP book, but there is an extensive transformer chapter that was a lot of work. Glad it's so well received!
------
AutoAugment paper:
https://arxiv.org/abs/1805.09501

transforms.AutoAugment docs:
https://pytorch.org/vision/main/generated/torchvision.transforms.AutoAugment.html…
------
The whimsical elements on GitHub always bring a sense of delight.
------
I outlined 9 different machine learning strategies, but which one to use? 
An attempt to bring a bit of structure into this:
------
Derivatives of regular supervised learning when you don't have enough labeled data:

1.  Transfer learning
2.  Self-supervised learning
3.  Semi-supervised learning
4.  Few-shot learning
5.  Meta-learning
6.  Active learning
7.  Weakly supervised learning
8.  Multi-task learning
------
Totally missed that one node had 3 arrows above. Just cleaned that up:
------
*  The black boxes are not terminal nodes but arch back to "Evaluate model performance" (arrows omitted to reduce clutter)

** Techniques such as data augmentation, multi-task, and multi-modal learning could be combined with all these strategies
------
ChatGPT is not a free university.

It’s an amazing tool that can increase your productivity, but “teaching” is not its strength.

If you want to learn coding, copywriting, finances, or any other topic, there are great educators out there.

ChatGPT is not one of them.
------
Had a fun podcast recording & chat with

@prateekvjoshi
 yesterday!

Realized on my morning walk that I totally forgot my currently fav AI use case:
Transcribing my voice recordings!

Earlier this year, I got a simple voice recorder that I use to take notes when driving/walking.
------
Here's a link to the code if you want to give it a try: https://gist.github.com/rasbt/4bfd783328171972d7aaf77e83489c28…

And if you prefer a nice and polished UI, we recently launched the Echo App 
@LightningAI
 on top of Whisper earlier this year: https://lightning.ai/echo
------
! I spend a good chunk of time each week reading books and articles to extend my Machine Learning flashcard decks in Anki.

On the one hand, it's quite rewarding, on the other hand, that's where I wouldn't mind some AI assistance.
------
My dream GPT product is a tool that can generate flash cards (like anki) from books/articles I'm reading so I can quiz myself
------
We made it three times faster to serve Stable Diffusion 

Here’s how we did it ...

(1/6)
------
One weird trick .
Someone suggested on can decrease time to convergence by "warmstarting" / overfitting to a small number of batches first (like during debugging).

Wanted to debunk that, and yeah, it doesn't work . 

Hm, crowdsourcing this suggestion: Anyone done that before?
------
It's the beginning of the semester, so some of you might be looking for interesting machine learning datasets for teaching or class projects.

Put together some resources here:
https://sebastianraschka.com/blog/2021/ml-dl-datasets.html…

(Haven't updated it in a few months -- is there's anything worthwhile to add?)
------
Wow, I just saw that "Machine Learning Q and AI" (https://leanpub.com/machine-learning-q-and-ai…) jumped to #2 in the "machine learning" and "AI" Bestseller lists on 
@leanpub
  
(just behind 
@burkov
's excellent 100 pg ML book)

Thx for all the support & early feedback, everyone. Really appreciate it! 
------
Whoa 
------
What's one machine learning topic that you wish was taught better in courses/books?
------
Btw if you are taking my Deep Learning Fundamentals course (https://lightning.ai/pages/courses/deep-learning-fundamentals/…), how do you feel about the difficulty of the quizzes so far?

Happy to adjust!
------
Happy to share that Unit 4 of my free Deep Learning Fundamentals class is now live!

It covers multilayer neural nets and important design considerations (using nonlinear activation functions, and small random weight initialization).

 https://lightning.ai/pages/courses/deep-learning-fundamentals/training-multilayer-neural-networks-overview/logistic-regression-for-multiple-classes-part-1-5/…

Happy Learning!
------
I am particularly looking forward to read Chapter 18 on Diffusion Models!

https://udlbook.github.io/udlbook/

Great work 
@SimonPrinceAI
!
------
Understanding Deep Learning

If you are looking for a comprehensive deep learning book with some of the more recent trends ( transformers, diffusion models, GNNs,...), this looks like a great option.

https://udlbook.github.io/udlbook/
------
Whoa, just looking up my book on Amazon and saw it's now #1 in Natural language processing. (https://amazon.com/Machine-Learning-PyTorch-Scikit-Learn-learning/dp/1801819319/ref=tmm_pap_swatch_0…)

Yeah, it's not an NLP book, but there is an extensive transformer chapter that was a lot of work. Glad it's so well received!
------
AutoAugment paper:
https://arxiv.org/abs/1805.09501

transforms.AutoAugment docs:
https://pytorch.org/vision/main/generated/torchvision.transforms.AutoAugment.html…
------
The whimsical elements on GitHub always bring a sense of delight.
------
I outlined 9 different machine learning strategies, but which one to use? 
An attempt to bring a bit of structure into this:
------
Derivatives of regular supervised learning when you don't have enough labeled data:

1.  Transfer learning
2.  Self-supervised learning
3.  Semi-supervised learning
4.  Few-shot learning
5.  Meta-learning
6.  Active learning
7.  Weakly supervised learning
8.  Multi-task learning
------
Totally missed that one node had 3 arrows above. Just cleaned that up:
------
*  The black boxes are not terminal nodes but arch back to "Evaluate model performance" (arrows omitted to reduce clutter)

** Techniques such as data augmentation, multi-task, and multi-modal learning could be combined with all these strategies
------
ChatGPT is not a free university.

It’s an amazing tool that can increase your productivity, but “teaching” is not its strength.

If you want to learn coding, copywriting, finances, or any other topic, there are great educators out there.

ChatGPT is not one of them.
------
Had a fun podcast recording & chat with

@prateekvjoshi
 yesterday!

Realized on my morning walk that I totally forgot my currently fav AI use case:
Transcribing my voice recordings!

Earlier this year, I got a simple voice recorder that I use to take notes when driving/walking.
------
Here's a link to the code if you want to give it a try: https://gist.github.com/rasbt/4bfd783328171972d7aaf77e83489c28…

And if you prefer a nice and polished UI, we recently launched the Echo App 
@LightningAI
 on top of Whisper earlier this year: https://lightning.ai/echo
------
! I spend a good chunk of time each week reading books and articles to extend my Machine Learning flashcard decks in Anki.

On the one hand, it's quite rewarding, on the other hand, that's where I wouldn't mind some AI assistance.
------
My dream GPT product is a tool that can generate flash cards (like anki) from books/articles I'm reading so I can quiz myself
------
We made it three times faster to serve Stable Diffusion 

Here’s how we did it ...

(1/6)
------
One weird trick .
Someone suggested on can decrease time to convergence by "warmstarting" / overfitting to a small number of batches first (like during debugging).

Wanted to debunk that, and yeah, it doesn't work . 

Hm, crowdsourcing this suggestion: Anyone done that before?
------
It's the beginning of the semester, so some of you might be looking for interesting machine learning datasets for teaching or class projects.

Put together some resources here:
https://sebastianraschka.com/blog/2021/ml-dl-datasets.html…

(Haven't updated it in a few months -- is there's anything worthwhile to add?)
------
Wow, I just saw that "Machine Learning Q and AI" (https://leanpub.com/machine-learning-q-and-ai…) jumped to #2 in the "machine learning" and "AI" Bestseller lists on 
@leanpub
  
(just behind 
@burkov
's excellent 100 pg ML book)

Thx for all the support & early feedback, everyone. Really appreciate it! 
------
Whoa 
------
What's one machine learning topic that you wish was taught better in courses/books?
------
Btw if you are taking my Deep Learning Fundamentals course (https://lightning.ai/pages/courses/deep-learning-fundamentals/…), how do you feel about the difficulty of the quizzes so far?

Happy to adjust!
------
Happy to share that Unit 4 of my free Deep Learning Fundamentals class is now live!

It covers multilayer neural nets and important design considerations (using nonlinear activation functions, and small random weight initialization).

 https://lightning.ai/pages/courses/deep-learning-fundamentals/training-multilayer-neural-networks-overview/logistic-regression-for-multiple-classes-part-1-5/…

Happy Learning!
------
I am particularly looking forward to read Chapter 18 on Diffusion Models!

https://udlbook.github.io/udlbook/

Great work 
@SimonPrinceAI
!
------
Understanding Deep Learning

If you are looking for a comprehensive deep learning book with some of the more recent trends ( transformers, diffusion models, GNNs,...), this looks like a great option.

https://udlbook.github.io/udlbook/
------
When ChatGPT got your attention — no pun intended
------
Galaxy brain: outsource your recruitment of company board members to ChatGPT
------
The first AGI will be developed in a Jupyter notebook
------
The world of artificial intelligence in general and machine learning in particular is still on a journey to rediscover the basic principles of  software engineering. twitter.com/unsorsodicorda…
------
Lecture slides for my courses

Introduction to #ComputerVision:
http://eecs.yorku.ca/~kosta/Courses/EECS4422…

#DeepLearning in Computer Vision:
http://eecs.yorku.ca/~kosta/Courses/EECS6322…

The included videos do not contain voiceovers, planned for a future revision.
------
Loved the first episode! Nice way to keep up with DL via a fun conversational interview.
------
Just finished recording episode #2 of The AI Buzz with @lantiga BAM! Come join us for the premiere on Tuesday at 11am (New York time): https://youtu.be/1xBB8iAXxds
------
Totally agree! Big fan of project-based learning. 
And my students always had a lot of fun with their class projects (+ it gives them something for their resume!)
https://proceedings.mlr.press/v170/raschka22a.html…
------
Solving problems is the best way to learn Machine Learning.

But most people don't know where to start.

Here are 10 projects you can use to kick off your journey:
------
Here are some fun student project examples:
------
Regarding choosing a good batch size:
Choosing a batch size to be as large as your hardware permits (up to a point) seems to be a good recommendation.
Compiled some resources via the figure below.
------
Was doing some more search and found a relevant post from 
@ylecun
 that argues for a smaller batch size. The discussion remains interesting :P
------
Training with large minibatches is bad for your health.
More importantly, it's bad for your test error.
Friends dont let friends use minibatches larger than 32. https://arxiv.org/abs/1804.07612
------
When I asked ChatGPT the "Why do ViTs need more data than ConvNets" question this morning, I saw the biggest blunder yet

> "Fewer parameters: Vision transformers have fewer parameters than CNNs, this means that they require more data [...]."

(via https://leanpub.com/machine-learning-q-and-ai/…)
------
Totally missed that one node had 3 arrows above. Just cleaned that up:
------
*  The black boxes are not terminal nodes but arch back to "Evaluate model performance" (arrows omitted to reduce clutter)

** Techniques such as data augmentation, multi-task, and multi-modal learning could be combined with all these strategies
------
ChatGPT is not a free university.

It’s an amazing tool that can increase your productivity, but “teaching” is not its strength.

If you want to learn coding, copywriting, finances, or any other topic, there are great educators out there.

ChatGPT is not one of them.
------
Had a fun podcast recording & chat with

@prateekvjoshi
 yesterday!

Realized on my morning walk that I totally forgot my currently fav AI use case:
Transcribing my voice recordings!

Earlier this year, I got a simple voice recorder that I use to take notes when driving/walking.
------
Here's a link to the code if you want to give it a try: https://gist.github.com/rasbt/4bfd783328171972d7aaf77e83489c28…

And if you prefer a nice and polished UI, we recently launched the Echo App 
@LightningAI
 on top of Whisper earlier this year: https://lightning.ai/echo
------
! I spend a good chunk of time each week reading books and articles to extend my Machine Learning flashcard decks in Anki.

On the one hand, it's quite rewarding, on the other hand, that's where I wouldn't mind some AI assistance.
------
My dream GPT product is a tool that can generate flash cards (like anki) from books/articles I'm reading so I can quiz myself
------
We made it three times faster to serve Stable Diffusion 

Here’s how we did it ...

(1/6)
------
One weird trick .
Someone suggested on can decrease time to convergence by "warmstarting" / overfitting to a small number of batches first (like during debugging).

Wanted to debunk that, and yeah, it doesn't work . 

Hm, crowdsourcing this suggestion: Anyone done that before?
------
It's the beginning of the semester, so some of you might be looking for interesting machine learning datasets for teaching or class projects.

Put together some resources here:
https://sebastianraschka.com/blog/2021/ml-dl-datasets.html…

(Haven't updated it in a few months -- is there's anything worthwhile to add?)
------
Wow, I just saw that "Machine Learning Q and AI" (https://leanpub.com/machine-learning-q-and-ai…) jumped to #2 in the "machine learning" and "AI" Bestseller lists on 
@leanpub
  
(just behind 
@burkov
's excellent 100 pg ML book)

Thx for all the support & early feedback, everyone. Really appreciate it! 
------
Whoa 
------
What's one machine learning topic that you wish was taught better in courses/books?
------
Btw if you are taking my Deep Learning Fundamentals course (https://lightning.ai/pages/courses/deep-learning-fundamentals/…), how do you feel about the difficulty of the quizzes so far?

Happy to adjust!
------
Happy to share that Unit 4 of my free Deep Learning Fundamentals class is now live!

It covers multilayer neural nets and important design considerations (using nonlinear activation functions, and small random weight initialization).

 https://lightning.ai/pages/courses/deep-learning-fundamentals/training-multilayer-neural-networks-overview/logistic-regression-for-multiple-classes-part-1-5/…

Happy Learning!
------
I am particularly looking forward to read Chapter 18 on Diffusion Models!

https://udlbook.github.io/udlbook/

Great work 
@SimonPrinceAI
!
------
Understanding Deep Learning

If you are looking for a comprehensive deep learning book with some of the more recent trends ( transformers, diffusion models, GNNs,...), this looks like a great option.

https://udlbook.github.io/udlbook/
------
When ChatGPT got your attention — no pun intended
------
Galaxy brain: outsource your recruitment of company board members to ChatGPT
------
The first AGI will be developed in a Jupyter notebook
------
The world of artificial intelligence in general and machine learning in particular is still on a journey to rediscover the basic principles of  software engineering. twitter.com/unsorsodicorda…
------
Lecture slides for my courses

Introduction to #ComputerVision:
http://eecs.yorku.ca/~kosta/Courses/EECS4422…

#DeepLearning in Computer Vision:
http://eecs.yorku.ca/~kosta/Courses/EECS6322…

The included videos do not contain voiceovers, planned for a future revision.
------
Loved the first episode! Nice way to keep up with DL via a fun conversational interview.
------
Just finished recording episode #2 of The AI Buzz with @lantiga BAM! Come join us for the premiere on Tuesday at 11am (New York time): https://youtu.be/1xBB8iAXxds
------
Totally agree! Big fan of project-based learning. 
And my students always had a lot of fun with their class projects (+ it gives them something for their resume!)
https://proceedings.mlr.press/v170/raschka22a.html…
------
Solving problems is the best way to learn Machine Learning.

But most people don't know where to start.

Here are 10 projects you can use to kick off your journey:
------
Here are some fun student project examples:
------
Regarding choosing a good batch size:
Choosing a batch size to be as large as your hardware permits (up to a point) seems to be a good recommendation.
Compiled some resources via the figure below.
------
Was doing some more search and found a relevant post from 
@ylecun
 that argues for a smaller batch size. The discussion remains interesting :P
------
Training with large minibatches is bad for your health.
More importantly, it's bad for your test error.
Friends dont let friends use minibatches larger than 32. https://arxiv.org/abs/1804.07612
------
When I asked ChatGPT the "Why do ViTs need more data than ConvNets" question this morning, I saw the biggest blunder yet

> "Fewer parameters: Vision transformers have fewer parameters than CNNs, this means that they require more data [...]."

(via https://leanpub.com/machine-learning-q-and-ai/…)
------
Another one regarding disadvantages of a large k in k-fold CV: 

> Reduced data for training: As k increases, the size of each fold decreases, which means that less data is available for training the model, which can lead to poor performance when the model is applied to new data.
------
@rasbt
's new book, with its Q&A format, is a valuable resource for learning and internalizing the fundamental concepts of machine learning. Can't wait to read the next chapters!

"Machine Learning Q and AI – Expand your machine learning knowledge with 30 questions and answers"
------
Just saw this post, and what a fun coincidence. Read all of these, and I can definitely recommend them all as well!
------
Best data science books to read in 2023 
https://link.medium.com/3fIui790Ewb 

#DataScience #MachineLearning #mlops
------
Just saw this post, and what a fun coincidence. Read all of these, and I can definitely recommend them all as well!
------
Best data science books to read in 2023 
https://link.medium.com/3fIui790Ewb 

#DataScience #MachineLearning #mlops
------
! I spend a good chunk of time each week reading books and articles to extend my Machine Learning flashcard decks in Anki.

On the one hand, it's quite rewarding, on the other hand, that's where I wouldn't mind some AI assistance.
------
My dream GPT product is a tool that can generate flash cards (like anki) from books/articles I'm reading so I can quiz myself
------
We made it three times faster to serve Stable Diffusion 

Here’s how we did it ...

(1/6)
------
One weird trick .
Someone suggested on can decrease time to convergence by "warmstarting" / overfitting to a small number of batches first (like during debugging).

Wanted to debunk that, and yeah, it doesn't work . 

Hm, crowdsourcing this suggestion: Anyone done that before?
------
It's the beginning of the semester, so some of you might be looking for interesting machine learning datasets for teaching or class projects.

Put together some resources here:
https://sebastianraschka.com/blog/2021/ml-dl-datasets.html…

(Haven't updated it in a few months -- is there's anything worthwhile to add?)
------
Wow, I just saw that "Machine Learning Q and AI" (https://leanpub.com/machine-learning-q-and-ai…) jumped to #2 in the "machine learning" and "AI" Bestseller lists on 
@leanpub
  
(just behind 
@burkov
's excellent 100 pg ML book)

Thx for all the support & early feedback, everyone. Really appreciate it! 
------
Whoa 
------
What's one machine learning topic that you wish was taught better in courses/books?
------
Btw if you are taking my Deep Learning Fundamentals course (https://lightning.ai/pages/courses/deep-learning-fundamentals/…), how do you feel about the difficulty of the quizzes so far?

Happy to adjust!
------
Happy to share that Unit 4 of my free Deep Learning Fundamentals class is now live!

It covers multilayer neural nets and important design considerations (using nonlinear activation functions, and small random weight initialization).

 https://lightning.ai/pages/courses/deep-learning-fundamentals/training-multilayer-neural-networks-overview/logistic-regression-for-multiple-classes-part-1-5/…

Happy Learning!
------
I am particularly looking forward to read Chapter 18 on Diffusion Models!

https://udlbook.github.io/udlbook/

Great work 
@SimonPrinceAI
!
------
Understanding Deep Learning

If you are looking for a comprehensive deep learning book with some of the more recent trends ( transformers, diffusion models, GNNs,...), this looks like a great option.

https://udlbook.github.io/udlbook/
------
When ChatGPT got your attention — no pun intended
------
Galaxy brain: outsource your recruitment of company board members to ChatGPT
------
The first AGI will be developed in a Jupyter notebook
------
The world of artificial intelligence in general and machine learning in particular is still on a journey to rediscover the basic principles of  software engineering. twitter.com/unsorsodicorda…
------
Lecture slides for my courses

Introduction to #ComputerVision:
http://eecs.yorku.ca/~kosta/Courses/EECS4422…

#DeepLearning in Computer Vision:
http://eecs.yorku.ca/~kosta/Courses/EECS6322…

The included videos do not contain voiceovers, planned for a future revision.
------
Loved the first episode! Nice way to keep up with DL via a fun conversational interview.
------
Just finished recording episode #2 of The AI Buzz with @lantiga BAM! Come join us for the premiere on Tuesday at 11am (New York time): https://youtu.be/1xBB8iAXxds
------
Totally agree! Big fan of project-based learning. 
And my students always had a lot of fun with their class projects (+ it gives them something for their resume!)
https://proceedings.mlr.press/v170/raschka22a.html…
------
Solving problems is the best way to learn Machine Learning.

But most people don't know where to start.

Here are 10 projects you can use to kick off your journey:
------
Here are some fun student project examples:
------
Regarding choosing a good batch size:
Choosing a batch size to be as large as your hardware permits (up to a point) seems to be a good recommendation.
Compiled some resources via the figure below.
------
Was doing some more search and found a relevant post from 
@ylecun
 that argues for a smaller batch size. The discussion remains interesting :P
------
Training with large minibatches is bad for your health.
More importantly, it's bad for your test error.
Friends dont let friends use minibatches larger than 32. https://arxiv.org/abs/1804.07612
------
When I asked ChatGPT the "Why do ViTs need more data than ConvNets" question this morning, I saw the biggest blunder yet

> "Fewer parameters: Vision transformers have fewer parameters than CNNs, this means that they require more data [...]."

(via https://leanpub.com/machine-learning-q-and-ai/…)
------
Another one regarding disadvantages of a large k in k-fold CV: 

> Reduced data for training: As k increases, the size of each fold decreases, which means that less data is available for training the model, which can lead to poor performance when the model is applied to new data.
------
@rasbt
's new book, with its Q&A format, is a valuable resource for learning and internalizing the fundamental concepts of machine learning. Can't wait to read the next chapters!

"Machine Learning Q and AI – Expand your machine learning knowledge with 30 questions and answers"
------
Just saw this post, and what a fun coincidence. Read all of these, and I can definitely recommend them all as well!
------
Best data science books to read in 2023 
https://link.medium.com/3fIui790Ewb 

#DataScience #MachineLearning #mlops
------
Just saw this post, and what a fun coincidence. Read all of these, and I can definitely recommend them all as well!
------
Best data science books to read in 2023 
https://link.medium.com/3fIui790Ewb 

#DataScience #MachineLearning #mlops
------
PS: I remember reading and liking 
@jakevdp
's book, but I  must have donated that some time back in 2018 when I moved to Madison.
And a little disclaimer, I never finished the Math for ML one.
------
denoising diffusion generative models are actually
𝗶𝗻𝘃𝗲𝗿𝘀𝗲 diffusions
------
Just reading the Deep Learning Tuning Playbook (https://github.com/google-research/tuning_playbook/blob/main/README.md…).

"The batch size governs the training speed and shouldn't be used to directly tune the validation set performance. Often, the ideal batch size will be the largest batch size supported [...]"

Good advice 
------
In other words, choose it as large as your hardware allows, but don't treat it as a tunable hyperparameter. But there is also no point in using larger batch sizes if it increased the training time.
------
Hi friends, I've started a crowdsourced Google Doc for open positions and helpful volunteer contacts to help laid-off Googlers find jobs. Please add any openings you know of, and list your contact information if you're willing to help!
------
Machine Learning Q and AI -- Expand your machine learning & AI knowledge with 30 questions and answers.

An early-access of my new book, my first in the era of ChatGPT, is now on Leanpub: https://leanpub.com/machine-learning-q-and-ai…

Would love to hear what you think!

Happy weekend & happy learning!
------
If I am not too optimistic, the ETA for the complete version is Q1/Q2 2023 
I have most questions already written last year. 
The plan is to be editing + formatting 2-3 q's per weekend. 
(Some of you may recognize snippets from all my social media posts over the years )
------
Wow, I just saw that "Machine Learning Q and AI" (https://leanpub.com/machine-learning-q-and-ai…) jumped to #2 in the "machine learning" and "AI" Bestseller lists on 
@leanpub
  
(just behind 
@burkov
's excellent 100 pg ML book)

Thx for all the support & early feedback, everyone. Really appreciate it! 
------
Whoa 
------
What's one machine learning topic that you wish was taught better in courses/books?
------
Btw if you are taking my Deep Learning Fundamentals course (https://lightning.ai/pages/courses/deep-learning-fundamentals/…), how do you feel about the difficulty of the quizzes so far?

Happy to adjust!
------
Happy to share that Unit 4 of my free Deep Learning Fundamentals class is now live!

It covers multilayer neural nets and important design considerations (using nonlinear activation functions, and small random weight initialization).

 https://lightning.ai/pages/courses/deep-learning-fundamentals/training-multilayer-neural-networks-overview/logistic-regression-for-multiple-classes-part-1-5/…

Happy Learning!
------
I am particularly looking forward to read Chapter 18 on Diffusion Models!

https://udlbook.github.io/udlbook/

Great work 
@SimonPrinceAI
!
------
Understanding Deep Learning

If you are looking for a comprehensive deep learning book with some of the more recent trends ( transformers, diffusion models, GNNs,...), this looks like a great option.

https://udlbook.github.io/udlbook/
------
When ChatGPT got your attention — no pun intended
------
Galaxy brain: outsource your recruitment of company board members to ChatGPT
------
The first AGI will be developed in a Jupyter notebook
------
The world of artificial intelligence in general and machine learning in particular is still on a journey to rediscover the basic principles of  software engineering. twitter.com/unsorsodicorda…
------
Lecture slides for my courses

Introduction to #ComputerVision:
http://eecs.yorku.ca/~kosta/Courses/EECS4422…

#DeepLearning in Computer Vision:
http://eecs.yorku.ca/~kosta/Courses/EECS6322…

The included videos do not contain voiceovers, planned for a future revision.
------
Loved the first episode! Nice way to keep up with DL via a fun conversational interview.
------
Just finished recording episode #2 of The AI Buzz with @lantiga BAM! Come join us for the premiere on Tuesday at 11am (New York time): https://youtu.be/1xBB8iAXxds
------
Totally agree! Big fan of project-based learning. 
And my students always had a lot of fun with their class projects (+ it gives them something for their resume!)
https://proceedings.mlr.press/v170/raschka22a.html…
------
Solving problems is the best way to learn Machine Learning.

But most people don't know where to start.

Here are 10 projects you can use to kick off your journey:
------
Here are some fun student project examples:
------
Regarding choosing a good batch size:
Choosing a batch size to be as large as your hardware permits (up to a point) seems to be a good recommendation.
Compiled some resources via the figure below.
------
Was doing some more search and found a relevant post from 
@ylecun
 that argues for a smaller batch size. The discussion remains interesting :P
------
Training with large minibatches is bad for your health.
More importantly, it's bad for your test error.
Friends dont let friends use minibatches larger than 32. https://arxiv.org/abs/1804.07612
------
When I asked ChatGPT the "Why do ViTs need more data than ConvNets" question this morning, I saw the biggest blunder yet

> "Fewer parameters: Vision transformers have fewer parameters than CNNs, this means that they require more data [...]."

(via https://leanpub.com/machine-learning-q-and-ai/…)
------
Another one regarding disadvantages of a large k in k-fold CV: 

> Reduced data for training: As k increases, the size of each fold decreases, which means that less data is available for training the model, which can lead to poor performance when the model is applied to new data.
------
@rasbt
's new book, with its Q&A format, is a valuable resource for learning and internalizing the fundamental concepts of machine learning. Can't wait to read the next chapters!

"Machine Learning Q and AI – Expand your machine learning knowledge with 30 questions and answers"
------
Just saw this post, and what a fun coincidence. Read all of these, and I can definitely recommend them all as well!
------
Best data science books to read in 2023 
https://link.medium.com/3fIui790Ewb 

#DataScience #MachineLearning #mlops
------
Just saw this post, and what a fun coincidence. Read all of these, and I can definitely recommend them all as well!
------
Best data science books to read in 2023 
https://link.medium.com/3fIui790Ewb 

#DataScience #MachineLearning #mlops
------
PS: I remember reading and liking 
@jakevdp
's book, but I  must have donated that some time back in 2018 when I moved to Madison.
And a little disclaimer, I never finished the Math for ML one.
------
denoising diffusion generative models are actually
𝗶𝗻𝘃𝗲𝗿𝘀𝗲 diffusions
------
Just reading the Deep Learning Tuning Playbook (https://github.com/google-research/tuning_playbook/blob/main/README.md…).

"The batch size governs the training speed and shouldn't be used to directly tune the validation set performance. Often, the ideal batch size will be the largest batch size supported [...]"

Good advice 
------
In other words, choose it as large as your hardware allows, but don't treat it as a tunable hyperparameter. But there is also no point in using larger batch sizes if it increased the training time.
------
Hi friends, I've started a crowdsourced Google Doc for open positions and helpful volunteer contacts to help laid-off Googlers find jobs. Please add any openings you know of, and list your contact information if you're willing to help!
------
Machine Learning Q and AI -- Expand your machine learning & AI knowledge with 30 questions and answers.

An early-access of my new book, my first in the era of ChatGPT, is now on Leanpub: https://leanpub.com/machine-learning-q-and-ai…

Would love to hear what you think!

Happy weekend & happy learning!
------
If I am not too optimistic, the ETA for the complete version is Q1/Q2 2023 
I have most questions already written last year. 
The plan is to be editing + formatting 2-3 q's per weekend. 
(Some of you may recognize snippets from all my social media posts over the years )
------
Now this paper has been accepted to #ICLR2023 as a spotlight! I truly appreciate everyone's efforts. Check out our repo http://github.com/keyu-tian/SparK for the latest demo video and updates! Title: "Designing BERT for Convolutional Networks: Sparse and Hierarchical Masked Modeling"
------
How can we leverage successful pretraining techniques from transformers to improve purely convolutional networks? The answer is *Sparse Convolutions*!

Let's see what happens when purely convolutional networks are pretrained with 1.28 million unlabeled images ...

1/7
------
Derivatives of regular supervised learning when you don't have enough labeled data:

1.  Transfer learning
2.  Self-supervised learning
3.  Semi-supervised learning
4.  Few-shot learning
5.  Meta-learning
6.  Active learning
7.  Weakly supervised learning
8.  Multi-task learning
------
4.  Few-shot learning: learn from a very small number of examples/class (e.g., 1 or 5) using meta-learning, transfer learning, or metric-based methods.

5.  Meta-learning: two definitions, a) either learn from meta-features, b) train a meta-learner in few-shot contexts
------
6.  Active learning: actively select the most informative examples to be labeled by a human

7.  Weakly supervised learning: create noisy, limited, or imprecise labels for a supervised learner

8.  Multi-task learning: train with multiple losses to perform well on multiple tasks
------
Big shoutout for 
@xamat
's LLM family tree blog post! It comes with nice & concise summaries of each model (63 pages, if you export it as a PDF!)

https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/…
------
The DL Fundamentals lectures will incl additional fun stuff & goodies that are usually not included in most other DL resources (like my book)

- learning rate schedulers
- automating hparam tuning
- logging & checkpointing
- mixed precision & quantization & multi-GPU training
...
------
What will you be able to do after @rasbt's Deep Learning Fundamentals course?

 Build classifiers for various kinds of data like tables, images, and text.

Tune models effectively to optimize predictive and computational performance

Get started now! https://lightning.ai/pages/courses/deep-learning-fundamentals/…
------
Better idea: let's make each of the 250 operators in PyTorch 2.0's PrimTorch a trading card.
------
Who wants to port pytorch to punch cards?
------
I am particularly looking forward to read Chapter 18 on Diffusion Models!

https://udlbook.github.io/udlbook/

Great work 
@SimonPrinceAI
!
------
Understanding Deep Learning

If you are looking for a comprehensive deep learning book with some of the more recent trends ( transformers, diffusion models, GNNs,...), this looks like a great option.

https://udlbook.github.io/udlbook/
------
When ChatGPT got your attention — no pun intended
------
Galaxy brain: outsource your recruitment of company board members to ChatGPT
------
The first AGI will be developed in a Jupyter notebook
------
The world of artificial intelligence in general and machine learning in particular is still on a journey to rediscover the basic principles of  software engineering. twitter.com/unsorsodicorda…
------
Lecture slides for my courses

Introduction to #ComputerVision:
http://eecs.yorku.ca/~kosta/Courses/EECS4422…

#DeepLearning in Computer Vision:
http://eecs.yorku.ca/~kosta/Courses/EECS6322…

The included videos do not contain voiceovers, planned for a future revision.
------
Loved the first episode! Nice way to keep up with DL via a fun conversational interview.
------
Just finished recording episode #2 of The AI Buzz with @lantiga BAM! Come join us for the premiere on Tuesday at 11am (New York time): https://youtu.be/1xBB8iAXxds
------
Totally agree! Big fan of project-based learning. 
And my students always had a lot of fun with their class projects (+ it gives them something for their resume!)
https://proceedings.mlr.press/v170/raschka22a.html…
------
Solving problems is the best way to learn Machine Learning.

But most people don't know where to start.

Here are 10 projects you can use to kick off your journey:
------
Here are some fun student project examples:
------
Regarding choosing a good batch size:
Choosing a batch size to be as large as your hardware permits (up to a point) seems to be a good recommendation.
Compiled some resources via the figure below.
------
Was doing some more search and found a relevant post from 
@ylecun
 that argues for a smaller batch size. The discussion remains interesting :P
------
Training with large minibatches is bad for your health.
More importantly, it's bad for your test error.
Friends dont let friends use minibatches larger than 32. https://arxiv.org/abs/1804.07612
------
When I asked ChatGPT the "Why do ViTs need more data than ConvNets" question this morning, I saw the biggest blunder yet

> "Fewer parameters: Vision transformers have fewer parameters than CNNs, this means that they require more data [...]."

(via https://leanpub.com/machine-learning-q-and-ai/…)
------
Another one regarding disadvantages of a large k in k-fold CV: 

> Reduced data for training: As k increases, the size of each fold decreases, which means that less data is available for training the model, which can lead to poor performance when the model is applied to new data.
------
@rasbt
's new book, with its Q&A format, is a valuable resource for learning and internalizing the fundamental concepts of machine learning. Can't wait to read the next chapters!

"Machine Learning Q and AI – Expand your machine learning knowledge with 30 questions and answers"
------
Just saw this post, and what a fun coincidence. Read all of these, and I can definitely recommend them all as well!
------
Best data science books to read in 2023 
https://link.medium.com/3fIui790Ewb 

#DataScience #MachineLearning #mlops
------
Just saw this post, and what a fun coincidence. Read all of these, and I can definitely recommend them all as well!
------
Best data science books to read in 2023 
https://link.medium.com/3fIui790Ewb 

#DataScience #MachineLearning #mlops
------
PS: I remember reading and liking 
@jakevdp
's book, but I  must have donated that some time back in 2018 when I moved to Madison.
And a little disclaimer, I never finished the Math for ML one.
------
denoising diffusion generative models are actually
𝗶𝗻𝘃𝗲𝗿𝘀𝗲 diffusions
------
Just reading the Deep Learning Tuning Playbook (https://github.com/google-research/tuning_playbook/blob/main/README.md…).

"The batch size governs the training speed and shouldn't be used to directly tune the validation set performance. Often, the ideal batch size will be the largest batch size supported [...]"

Good advice 
------
In other words, choose it as large as your hardware allows, but don't treat it as a tunable hyperparameter. But there is also no point in using larger batch sizes if it increased the training time.
------
Hi friends, I've started a crowdsourced Google Doc for open positions and helpful volunteer contacts to help laid-off Googlers find jobs. Please add any openings you know of, and list your contact information if you're willing to help!
------
Machine Learning Q and AI -- Expand your machine learning & AI knowledge with 30 questions and answers.

An early-access of my new book, my first in the era of ChatGPT, is now on Leanpub: https://leanpub.com/machine-learning-q-and-ai…

Would love to hear what you think!

Happy weekend & happy learning!
------
If I am not too optimistic, the ETA for the complete version is Q1/Q2 2023 
I have most questions already written last year. 
The plan is to be editing + formatting 2-3 q's per weekend. 
(Some of you may recognize snippets from all my social media posts over the years )
------
Now this paper has been accepted to #ICLR2023 as a spotlight! I truly appreciate everyone's efforts. Check out our repo http://github.com/keyu-tian/SparK for the latest demo video and updates! Title: "Designing BERT for Convolutional Networks: Sparse and Hierarchical Masked Modeling"
------
How can we leverage successful pretraining techniques from transformers to improve purely convolutional networks? The answer is *Sparse Convolutions*!

Let's see what happens when purely convolutional networks are pretrained with 1.28 million unlabeled images ...

1/7
------
Derivatives of regular supervised learning when you don't have enough labeled data:

1.  Transfer learning
2.  Self-supervised learning
3.  Semi-supervised learning
4.  Few-shot learning
5.  Meta-learning
6.  Active learning
7.  Weakly supervised learning
8.  Multi-task learning
------
4.  Few-shot learning: learn from a very small number of examples/class (e.g., 1 or 5) using meta-learning, transfer learning, or metric-based methods.

5.  Meta-learning: two definitions, a) either learn from meta-features, b) train a meta-learner in few-shot contexts
------
6.  Active learning: actively select the most informative examples to be labeled by a human

7.  Weakly supervised learning: create noisy, limited, or imprecise labels for a supervised learner

8.  Multi-task learning: train with multiple losses to perform well on multiple tasks
------
Big shoutout for 
@xamat
's LLM family tree blog post! It comes with nice & concise summaries of each model (63 pages, if you export it as a PDF!)

https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/…
------
The DL Fundamentals lectures will incl additional fun stuff & goodies that are usually not included in most other DL resources (like my book)

- learning rate schedulers
- automating hparam tuning
- logging & checkpointing
- mixed precision & quantization & multi-GPU training
...
------
What will you be able to do after @rasbt's Deep Learning Fundamentals course?

 Build classifiers for various kinds of data like tables, images, and text.

Tune models effectively to optimize predictive and computational performance

Get started now! https://lightning.ai/pages/courses/deep-learning-fundamentals/…
------
Better idea: let's make each of the 250 operators in PyTorch 2.0's PrimTorch a trading card.
------
Who wants to port pytorch to punch cards?
------
From a recent collaboration: Gradient boosting is usually great, but beware of overtuning. And no, k-fold cross-validation does not prevent overfitting. 

Tune a large number of hyperparameters, and you will overfit to your k-fold partitions:
------
Btw I am a big fan of k-fold cross validation. Don’t want to imply that cross-validation (k-fold or nested) is bad.
Just wanted to make a point that it doesn’t completely save you from overfitting if you run crazy extensive hyperparameter tuning sessions.
------
I mean it ! 
I wrote an article on model evaluation a few years back, and it's literally like an ad for cross-validation. And I didn't change my mind since then !
------
The Machine Learning FAQs by 
@rasbt
 are a gold mine for new and experienced data scientists.
------
Classic bias-variance theory suggests that reducing model size can reduce overfitting. Why? As a rule of thumb, the smaller the number of model parameters the smaller its capacity -- a smaller capacity reduces the model's capacity to memorize or overfit to noise in the data. 1/4
------
How can we reconcile the observation that smaller (e.g., pruned) models can exhibit better generalization performance with contradictory observations from studies of double-decent and grokking? 
3/4
------
Recent research showed that the reduction of overfitting in smaller models (e.g. via pruning) can be partly explained by the improved training processes (https://arxiv.org/abs/2210.13738).

I wonder if there are any alternative theories or pointers to explain this?
4/4
------
Loved the first episode! Nice way to keep up with DL via a fun conversational interview.
------
Just finished recording episode #2 of The AI Buzz with @lantiga BAM! Come join us for the premiere on Tuesday at 11am (New York time): https://youtu.be/1xBB8iAXxds
------
Totally agree! Big fan of project-based learning. 
And my students always had a lot of fun with their class projects (+ it gives them something for their resume!)
https://proceedings.mlr.press/v170/raschka22a.html…
------
Solving problems is the best way to learn Machine Learning.

But most people don't know where to start.

Here are 10 projects you can use to kick off your journey:
------
Here are some fun student project examples:
------
Regarding choosing a good batch size:
Choosing a batch size to be as large as your hardware permits (up to a point) seems to be a good recommendation.
Compiled some resources via the figure below.
------
Was doing some more search and found a relevant post from 
@ylecun
 that argues for a smaller batch size. The discussion remains interesting :P
------
Training with large minibatches is bad for your health.
More importantly, it's bad for your test error.
Friends dont let friends use minibatches larger than 32. https://arxiv.org/abs/1804.07612
------
When I asked ChatGPT the "Why do ViTs need more data than ConvNets" question this morning, I saw the biggest blunder yet

> "Fewer parameters: Vision transformers have fewer parameters than CNNs, this means that they require more data [...]."

(via https://leanpub.com/machine-learning-q-and-ai/…)
------
Another one regarding disadvantages of a large k in k-fold CV: 

> Reduced data for training: As k increases, the size of each fold decreases, which means that less data is available for training the model, which can lead to poor performance when the model is applied to new data.
------
@rasbt
's new book, with its Q&A format, is a valuable resource for learning and internalizing the fundamental concepts of machine learning. Can't wait to read the next chapters!

"Machine Learning Q and AI – Expand your machine learning knowledge with 30 questions and answers"
------
Just saw this post, and what a fun coincidence. Read all of these, and I can definitely recommend them all as well!
------
Best data science books to read in 2023 
https://link.medium.com/3fIui790Ewb 

#DataScience #MachineLearning #mlops
------
Just saw this post, and what a fun coincidence. Read all of these, and I can definitely recommend them all as well!
------
Best data science books to read in 2023 
https://link.medium.com/3fIui790Ewb 

#DataScience #MachineLearning #mlops
------
PS: I remember reading and liking 
@jakevdp
's book, but I  must have donated that some time back in 2018 when I moved to Madison.
And a little disclaimer, I never finished the Math for ML one.
------
denoising diffusion generative models are actually
𝗶𝗻𝘃𝗲𝗿𝘀𝗲 diffusions
------
Just reading the Deep Learning Tuning Playbook (https://github.com/google-research/tuning_playbook/blob/main/README.md…).

"The batch size governs the training speed and shouldn't be used to directly tune the validation set performance. Often, the ideal batch size will be the largest batch size supported [...]"

Good advice 
------
In other words, choose it as large as your hardware allows, but don't treat it as a tunable hyperparameter. But there is also no point in using larger batch sizes if it increased the training time.
------
Hi friends, I've started a crowdsourced Google Doc for open positions and helpful volunteer contacts to help laid-off Googlers find jobs. Please add any openings you know of, and list your contact information if you're willing to help!
------
Machine Learning Q and AI -- Expand your machine learning & AI knowledge with 30 questions and answers.

An early-access of my new book, my first in the era of ChatGPT, is now on Leanpub: https://leanpub.com/machine-learning-q-and-ai…

Would love to hear what you think!

Happy weekend & happy learning!
------
If I am not too optimistic, the ETA for the complete version is Q1/Q2 2023 
I have most questions already written last year. 
The plan is to be editing + formatting 2-3 q's per weekend. 
(Some of you may recognize snippets from all my social media posts over the years )
------
Now this paper has been accepted to #ICLR2023 as a spotlight! I truly appreciate everyone's efforts. Check out our repo http://github.com/keyu-tian/SparK for the latest demo video and updates! Title: "Designing BERT for Convolutional Networks: Sparse and Hierarchical Masked Modeling"
------
How can we leverage successful pretraining techniques from transformers to improve purely convolutional networks? The answer is *Sparse Convolutions*!

Let's see what happens when purely convolutional networks are pretrained with 1.28 million unlabeled images ...

1/7
------
Derivatives of regular supervised learning when you don't have enough labeled data:

1.  Transfer learning
2.  Self-supervised learning
3.  Semi-supervised learning
4.  Few-shot learning
5.  Meta-learning
6.  Active learning
7.  Weakly supervised learning
8.  Multi-task learning
------
4.  Few-shot learning: learn from a very small number of examples/class (e.g., 1 or 5) using meta-learning, transfer learning, or metric-based methods.

5.  Meta-learning: two definitions, a) either learn from meta-features, b) train a meta-learner in few-shot contexts
------
6.  Active learning: actively select the most informative examples to be labeled by a human

7.  Weakly supervised learning: create noisy, limited, or imprecise labels for a supervised learner

8.  Multi-task learning: train with multiple losses to perform well on multiple tasks
------
Big shoutout for 
@xamat
's LLM family tree blog post! It comes with nice & concise summaries of each model (63 pages, if you export it as a PDF!)

https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/…
------
The DL Fundamentals lectures will incl additional fun stuff & goodies that are usually not included in most other DL resources (like my book)

- learning rate schedulers
- automating hparam tuning
- logging & checkpointing
- mixed precision & quantization & multi-GPU training
...
------
What will you be able to do after @rasbt's Deep Learning Fundamentals course?

 Build classifiers for various kinds of data like tables, images, and text.

Tune models effectively to optimize predictive and computational performance

Get started now! https://lightning.ai/pages/courses/deep-learning-fundamentals/…
------
Better idea: let's make each of the 250 operators in PyTorch 2.0's PrimTorch a trading card.
------
Who wants to port pytorch to punch cards?
------
From a recent collaboration: Gradient boosting is usually great, but beware of overtuning. And no, k-fold cross-validation does not prevent overfitting. 

Tune a large number of hyperparameters, and you will overfit to your k-fold partitions:
------
Btw I am a big fan of k-fold cross validation. Don’t want to imply that cross-validation (k-fold or nested) is bad.
Just wanted to make a point that it doesn’t completely save you from overfitting if you run crazy extensive hyperparameter tuning sessions.
------
I mean it ! 
I wrote an article on model evaluation a few years back, and it's literally like an ad for cross-validation. And I didn't change my mind since then !
------
The Machine Learning FAQs by 
@rasbt
 are a gold mine for new and experienced data scientists.
------
Classic bias-variance theory suggests that reducing model size can reduce overfitting. Why? As a rule of thumb, the smaller the number of model parameters the smaller its capacity -- a smaller capacity reduces the model's capacity to memorize or overfit to noise in the data. 1/4
------
How can we reconcile the observation that smaller (e.g., pruned) models can exhibit better generalization performance with contradictory observations from studies of double-decent and grokking? 
3/4
------
Recent research showed that the reduction of overfitting in smaller models (e.g. via pruning) can be partly explained by the improved training processes (https://arxiv.org/abs/2210.13738).

I wonder if there are any alternative theories or pointers to explain this?
4/4
------
Large Transformer Model Inference Optimization (https://lilianweng.github.io/posts/2023-01-10-inference-optimization/…)-- a really nice & thorough article by 
@lilianweng
.

Interesting tidbit: low-bit quantization can cause severe qualitative drops due to outliers; fixes are mixed-precision techniques and outlier smoothing
------
My GPT-3 book moved from #1 new release in NLP to #1 bestseller in NLP on Amazon 

My last post went viral and pushed the book to #1 in the NLP category, wouldn't be possible with you all! 

Thank you so much everyone for the amazing support and feedback 
------
Yes, sitting down and spending a few hours on fixing labeling issues can sometimes save you days fiddling with hyperparameters!

(Days in terms of both human days and GPU days!) 

As an added bonus: you get to keep & reuse the improved dataset for any future model.
------
Most people don't know this:

MNIST is the most popular dataset in Machine Learning, but despite millions of people trying, there has never been a model that solves it with 100% accuracy.

And I just found out why and how to fix it!

1 of 5
------
Another one regarding disadvantages of a large k in k-fold CV: 

> Reduced data for training: As k increases, the size of each fold decreases, which means that less data is available for training the model, which can lead to poor performance when the model is applied to new data.
------
@rasbt
's new book, with its Q&A format, is a valuable resource for learning and internalizing the fundamental concepts of machine learning. Can't wait to read the next chapters!

"Machine Learning Q and AI – Expand your machine learning knowledge with 30 questions and answers"
------
Just saw this post, and what a fun coincidence. Read all of these, and I can definitely recommend them all as well!
------
Best data science books to read in 2023 
https://link.medium.com/3fIui790Ewb 

#DataScience #MachineLearning #mlops
------
Just saw this post, and what a fun coincidence. Read all of these, and I can definitely recommend them all as well!
------
Best data science books to read in 2023 
https://link.medium.com/3fIui790Ewb 

#DataScience #MachineLearning #mlops
------
PS: I remember reading and liking 
@jakevdp
's book, but I  must have donated that some time back in 2018 when I moved to Madison.
And a little disclaimer, I never finished the Math for ML one.
------
denoising diffusion generative models are actually
𝗶𝗻𝘃𝗲𝗿𝘀𝗲 diffusions
------
Just reading the Deep Learning Tuning Playbook (https://github.com/google-research/tuning_playbook/blob/main/README.md…).

"The batch size governs the training speed and shouldn't be used to directly tune the validation set performance. Often, the ideal batch size will be the largest batch size supported [...]"

Good advice 
------
In other words, choose it as large as your hardware allows, but don't treat it as a tunable hyperparameter. But there is also no point in using larger batch sizes if it increased the training time.
------
Hi friends, I've started a crowdsourced Google Doc for open positions and helpful volunteer contacts to help laid-off Googlers find jobs. Please add any openings you know of, and list your contact information if you're willing to help!
------
Machine Learning Q and AI -- Expand your machine learning & AI knowledge with 30 questions and answers.

An early-access of my new book, my first in the era of ChatGPT, is now on Leanpub: https://leanpub.com/machine-learning-q-and-ai…

Would love to hear what you think!

Happy weekend & happy learning!
------
If I am not too optimistic, the ETA for the complete version is Q1/Q2 2023 
I have most questions already written last year. 
The plan is to be editing + formatting 2-3 q's per weekend. 
(Some of you may recognize snippets from all my social media posts over the years )
------
Now this paper has been accepted to #ICLR2023 as a spotlight! I truly appreciate everyone's efforts. Check out our repo http://github.com/keyu-tian/SparK for the latest demo video and updates! Title: "Designing BERT for Convolutional Networks: Sparse and Hierarchical Masked Modeling"
------
How can we leverage successful pretraining techniques from transformers to improve purely convolutional networks? The answer is *Sparse Convolutions*!

Let's see what happens when purely convolutional networks are pretrained with 1.28 million unlabeled images ...

1/7
------
Derivatives of regular supervised learning when you don't have enough labeled data:

1.  Transfer learning
2.  Self-supervised learning
3.  Semi-supervised learning
4.  Few-shot learning
5.  Meta-learning
6.  Active learning
7.  Weakly supervised learning
8.  Multi-task learning
------
4.  Few-shot learning: learn from a very small number of examples/class (e.g., 1 or 5) using meta-learning, transfer learning, or metric-based methods.

5.  Meta-learning: two definitions, a) either learn from meta-features, b) train a meta-learner in few-shot contexts
------
6.  Active learning: actively select the most informative examples to be labeled by a human

7.  Weakly supervised learning: create noisy, limited, or imprecise labels for a supervised learner

8.  Multi-task learning: train with multiple losses to perform well on multiple tasks
------
Big shoutout for 
@xamat
's LLM family tree blog post! It comes with nice & concise summaries of each model (63 pages, if you export it as a PDF!)

https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/…
------
The DL Fundamentals lectures will incl additional fun stuff & goodies that are usually not included in most other DL resources (like my book)

- learning rate schedulers
- automating hparam tuning
- logging & checkpointing
- mixed precision & quantization & multi-GPU training
...
------
What will you be able to do after @rasbt's Deep Learning Fundamentals course?

 Build classifiers for various kinds of data like tables, images, and text.

Tune models effectively to optimize predictive and computational performance

Get started now! https://lightning.ai/pages/courses/deep-learning-fundamentals/…
------
Better idea: let's make each of the 250 operators in PyTorch 2.0's PrimTorch a trading card.
------
Who wants to port pytorch to punch cards?
------
From a recent collaboration: Gradient boosting is usually great, but beware of overtuning. And no, k-fold cross-validation does not prevent overfitting. 

Tune a large number of hyperparameters, and you will overfit to your k-fold partitions:
------
Btw I am a big fan of k-fold cross validation. Don’t want to imply that cross-validation (k-fold or nested) is bad.
Just wanted to make a point that it doesn’t completely save you from overfitting if you run crazy extensive hyperparameter tuning sessions.
------
I mean it ! 
I wrote an article on model evaluation a few years back, and it's literally like an ad for cross-validation. And I didn't change my mind since then !
------
The Machine Learning FAQs by 
@rasbt
 are a gold mine for new and experienced data scientists.
------
Classic bias-variance theory suggests that reducing model size can reduce overfitting. Why? As a rule of thumb, the smaller the number of model parameters the smaller its capacity -- a smaller capacity reduces the model's capacity to memorize or overfit to noise in the data. 1/4
------
How can we reconcile the observation that smaller (e.g., pruned) models can exhibit better generalization performance with contradictory observations from studies of double-decent and grokking? 
3/4
------
Recent research showed that the reduction of overfitting in smaller models (e.g. via pruning) can be partly explained by the improved training processes (https://arxiv.org/abs/2210.13738).

I wonder if there are any alternative theories or pointers to explain this?
4/4
------
Large Transformer Model Inference Optimization (https://lilianweng.github.io/posts/2023-01-10-inference-optimization/…)-- a really nice & thorough article by 
@lilianweng
.

Interesting tidbit: low-bit quantization can cause severe qualitative drops due to outliers; fixes are mixed-precision techniques and outlier smoothing
------
My GPT-3 book moved from #1 new release in NLP to #1 bestseller in NLP on Amazon 

My last post went viral and pushed the book to #1 in the NLP category, wouldn't be possible with you all! 

Thank you so much everyone for the amazing support and feedback 
------
Yes, sitting down and spending a few hours on fixing labeling issues can sometimes save you days fiddling with hyperparameters!

(Days in terms of both human days and GPU days!) 

As an added bonus: you get to keep & reuse the improved dataset for any future model.
------
Most people don't know this:

MNIST is the most popular dataset in Machine Learning, but despite millions of people trying, there has never been a model that solves it with 100% accuracy.

And I just found out why and how to fix it!

1 of 5
------
What do you want more of from Lightning in 2023?
------
Large language models are more like a thesaurus, but for complete sentences and paragraphs.
------
The best analogy that I‘ve found for AI is that it’s like a calculator for reading and writing.
------
In contrast, to me, a calculator is something that's precise & deterministic, the opposite of current-gen AI.
------
How can we leverage successful pretraining techniques from transformers to improve purely convolutional networks? The answer is *Sparse Convolutions*!

Let's see what happens when purely convolutional networks are pretrained with 1.28 million unlabeled images ...

1/7
------
Speaking of ConvNeXt, remember the ConvNeXt paper I shared a few days ago? Yes, this follows a similar idea. This paper here appears to have been published around the same time (perhaps even earlier if we consider the OpenReview version)

6/7
------
Anyways, here are some more resources and links if you want to check it out:

openpeview: https://openreview.net/forum?id=NRxydtWup1S…
arxiv: https://arxiv.org/abs/2301.03580
github: https://github.com/keyu-tian/SparK

7/7
------
In the RTX 40 post, I introduce a GPU recommendation chart and discuss the new Tensor Memory Accelerator (TMA) and FP8 computation. Overall, RTX 40s are faster for inference and shine through their FP8 performance but are inefficient for 16-bit training. https://timdettmers.com/2023/01/16/which-gpu-for-deep-learning/…
------
Just reading the Deep Learning Tuning Playbook (https://github.com/google-research/tuning_playbook/blob/main/README.md…).

"The batch size governs the training speed and shouldn't be used to directly tune the validation set performance. Often, the ideal batch size will be the largest batch size supported [...]"

Good advice 
------
In other words, choose it as large as your hardware allows, but don't treat it as a tunable hyperparameter. But there is also no point in using larger batch sizes if it increased the training time.
------
Hi friends, I've started a crowdsourced Google Doc for open positions and helpful volunteer contacts to help laid-off Googlers find jobs. Please add any openings you know of, and list your contact information if you're willing to help!
------
Machine Learning Q and AI -- Expand your machine learning & AI knowledge with 30 questions and answers.

An early-access of my new book, my first in the era of ChatGPT, is now on Leanpub: https://leanpub.com/machine-learning-q-and-ai…

Would love to hear what you think!

Happy weekend & happy learning!
------
If I am not too optimistic, the ETA for the complete version is Q1/Q2 2023 
I have most questions already written last year. 
The plan is to be editing + formatting 2-3 q's per weekend. 
(Some of you may recognize snippets from all my social media posts over the years )
------
Now this paper has been accepted to #ICLR2023 as a spotlight! I truly appreciate everyone's efforts. Check out our repo http://github.com/keyu-tian/SparK for the latest demo video and updates! Title: "Designing BERT for Convolutional Networks: Sparse and Hierarchical Masked Modeling"
------
How can we leverage successful pretraining techniques from transformers to improve purely convolutional networks? The answer is *Sparse Convolutions*!

Let's see what happens when purely convolutional networks are pretrained with 1.28 million unlabeled images ...

1/7
------
Derivatives of regular supervised learning when you don't have enough labeled data:

1.  Transfer learning
2.  Self-supervised learning
3.  Semi-supervised learning
4.  Few-shot learning
5.  Meta-learning
6.  Active learning
7.  Weakly supervised learning
8.  Multi-task learning
------
4.  Few-shot learning: learn from a very small number of examples/class (e.g., 1 or 5) using meta-learning, transfer learning, or metric-based methods.

5.  Meta-learning: two definitions, a) either learn from meta-features, b) train a meta-learner in few-shot contexts
------
6.  Active learning: actively select the most informative examples to be labeled by a human

7.  Weakly supervised learning: create noisy, limited, or imprecise labels for a supervised learner

8.  Multi-task learning: train with multiple losses to perform well on multiple tasks
------
Big shoutout for 
@xamat
's LLM family tree blog post! It comes with nice & concise summaries of each model (63 pages, if you export it as a PDF!)

https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/…
------
The DL Fundamentals lectures will incl additional fun stuff & goodies that are usually not included in most other DL resources (like my book)

- learning rate schedulers
- automating hparam tuning
- logging & checkpointing
- mixed precision & quantization & multi-GPU training
...
------
What will you be able to do after @rasbt's Deep Learning Fundamentals course?

 Build classifiers for various kinds of data like tables, images, and text.

Tune models effectively to optimize predictive and computational performance

Get started now! https://lightning.ai/pages/courses/deep-learning-fundamentals/…
------
Better idea: let's make each of the 250 operators in PyTorch 2.0's PrimTorch a trading card.
------
Who wants to port pytorch to punch cards?
------
From a recent collaboration: Gradient boosting is usually great, but beware of overtuning. And no, k-fold cross-validation does not prevent overfitting. 

Tune a large number of hyperparameters, and you will overfit to your k-fold partitions:
------
Btw I am a big fan of k-fold cross validation. Don’t want to imply that cross-validation (k-fold or nested) is bad.
Just wanted to make a point that it doesn’t completely save you from overfitting if you run crazy extensive hyperparameter tuning sessions.
------
I mean it ! 
I wrote an article on model evaluation a few years back, and it's literally like an ad for cross-validation. And I didn't change my mind since then !
------
The Machine Learning FAQs by 
@rasbt
 are a gold mine for new and experienced data scientists.
------
Classic bias-variance theory suggests that reducing model size can reduce overfitting. Why? As a rule of thumb, the smaller the number of model parameters the smaller its capacity -- a smaller capacity reduces the model's capacity to memorize or overfit to noise in the data. 1/4
------
How can we reconcile the observation that smaller (e.g., pruned) models can exhibit better generalization performance with contradictory observations from studies of double-decent and grokking? 
3/4
------
Recent research showed that the reduction of overfitting in smaller models (e.g. via pruning) can be partly explained by the improved training processes (https://arxiv.org/abs/2210.13738).

I wonder if there are any alternative theories or pointers to explain this?
4/4
------
Large Transformer Model Inference Optimization (https://lilianweng.github.io/posts/2023-01-10-inference-optimization/…)-- a really nice & thorough article by 
@lilianweng
.

Interesting tidbit: low-bit quantization can cause severe qualitative drops due to outliers; fixes are mixed-precision techniques and outlier smoothing
------
My GPT-3 book moved from #1 new release in NLP to #1 bestseller in NLP on Amazon 

My last post went viral and pushed the book to #1 in the NLP category, wouldn't be possible with you all! 

Thank you so much everyone for the amazing support and feedback 
------
Yes, sitting down and spending a few hours on fixing labeling issues can sometimes save you days fiddling with hyperparameters!

(Days in terms of both human days and GPU days!) 

As an added bonus: you get to keep & reuse the improved dataset for any future model.
------
Most people don't know this:

MNIST is the most popular dataset in Machine Learning, but despite millions of people trying, there has never been a model that solves it with 100% accuracy.

And I just found out why and how to fix it!

1 of 5
------
What do you want more of from Lightning in 2023?
------
Large language models are more like a thesaurus, but for complete sentences and paragraphs.
------
The best analogy that I‘ve found for AI is that it’s like a calculator for reading and writing.
------
In contrast, to me, a calculator is something that's precise & deterministic, the opposite of current-gen AI.
------
How can we leverage successful pretraining techniques from transformers to improve purely convolutional networks? The answer is *Sparse Convolutions*!

Let's see what happens when purely convolutional networks are pretrained with 1.28 million unlabeled images ...

1/7
------
Speaking of ConvNeXt, remember the ConvNeXt paper I shared a few days ago? Yes, this follows a similar idea. This paper here appears to have been published around the same time (perhaps even earlier if we consider the OpenReview version)

6/7
------
Anyways, here are some more resources and links if you want to check it out:

openpeview: https://openreview.net/forum?id=NRxydtWup1S…
arxiv: https://arxiv.org/abs/2301.03580
github: https://github.com/keyu-tian/SparK

7/7
------
In the RTX 40 post, I introduce a GPU recommendation chart and discuss the new Tensor Memory Accelerator (TMA) and FP8 computation. Overall, RTX 40s are faster for inference and shine through their FP8 performance but are inefficient for 16-bit training. https://timdettmers.com/2023/01/16/which-gpu-for-deep-learning/…
------
Training factually correct LLMs requires vast amounts of factually correct training data.

Curating & creating factually correct training data for LLMs is very inefficient (vs using this data directly in the first place).

Wrote a shot post about it here:
------
Adding references DOES NOT automatically solve all the issues with misinformation. 
Eg consider the following sentence from Perplexity AI, which is obviously wrong. 

But it helps us understand why it's wrong (the model didn't dream it up, the training source contains the error.)
------
"DeepMind is also considering releasing its own chatbot, called Sparrow, for a “private beta” some time in 2023. (The delay is in order for DeepMind to work on reinforcement learning-based features that ChatGPT lacks, **like citing its sources**  ...)"

https://time.com/6246119/demis-hassabis-deepmind-interview/…
------
I think systems like ChatGPT would be a lot less controversial if they cited references.

Eg when querying “What are the origins of AI”, sure have it provide a summary. But also append the sources where it learned this info from. To a) give credit & b) so we can validate the info
------
Woke up and felt like writing a blog post on how to train an XGBoost Classifier with cloud GPUs, avoiding infrastructure headaches.
Happy Sunday! 


------
Derivatives of regular supervised learning when you don't have enough labeled data:

1.  Transfer learning
2.  Self-supervised learning
3.  Semi-supervised learning
4.  Few-shot learning
5.  Meta-learning
6.  Active learning
7.  Weakly supervised learning
8.  Multi-task learning
------
4.  Few-shot learning: learn from a very small number of examples/class (e.g., 1 or 5) using meta-learning, transfer learning, or metric-based methods.

5.  Meta-learning: two definitions, a) either learn from meta-features, b) train a meta-learner in few-shot contexts
------
6.  Active learning: actively select the most informative examples to be labeled by a human

7.  Weakly supervised learning: create noisy, limited, or imprecise labels for a supervised learner

8.  Multi-task learning: train with multiple losses to perform well on multiple tasks
------
Big shoutout for 
@xamat
's LLM family tree blog post! It comes with nice & concise summaries of each model (63 pages, if you export it as a PDF!)

https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/…
------
The DL Fundamentals lectures will incl additional fun stuff & goodies that are usually not included in most other DL resources (like my book)

- learning rate schedulers
- automating hparam tuning
- logging & checkpointing
- mixed precision & quantization & multi-GPU training
...
------
What will you be able to do after @rasbt's Deep Learning Fundamentals course?

 Build classifiers for various kinds of data like tables, images, and text.

Tune models effectively to optimize predictive and computational performance

Get started now! https://lightning.ai/pages/courses/deep-learning-fundamentals/…
------
Better idea: let's make each of the 250 operators in PyTorch 2.0's PrimTorch a trading card.
------
Who wants to port pytorch to punch cards?
------
From a recent collaboration: Gradient boosting is usually great, but beware of overtuning. And no, k-fold cross-validation does not prevent overfitting. 

Tune a large number of hyperparameters, and you will overfit to your k-fold partitions:
------
Btw I am a big fan of k-fold cross validation. Don’t want to imply that cross-validation (k-fold or nested) is bad.
Just wanted to make a point that it doesn’t completely save you from overfitting if you run crazy extensive hyperparameter tuning sessions.
------
I mean it ! 
I wrote an article on model evaluation a few years back, and it's literally like an ad for cross-validation. And I didn't change my mind since then !
------
The Machine Learning FAQs by 
@rasbt
 are a gold mine for new and experienced data scientists.
------
Classic bias-variance theory suggests that reducing model size can reduce overfitting. Why? As a rule of thumb, the smaller the number of model parameters the smaller its capacity -- a smaller capacity reduces the model's capacity to memorize or overfit to noise in the data. 1/4
------
How can we reconcile the observation that smaller (e.g., pruned) models can exhibit better generalization performance with contradictory observations from studies of double-decent and grokking? 
3/4
------
Recent research showed that the reduction of overfitting in smaller models (e.g. via pruning) can be partly explained by the improved training processes (https://arxiv.org/abs/2210.13738).

I wonder if there are any alternative theories or pointers to explain this?
4/4
------
Large Transformer Model Inference Optimization (https://lilianweng.github.io/posts/2023-01-10-inference-optimization/…)-- a really nice & thorough article by 
@lilianweng
.

Interesting tidbit: low-bit quantization can cause severe qualitative drops due to outliers; fixes are mixed-precision techniques and outlier smoothing
------
My GPT-3 book moved from #1 new release in NLP to #1 bestseller in NLP on Amazon 

My last post went viral and pushed the book to #1 in the NLP category, wouldn't be possible with you all! 

Thank you so much everyone for the amazing support and feedback 
------
Yes, sitting down and spending a few hours on fixing labeling issues can sometimes save you days fiddling with hyperparameters!

(Days in terms of both human days and GPU days!) 

As an added bonus: you get to keep & reuse the improved dataset for any future model.
------
Most people don't know this:

MNIST is the most popular dataset in Machine Learning, but despite millions of people trying, there has never been a model that solves it with 100% accuracy.

And I just found out why and how to fix it!

1 of 5
------
What do you want more of from Lightning in 2023?
------
Large language models are more like a thesaurus, but for complete sentences and paragraphs.
------
The best analogy that I‘ve found for AI is that it’s like a calculator for reading and writing.
------
In contrast, to me, a calculator is something that's precise & deterministic, the opposite of current-gen AI.
------
How can we leverage successful pretraining techniques from transformers to improve purely convolutional networks? The answer is *Sparse Convolutions*!

Let's see what happens when purely convolutional networks are pretrained with 1.28 million unlabeled images ...

1/7
------
Speaking of ConvNeXt, remember the ConvNeXt paper I shared a few days ago? Yes, this follows a similar idea. This paper here appears to have been published around the same time (perhaps even earlier if we consider the OpenReview version)

6/7
------
Anyways, here are some more resources and links if you want to check it out:

openpeview: https://openreview.net/forum?id=NRxydtWup1S…
arxiv: https://arxiv.org/abs/2301.03580
github: https://github.com/keyu-tian/SparK

7/7
------
In the RTX 40 post, I introduce a GPU recommendation chart and discuss the new Tensor Memory Accelerator (TMA) and FP8 computation. Overall, RTX 40s are faster for inference and shine through their FP8 performance but are inefficient for 16-bit training. https://timdettmers.com/2023/01/16/which-gpu-for-deep-learning/…
------
Training factually correct LLMs requires vast amounts of factually correct training data.

Curating & creating factually correct training data for LLMs is very inefficient (vs using this data directly in the first place).

Wrote a shot post about it here:
------
Adding references DOES NOT automatically solve all the issues with misinformation. 
Eg consider the following sentence from Perplexity AI, which is obviously wrong. 

But it helps us understand why it's wrong (the model didn't dream it up, the training source contains the error.)
------
"DeepMind is also considering releasing its own chatbot, called Sparrow, for a “private beta” some time in 2023. (The delay is in order for DeepMind to work on reinforcement learning-based features that ChatGPT lacks, **like citing its sources**  ...)"

https://time.com/6246119/demis-hassabis-deepmind-interview/…
------
I think systems like ChatGPT would be a lot less controversial if they cited references.

Eg when querying “What are the origins of AI”, sure have it provide a summary. But also append the sources where it learned this info from. To a) give credit & b) so we can validate the info
------
Woke up and felt like writing a blog post on how to train an XGBoost Classifier with cloud GPUs, avoiding infrastructure headaches.
Happy Sunday! 


------
The story of weight decay in pictures:

weight decay ...
1)  improves data efficiency by > 50%
2) is frequently found in the best hyperparam configs
3) is among the most important hparams to tune
4) is also tricky to tune
------
References:
1) "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets"
https://arxiv.org/abs/2201.02177

2) "Well-tuned Simple Nets Excel on Tabular Datasets"
https://arxiv.org/abs/2106.11189

3) "S4L: Self-Supervised Semi-Supervised Learning"
------
4) "Big Transfer (BiT): General Visual Representation Learning"
------
Yes weight decay is criminally under-tuned.

From us:
- It was *the* key hparam going from 100% to 10% to 1% of ImageNet in https://arxiv.org/abs/1905.03670
- see this figure in BiT paper (https://arxiv.org/abs/1912.11370) for how it can be misleading hence hard to tune:
------
From a recent collaboration: Gradient boosting is usually great, but beware of overtuning. And no, k-fold cross-validation does not prevent overfitting. 

Tune a large number of hyperparameters, and you will overfit to your k-fold partitions:
------
Btw I am a big fan of k-fold cross validation. Don’t want to imply that cross-validation (k-fold or nested) is bad.
Just wanted to make a point that it doesn’t completely save you from overfitting if you run crazy extensive hyperparameter tuning sessions.
------
I mean it ! 
I wrote an article on model evaluation a few years back, and it's literally like an ad for cross-validation. And I didn't change my mind since then !
------
The Machine Learning FAQs by 
@rasbt
 are a gold mine for new and experienced data scientists.
------
Classic bias-variance theory suggests that reducing model size can reduce overfitting. Why? As a rule of thumb, the smaller the number of model parameters the smaller its capacity -- a smaller capacity reduces the model's capacity to memorize or overfit to noise in the data. 1/4
------
How can we reconcile the observation that smaller (e.g., pruned) models can exhibit better generalization performance with contradictory observations from studies of double-decent and grokking? 
3/4
------
Recent research showed that the reduction of overfitting in smaller models (e.g. via pruning) can be partly explained by the improved training processes (https://arxiv.org/abs/2210.13738).

I wonder if there are any alternative theories or pointers to explain this?
4/4
------
Large Transformer Model Inference Optimization (https://lilianweng.github.io/posts/2023-01-10-inference-optimization/…)-- a really nice & thorough article by 
@lilianweng
.

Interesting tidbit: low-bit quantization can cause severe qualitative drops due to outliers; fixes are mixed-precision techniques and outlier smoothing
------
My GPT-3 book moved from #1 new release in NLP to #1 bestseller in NLP on Amazon 

My last post went viral and pushed the book to #1 in the NLP category, wouldn't be possible with you all! 

Thank you so much everyone for the amazing support and feedback 
------
Yes, sitting down and spending a few hours on fixing labeling issues can sometimes save you days fiddling with hyperparameters!

(Days in terms of both human days and GPU days!) 

As an added bonus: you get to keep & reuse the improved dataset for any future model.
------
Most people don't know this:

MNIST is the most popular dataset in Machine Learning, but despite millions of people trying, there has never been a model that solves it with 100% accuracy.

And I just found out why and how to fix it!

1 of 5
------
What do you want more of from Lightning in 2023?
------
Large language models are more like a thesaurus, but for complete sentences and paragraphs.
------
The best analogy that I‘ve found for AI is that it’s like a calculator for reading and writing.
------
In contrast, to me, a calculator is something that's precise & deterministic, the opposite of current-gen AI.
------
How can we leverage successful pretraining techniques from transformers to improve purely convolutional networks? The answer is *Sparse Convolutions*!

Let's see what happens when purely convolutional networks are pretrained with 1.28 million unlabeled images ...

1/7
------
Speaking of ConvNeXt, remember the ConvNeXt paper I shared a few days ago? Yes, this follows a similar idea. This paper here appears to have been published around the same time (perhaps even earlier if we consider the OpenReview version)

6/7
------
Anyways, here are some more resources and links if you want to check it out:

openpeview: https://openreview.net/forum?id=NRxydtWup1S…
arxiv: https://arxiv.org/abs/2301.03580
github: https://github.com/keyu-tian/SparK

7/7
------
In the RTX 40 post, I introduce a GPU recommendation chart and discuss the new Tensor Memory Accelerator (TMA) and FP8 computation. Overall, RTX 40s are faster for inference and shine through their FP8 performance but are inefficient for 16-bit training. https://timdettmers.com/2023/01/16/which-gpu-for-deep-learning/…
------
Training factually correct LLMs requires vast amounts of factually correct training data.

Curating & creating factually correct training data for LLMs is very inefficient (vs using this data directly in the first place).

Wrote a shot post about it here:
------
Adding references DOES NOT automatically solve all the issues with misinformation. 
Eg consider the following sentence from Perplexity AI, which is obviously wrong. 

But it helps us understand why it's wrong (the model didn't dream it up, the training source contains the error.)
------
"DeepMind is also considering releasing its own chatbot, called Sparrow, for a “private beta” some time in 2023. (The delay is in order for DeepMind to work on reinforcement learning-based features that ChatGPT lacks, **like citing its sources**  ...)"

https://time.com/6246119/demis-hassabis-deepmind-interview/…
------
I think systems like ChatGPT would be a lot less controversial if they cited references.

Eg when querying “What are the origins of AI”, sure have it provide a summary. But also append the sources where it learned this info from. To a) give credit & b) so we can validate the info
------
Woke up and felt like writing a blog post on how to train an XGBoost Classifier with cloud GPUs, avoiding infrastructure headaches.
Happy Sunday! 


------
The story of weight decay in pictures:

weight decay ...
1)  improves data efficiency by > 50%
2) is frequently found in the best hyperparam configs
3) is among the most important hparams to tune
4) is also tricky to tune
------
References:
1) "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets"
https://arxiv.org/abs/2201.02177

2) "Well-tuned Simple Nets Excel on Tabular Datasets"
https://arxiv.org/abs/2106.11189

3) "S4L: Self-Supervised Semi-Supervised Learning"
------
4) "Big Transfer (BiT): General Visual Representation Learning"
------
Yes weight decay is criminally under-tuned.

From us:
- It was *the* key hparam going from 100% to 10% to 1% of ImageNet in https://arxiv.org/abs/1905.03670
- see this figure in BiT paper (https://arxiv.org/abs/1912.11370) for how it can be misleading hence hard to tune:
------
Weight decay (with AdamW) can yield a significant improvement in data efficiency compared to other methods (like Dropout), reducing the required amount of samples by over 50%
(via "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets, https://arxiv.org/abs/2201.02177)
------
And as a companion: Effective ways to reduce overfitting *via model changes*:

1. Regularization: L2 penalty, weight decay, dropout, early stopping
2. Smaller models: lottery ticket hypothesis, knowledge distillation
3. Ensemble models

https://twitter.com/rasbt/status/1613170269186854913?s=20&t=6n86jbUUee1cCvIGSSkZ2A…
------
**13 regularization techniques compared on tabular datasets in https://arxiv.org/abs/2106.11189

WD is a strong contributor


------
Even wrote a summary back then . Looks like WD is one of the big contributors to the best hparam cocktail
------
Large Transformer Model Inference Optimization (https://lilianweng.github.io/posts/2023-01-10-inference-optimization/…)-- a really nice & thorough article by 
@lilianweng
.

Interesting tidbit: low-bit quantization can cause severe qualitative drops due to outliers; fixes are mixed-precision techniques and outlier smoothing
------
My GPT-3 book moved from #1 new release in NLP to #1 bestseller in NLP on Amazon 

My last post went viral and pushed the book to #1 in the NLP category, wouldn't be possible with you all! 

Thank you so much everyone for the amazing support and feedback 
------
Yes, sitting down and spending a few hours on fixing labeling issues can sometimes save you days fiddling with hyperparameters!

(Days in terms of both human days and GPU days!) 

As an added bonus: you get to keep & reuse the improved dataset for any future model.
------
Most people don't know this:

MNIST is the most popular dataset in Machine Learning, but despite millions of people trying, there has never been a model that solves it with 100% accuracy.

And I just found out why and how to fix it!

1 of 5
------
What do you want more of from Lightning in 2023?
------
Large language models are more like a thesaurus, but for complete sentences and paragraphs.
------
The best analogy that I‘ve found for AI is that it’s like a calculator for reading and writing.
------
In contrast, to me, a calculator is something that's precise & deterministic, the opposite of current-gen AI.
------
How can we leverage successful pretraining techniques from transformers to improve purely convolutional networks? The answer is *Sparse Convolutions*!

Let's see what happens when purely convolutional networks are pretrained with 1.28 million unlabeled images ...

1/7
------
Speaking of ConvNeXt, remember the ConvNeXt paper I shared a few days ago? Yes, this follows a similar idea. This paper here appears to have been published around the same time (perhaps even earlier if we consider the OpenReview version)

6/7
------
Anyways, here are some more resources and links if you want to check it out:

openpeview: https://openreview.net/forum?id=NRxydtWup1S…
arxiv: https://arxiv.org/abs/2301.03580
github: https://github.com/keyu-tian/SparK

7/7
------
In the RTX 40 post, I introduce a GPU recommendation chart and discuss the new Tensor Memory Accelerator (TMA) and FP8 computation. Overall, RTX 40s are faster for inference and shine through their FP8 performance but are inefficient for 16-bit training. https://timdettmers.com/2023/01/16/which-gpu-for-deep-learning/…
------
Training factually correct LLMs requires vast amounts of factually correct training data.

Curating & creating factually correct training data for LLMs is very inefficient (vs using this data directly in the first place).

Wrote a shot post about it here:
------
Adding references DOES NOT automatically solve all the issues with misinformation. 
Eg consider the following sentence from Perplexity AI, which is obviously wrong. 

But it helps us understand why it's wrong (the model didn't dream it up, the training source contains the error.)
------
"DeepMind is also considering releasing its own chatbot, called Sparrow, for a “private beta” some time in 2023. (The delay is in order for DeepMind to work on reinforcement learning-based features that ChatGPT lacks, **like citing its sources**  ...)"

https://time.com/6246119/demis-hassabis-deepmind-interview/…
------
I think systems like ChatGPT would be a lot less controversial if they cited references.

Eg when querying “What are the origins of AI”, sure have it provide a summary. But also append the sources where it learned this info from. To a) give credit & b) so we can validate the info
------
Woke up and felt like writing a blog post on how to train an XGBoost Classifier with cloud GPUs, avoiding infrastructure headaches.
Happy Sunday! 


------
The story of weight decay in pictures:

weight decay ...
1)  improves data efficiency by > 50%
2) is frequently found in the best hyperparam configs
3) is among the most important hparams to tune
4) is also tricky to tune
------
References:
1) "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets"
https://arxiv.org/abs/2201.02177

2) "Well-tuned Simple Nets Excel on Tabular Datasets"
https://arxiv.org/abs/2106.11189

3) "S4L: Self-Supervised Semi-Supervised Learning"
------
4) "Big Transfer (BiT): General Visual Representation Learning"
------
Yes weight decay is criminally under-tuned.

From us:
- It was *the* key hparam going from 100% to 10% to 1% of ImageNet in https://arxiv.org/abs/1905.03670
- see this figure in BiT paper (https://arxiv.org/abs/1912.11370) for how it can be misleading hence hard to tune:
------
Weight decay (with AdamW) can yield a significant improvement in data efficiency compared to other methods (like Dropout), reducing the required amount of samples by over 50%
(via "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets, https://arxiv.org/abs/2201.02177)
------
And as a companion: Effective ways to reduce overfitting *via model changes*:

1. Regularization: L2 penalty, weight decay, dropout, early stopping
2. Smaller models: lottery ticket hypothesis, knowledge distillation
3. Ensemble models

https://twitter.com/rasbt/status/1613170269186854913?s=20&t=6n86jbUUee1cCvIGSSkZ2A…
------
**13 regularization techniques compared on tabular datasets in https://arxiv.org/abs/2106.11189

WD is a strong contributor


------
Even wrote a summary back then . Looks like WD is one of the big contributors to the best hparam cocktail
------
* This is for a specific, synthetic dataset they investigated. I wonder if there are any recent, larger benchmarks for various overfitting remedies.
------
TMLR numbers:
- 651 submissions
- 188 accepted papers
- currently ~100 submissions/month
- 189 action editors
- 1846 reviewers
- acceptance rate: 62% (46% if you count desk rejections and withdrawls)
- median time to decision: 76 days (#NeurIPS2022: 118 days, JMLR: >200 days) 2/n
------
Ready to start the day 
@rasbt
------
Agreed, there will be many open source implementations of ChatGPT. But there won't be many high-quality models.
I think we underestimate how much people hate labeling (or worse: writing) training data by hand.
------
Prediction:

there'll be 10 open reproductions of ChatGPT in 6 months

and if i understand correctly, models are quite smaller than LLMs so they might be significantly easier to deploy at scale (maybe even single-GPU)
------
In an academic collaboration, it was hard to convince my collaborators to even label 50 additional short texts for training a classifier. 
They preferred spending days (if not weeks) fine-tuning hyperparameters.
------
Interested in learning or transitioning to 
@PyTorch
 over the weekend?

The core API can be explained in two steps:
(1) defining the model, and
(2) setting up the training loop.

I have a concise 3-minute video explaining the PyTorch API here: https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/3-5-the-pytorch-api-parts-1-2/…
------
How can we leverage successful pretraining techniques from transformers to improve purely convolutional networks? The answer is *Sparse Convolutions*!

Let's see what happens when purely convolutional networks are pretrained with 1.28 million unlabeled images ...

1/7
------
Speaking of ConvNeXt, remember the ConvNeXt paper I shared a few days ago? Yes, this follows a similar idea. This paper here appears to have been published around the same time (perhaps even earlier if we consider the OpenReview version)

6/7
------
Anyways, here are some more resources and links if you want to check it out:

openpeview: https://openreview.net/forum?id=NRxydtWup1S…
arxiv: https://arxiv.org/abs/2301.03580
github: https://github.com/keyu-tian/SparK

7/7
------
In the RTX 40 post, I introduce a GPU recommendation chart and discuss the new Tensor Memory Accelerator (TMA) and FP8 computation. Overall, RTX 40s are faster for inference and shine through their FP8 performance but are inefficient for 16-bit training. https://timdettmers.com/2023/01/16/which-gpu-for-deep-learning/…
------
Training factually correct LLMs requires vast amounts of factually correct training data.

Curating & creating factually correct training data for LLMs is very inefficient (vs using this data directly in the first place).

Wrote a shot post about it here:
------
Adding references DOES NOT automatically solve all the issues with misinformation. 
Eg consider the following sentence from Perplexity AI, which is obviously wrong. 

But it helps us understand why it's wrong (the model didn't dream it up, the training source contains the error.)
------
"DeepMind is also considering releasing its own chatbot, called Sparrow, for a “private beta” some time in 2023. (The delay is in order for DeepMind to work on reinforcement learning-based features that ChatGPT lacks, **like citing its sources**  ...)"

https://time.com/6246119/demis-hassabis-deepmind-interview/…
------
I think systems like ChatGPT would be a lot less controversial if they cited references.

Eg when querying “What are the origins of AI”, sure have it provide a summary. But also append the sources where it learned this info from. To a) give credit & b) so we can validate the info
------
Woke up and felt like writing a blog post on how to train an XGBoost Classifier with cloud GPUs, avoiding infrastructure headaches.
Happy Sunday! 


------
The story of weight decay in pictures:

weight decay ...
1)  improves data efficiency by > 50%
2) is frequently found in the best hyperparam configs
3) is among the most important hparams to tune
4) is also tricky to tune
------
References:
1) "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets"
https://arxiv.org/abs/2201.02177

2) "Well-tuned Simple Nets Excel on Tabular Datasets"
https://arxiv.org/abs/2106.11189

3) "S4L: Self-Supervised Semi-Supervised Learning"
------
4) "Big Transfer (BiT): General Visual Representation Learning"
------
Yes weight decay is criminally under-tuned.

From us:
- It was *the* key hparam going from 100% to 10% to 1% of ImageNet in https://arxiv.org/abs/1905.03670
- see this figure in BiT paper (https://arxiv.org/abs/1912.11370) for how it can be misleading hence hard to tune:
------
Weight decay (with AdamW) can yield a significant improvement in data efficiency compared to other methods (like Dropout), reducing the required amount of samples by over 50%
(via "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets, https://arxiv.org/abs/2201.02177)
------
And as a companion: Effective ways to reduce overfitting *via model changes*:

1. Regularization: L2 penalty, weight decay, dropout, early stopping
2. Smaller models: lottery ticket hypothesis, knowledge distillation
3. Ensemble models

https://twitter.com/rasbt/status/1613170269186854913?s=20&t=6n86jbUUee1cCvIGSSkZ2A…
------
**13 regularization techniques compared on tabular datasets in https://arxiv.org/abs/2106.11189

WD is a strong contributor


------
Even wrote a summary back then . Looks like WD is one of the big contributors to the best hparam cocktail
------
* This is for a specific, synthetic dataset they investigated. I wonder if there are any recent, larger benchmarks for various overfitting remedies.
------
TMLR numbers:
- 651 submissions
- 188 accepted papers
- currently ~100 submissions/month
- 189 action editors
- 1846 reviewers
- acceptance rate: 62% (46% if you count desk rejections and withdrawls)
- median time to decision: 76 days (#NeurIPS2022: 118 days, JMLR: >200 days) 2/n
------
Ready to start the day 
@rasbt
------
Agreed, there will be many open source implementations of ChatGPT. But there won't be many high-quality models.
I think we underestimate how much people hate labeling (or worse: writing) training data by hand.
------
Prediction:

there'll be 10 open reproductions of ChatGPT in 6 months

and if i understand correctly, models are quite smaller than LLMs so they might be significantly easier to deploy at scale (maybe even single-GPU)
------
In an academic collaboration, it was hard to convince my collaborators to even label 50 additional short texts for training a classifier. 
They preferred spending days (if not weeks) fine-tuning hyperparameters.
------
Interested in learning or transitioning to 
@PyTorch
 over the weekend?

The core API can be explained in two steps:
(1) defining the model, and
(2) setting up the training loop.

I have a concise 3-minute video explaining the PyTorch API here: https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/3-5-the-pytorch-api-parts-1-2/…
------
Here we go. I guess that ship has sailed. Researchers started adding ChatGPT as co-author on their papers 

PS: Dear researchers, you forgot the version number!

https://medrxiv.org/content/10.1101/2022.12.19.22283643v2…

(thx 
@3scorciav
 for sharing this!)
------
Re: ChatGPT and authorship.

If a person creates or contributes results for a paper, this person is a coauthor. But this naturally doesn’t extend to models or algorithms.

Imagine AlphaFold were an author on the AlphaFold paper.

So, why would/should it be different for ChatGPT?!
------
Looks like the first open source equivalent of ChatGPT has arrived: https://github.com/lucidrains/PaLM-rlhf-pytorch…

I.e., an implementation of RLHF (Reinforcement Learning with Human Feedback) on top of Google’s 540 billion parameter PaLM architecture
------
Whoa, maybe it's worthwhile investing more time in learning how to use visual debuggers.
------
http://gctpy.com helped me find a bug in this code in under 5 minutes of looking through the repo and got it merged by Phil Wang (aka lucidrains): https://github.com/lucidrains/PaLM-rlhf-pytorch/pull/10…
------
Training factually correct LLMs requires vast amounts of factually correct training data.

Curating & creating factually correct training data for LLMs is very inefficient (vs using this data directly in the first place).

Wrote a shot post about it here:
------
Adding references DOES NOT automatically solve all the issues with misinformation. 
Eg consider the following sentence from Perplexity AI, which is obviously wrong. 

But it helps us understand why it's wrong (the model didn't dream it up, the training source contains the error.)
------
"DeepMind is also considering releasing its own chatbot, called Sparrow, for a “private beta” some time in 2023. (The delay is in order for DeepMind to work on reinforcement learning-based features that ChatGPT lacks, **like citing its sources**  ...)"

https://time.com/6246119/demis-hassabis-deepmind-interview/…
------
I think systems like ChatGPT would be a lot less controversial if they cited references.

Eg when querying “What are the origins of AI”, sure have it provide a summary. But also append the sources where it learned this info from. To a) give credit & b) so we can validate the info
------
Woke up and felt like writing a blog post on how to train an XGBoost Classifier with cloud GPUs, avoiding infrastructure headaches.
Happy Sunday! 


------
The story of weight decay in pictures:

weight decay ...
1)  improves data efficiency by > 50%
2) is frequently found in the best hyperparam configs
3) is among the most important hparams to tune
4) is also tricky to tune
------
References:
1) "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets"
https://arxiv.org/abs/2201.02177

2) "Well-tuned Simple Nets Excel on Tabular Datasets"
https://arxiv.org/abs/2106.11189

3) "S4L: Self-Supervised Semi-Supervised Learning"
------
4) "Big Transfer (BiT): General Visual Representation Learning"
------
Yes weight decay is criminally under-tuned.

From us:
- It was *the* key hparam going from 100% to 10% to 1% of ImageNet in https://arxiv.org/abs/1905.03670
- see this figure in BiT paper (https://arxiv.org/abs/1912.11370) for how it can be misleading hence hard to tune:
------
Weight decay (with AdamW) can yield a significant improvement in data efficiency compared to other methods (like Dropout), reducing the required amount of samples by over 50%
(via "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets, https://arxiv.org/abs/2201.02177)
------
And as a companion: Effective ways to reduce overfitting *via model changes*:

1. Regularization: L2 penalty, weight decay, dropout, early stopping
2. Smaller models: lottery ticket hypothesis, knowledge distillation
3. Ensemble models

https://twitter.com/rasbt/status/1613170269186854913?s=20&t=6n86jbUUee1cCvIGSSkZ2A…
------
**13 regularization techniques compared on tabular datasets in https://arxiv.org/abs/2106.11189

WD is a strong contributor


------
Even wrote a summary back then . Looks like WD is one of the big contributors to the best hparam cocktail
------
* This is for a specific, synthetic dataset they investigated. I wonder if there are any recent, larger benchmarks for various overfitting remedies.
------
TMLR numbers:
- 651 submissions
- 188 accepted papers
- currently ~100 submissions/month
- 189 action editors
- 1846 reviewers
- acceptance rate: 62% (46% if you count desk rejections and withdrawls)
- median time to decision: 76 days (#NeurIPS2022: 118 days, JMLR: >200 days) 2/n
------
Ready to start the day 
@rasbt
------
Agreed, there will be many open source implementations of ChatGPT. But there won't be many high-quality models.
I think we underestimate how much people hate labeling (or worse: writing) training data by hand.
------
Prediction:

there'll be 10 open reproductions of ChatGPT in 6 months

and if i understand correctly, models are quite smaller than LLMs so they might be significantly easier to deploy at scale (maybe even single-GPU)
------
In an academic collaboration, it was hard to convince my collaborators to even label 50 additional short texts for training a classifier. 
They preferred spending days (if not weeks) fine-tuning hyperparameters.
------
Interested in learning or transitioning to 
@PyTorch
 over the weekend?

The core API can be explained in two steps:
(1) defining the model, and
(2) setting up the training loop.

I have a concise 3-minute video explaining the PyTorch API here: https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/3-5-the-pytorch-api-parts-1-2/…
------
Here we go. I guess that ship has sailed. Researchers started adding ChatGPT as co-author on their papers 

PS: Dear researchers, you forgot the version number!

https://medrxiv.org/content/10.1101/2022.12.19.22283643v2…

(thx 
@3scorciav
 for sharing this!)
------
Re: ChatGPT and authorship.

If a person creates or contributes results for a paper, this person is a coauthor. But this naturally doesn’t extend to models or algorithms.

Imagine AlphaFold were an author on the AlphaFold paper.

So, why would/should it be different for ChatGPT?!
------
Looks like the first open source equivalent of ChatGPT has arrived: https://github.com/lucidrains/PaLM-rlhf-pytorch…

I.e., an implementation of RLHF (Reinforcement Learning with Human Feedback) on top of Google’s 540 billion parameter PaLM architecture
------
Whoa, maybe it's worthwhile investing more time in learning how to use visual debuggers.
------
http://gctpy.com helped me find a bug in this code in under 5 minutes of looking through the repo and got it merged by Phil Wang (aka lucidrains): https://github.com/lucidrains/PaLM-rlhf-pytorch/pull/10…
------
Yesterday, I shared a list of approaches that improve generalization.

Thx to 
@jefrankle
, I discovered his work (https://arxiv.org/abs/2210.13738) studying the observation that pruning improves model generalization, decomposing this phenomenon into 2 factors.
The magic of Twitter!

1/4
------
I know of no evidence that smaller models via pruning/distillation usefully reduce overfitting in any circumstance. It's an ML101 about capacity control, but ML101 intuitions don't apply cleanly to NNs (see double descent). I'd never recommend you do this if you're overfitting. twitter.com/rasbt/status/1…
------
For standard datasets, the authors found that it's due to the improved training regimes when models are pruned. For instance, the replaying of the learning rate schedule in modern pruning techniques is ~ like increasing the number of epochs and using a cyclical schedule.
3/4
------
For datasets with noisy labels the training loss of a pruned model worsens on noisy examples. This suggests that the pruned model does not fit the noisy examples (similar to models with smaller widths). This, I think, is consistent with the original intuition.
4/4
------
And as a companion: Effective ways to reduce overfitting *via model changes*:

1. Regularization: L2 penalty, weight decay, dropout, early stopping
2. Smaller models: lottery ticket hypothesis, knowledge distillation
3. Ensemble models
------
Effective ways to reduce overfitting through additional & altered data:

1.  Get more labeled data
2.  Data augmentation & synthetic data
3. Labeled data from related domain for pretraining (via transfer learning)
4.  Unlabeled data for pretraining (via self-supervised learning)
------
The story of weight decay in pictures:

weight decay ...
1)  improves data efficiency by > 50%
2) is frequently found in the best hyperparam configs
3) is among the most important hparams to tune
4) is also tricky to tune
------
References:
1) "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets"
https://arxiv.org/abs/2201.02177

2) "Well-tuned Simple Nets Excel on Tabular Datasets"
https://arxiv.org/abs/2106.11189

3) "S4L: Self-Supervised Semi-Supervised Learning"
------
4) "Big Transfer (BiT): General Visual Representation Learning"
------
Yes weight decay is criminally under-tuned.

From us:
- It was *the* key hparam going from 100% to 10% to 1% of ImageNet in https://arxiv.org/abs/1905.03670
- see this figure in BiT paper (https://arxiv.org/abs/1912.11370) for how it can be misleading hence hard to tune:
------
Weight decay (with AdamW) can yield a significant improvement in data efficiency compared to other methods (like Dropout), reducing the required amount of samples by over 50%
(via "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets, https://arxiv.org/abs/2201.02177)
------
And as a companion: Effective ways to reduce overfitting *via model changes*:

1. Regularization: L2 penalty, weight decay, dropout, early stopping
2. Smaller models: lottery ticket hypothesis, knowledge distillation
3. Ensemble models

https://twitter.com/rasbt/status/1613170269186854913?s=20&t=6n86jbUUee1cCvIGSSkZ2A…
------
**13 regularization techniques compared on tabular datasets in https://arxiv.org/abs/2106.11189

WD is a strong contributor


------
Even wrote a summary back then . Looks like WD is one of the big contributors to the best hparam cocktail
------
* This is for a specific, synthetic dataset they investigated. I wonder if there are any recent, larger benchmarks for various overfitting remedies.
------
TMLR numbers:
- 651 submissions
- 188 accepted papers
- currently ~100 submissions/month
- 189 action editors
- 1846 reviewers
- acceptance rate: 62% (46% if you count desk rejections and withdrawls)
- median time to decision: 76 days (#NeurIPS2022: 118 days, JMLR: >200 days) 2/n
------
Ready to start the day 
@rasbt
------
Agreed, there will be many open source implementations of ChatGPT. But there won't be many high-quality models.
I think we underestimate how much people hate labeling (or worse: writing) training data by hand.
------
Prediction:

there'll be 10 open reproductions of ChatGPT in 6 months

and if i understand correctly, models are quite smaller than LLMs so they might be significantly easier to deploy at scale (maybe even single-GPU)
------
In an academic collaboration, it was hard to convince my collaborators to even label 50 additional short texts for training a classifier. 
They preferred spending days (if not weeks) fine-tuning hyperparameters.
------
Interested in learning or transitioning to 
@PyTorch
 over the weekend?

The core API can be explained in two steps:
(1) defining the model, and
(2) setting up the training loop.

I have a concise 3-minute video explaining the PyTorch API here: https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/3-5-the-pytorch-api-parts-1-2/…
------
Here we go. I guess that ship has sailed. Researchers started adding ChatGPT as co-author on their papers 

PS: Dear researchers, you forgot the version number!

https://medrxiv.org/content/10.1101/2022.12.19.22283643v2…

(thx 
@3scorciav
 for sharing this!)
------
Re: ChatGPT and authorship.

If a person creates or contributes results for a paper, this person is a coauthor. But this naturally doesn’t extend to models or algorithms.

Imagine AlphaFold were an author on the AlphaFold paper.

So, why would/should it be different for ChatGPT?!
------
Looks like the first open source equivalent of ChatGPT has arrived: https://github.com/lucidrains/PaLM-rlhf-pytorch…

I.e., an implementation of RLHF (Reinforcement Learning with Human Feedback) on top of Google’s 540 billion parameter PaLM architecture
------
Whoa, maybe it's worthwhile investing more time in learning how to use visual debuggers.
------
http://gctpy.com helped me find a bug in this code in under 5 minutes of looking through the repo and got it merged by Phil Wang (aka lucidrains): https://github.com/lucidrains/PaLM-rlhf-pytorch/pull/10…
------
Yesterday, I shared a list of approaches that improve generalization.

Thx to 
@jefrankle
, I discovered his work (https://arxiv.org/abs/2210.13738) studying the observation that pruning improves model generalization, decomposing this phenomenon into 2 factors.
The magic of Twitter!

1/4
------
I know of no evidence that smaller models via pruning/distillation usefully reduce overfitting in any circumstance. It's an ML101 about capacity control, but ML101 intuitions don't apply cleanly to NNs (see double descent). I'd never recommend you do this if you're overfitting. twitter.com/rasbt/status/1…
------
For standard datasets, the authors found that it's due to the improved training regimes when models are pruned. For instance, the replaying of the learning rate schedule in modern pruning techniques is ~ like increasing the number of epochs and using a cyclical schedule.
3/4
------
For datasets with noisy labels the training loss of a pruned model worsens on noisy examples. This suggests that the pruned model does not fit the noisy examples (similar to models with smaller widths). This, I think, is consistent with the original intuition.
4/4
------
And as a companion: Effective ways to reduce overfitting *via model changes*:

1. Regularization: L2 penalty, weight decay, dropout, early stopping
2. Smaller models: lottery ticket hypothesis, knowledge distillation
3. Ensemble models
------
Effective ways to reduce overfitting through additional & altered data:

1.  Get more labeled data
2.  Data augmentation & synthetic data
3. Labeled data from related domain for pretraining (via transfer learning)
4.  Unlabeled data for pretraining (via self-supervised learning)
------
An additional method might be using label smoothing, or focal losses. But I never had good experiences with that.
------
Effective ways to reduce overfitting through additional & altered data:

1.  Get more labeled data
2.  Data augmentation & synthetic data
3. Labeled data from related domain for pretraining (via transfer learning)
4.  Unlabeled data for pretraining (via self-supervised learning)
------
The secret to using matplotlib is building a personal gallery of your most frequently used plots 

https://github.com/rasbt/matplotlib-gallery…
------
Hello, I've been using Matplotlib for 7+ years and I still google how to do everything except plt.plot()
------
Finished section 1 of Chapter 3

Starting Section 2 now! Loving the read!!

“ML…” by ⁦
@rasbt
⁩
------
One Q during today's discussion was how much math we actually need in order to use deep learning.

I am a big fan of covering the basics: e.g., applying the chain rule (with computation graphs ) & always try to reserve some time for it in my lectures.

https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/3-3-model-training-with-stochastic-gradient-descent-part-1-4/…
------
Weight decay (with AdamW) can yield a significant improvement in data efficiency compared to other methods (like Dropout), reducing the required amount of samples by over 50%
(via "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets, https://arxiv.org/abs/2201.02177)
------
And as a companion: Effective ways to reduce overfitting *via model changes*:

1. Regularization: L2 penalty, weight decay, dropout, early stopping
2. Smaller models: lottery ticket hypothesis, knowledge distillation
3. Ensemble models

https://twitter.com/rasbt/status/1613170269186854913?s=20&t=6n86jbUUee1cCvIGSSkZ2A…
------
**13 regularization techniques compared on tabular datasets in https://arxiv.org/abs/2106.11189

WD is a strong contributor


------
Even wrote a summary back then . Looks like WD is one of the big contributors to the best hparam cocktail
------
* This is for a specific, synthetic dataset they investigated. I wonder if there are any recent, larger benchmarks for various overfitting remedies.
------
TMLR numbers:
- 651 submissions
- 188 accepted papers
- currently ~100 submissions/month
- 189 action editors
- 1846 reviewers
- acceptance rate: 62% (46% if you count desk rejections and withdrawls)
- median time to decision: 76 days (#NeurIPS2022: 118 days, JMLR: >200 days) 2/n
------
Ready to start the day 
@rasbt
------
Agreed, there will be many open source implementations of ChatGPT. But there won't be many high-quality models.
I think we underestimate how much people hate labeling (or worse: writing) training data by hand.
------
Prediction:

there'll be 10 open reproductions of ChatGPT in 6 months

and if i understand correctly, models are quite smaller than LLMs so they might be significantly easier to deploy at scale (maybe even single-GPU)
------
In an academic collaboration, it was hard to convince my collaborators to even label 50 additional short texts for training a classifier. 
They preferred spending days (if not weeks) fine-tuning hyperparameters.
------
Interested in learning or transitioning to 
@PyTorch
 over the weekend?

The core API can be explained in two steps:
(1) defining the model, and
(2) setting up the training loop.

I have a concise 3-minute video explaining the PyTorch API here: https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/3-5-the-pytorch-api-parts-1-2/…
------
Here we go. I guess that ship has sailed. Researchers started adding ChatGPT as co-author on their papers 

PS: Dear researchers, you forgot the version number!

https://medrxiv.org/content/10.1101/2022.12.19.22283643v2…

(thx 
@3scorciav
 for sharing this!)
------
Re: ChatGPT and authorship.

If a person creates or contributes results for a paper, this person is a coauthor. But this naturally doesn’t extend to models or algorithms.

Imagine AlphaFold were an author on the AlphaFold paper.

So, why would/should it be different for ChatGPT?!
------
Looks like the first open source equivalent of ChatGPT has arrived: https://github.com/lucidrains/PaLM-rlhf-pytorch…

I.e., an implementation of RLHF (Reinforcement Learning with Human Feedback) on top of Google’s 540 billion parameter PaLM architecture
------
Whoa, maybe it's worthwhile investing more time in learning how to use visual debuggers.
------
http://gctpy.com helped me find a bug in this code in under 5 minutes of looking through the repo and got it merged by Phil Wang (aka lucidrains): https://github.com/lucidrains/PaLM-rlhf-pytorch/pull/10…
------
Yesterday, I shared a list of approaches that improve generalization.

Thx to 
@jefrankle
, I discovered his work (https://arxiv.org/abs/2210.13738) studying the observation that pruning improves model generalization, decomposing this phenomenon into 2 factors.
The magic of Twitter!

1/4
------
I know of no evidence that smaller models via pruning/distillation usefully reduce overfitting in any circumstance. It's an ML101 about capacity control, but ML101 intuitions don't apply cleanly to NNs (see double descent). I'd never recommend you do this if you're overfitting. twitter.com/rasbt/status/1…
------
For standard datasets, the authors found that it's due to the improved training regimes when models are pruned. For instance, the replaying of the learning rate schedule in modern pruning techniques is ~ like increasing the number of epochs and using a cyclical schedule.
3/4
------
For datasets with noisy labels the training loss of a pruned model worsens on noisy examples. This suggests that the pruned model does not fit the noisy examples (similar to models with smaller widths). This, I think, is consistent with the original intuition.
4/4
------
And as a companion: Effective ways to reduce overfitting *via model changes*:

1. Regularization: L2 penalty, weight decay, dropout, early stopping
2. Smaller models: lottery ticket hypothesis, knowledge distillation
3. Ensemble models
------
Effective ways to reduce overfitting through additional & altered data:

1.  Get more labeled data
2.  Data augmentation & synthetic data
3. Labeled data from related domain for pretraining (via transfer learning)
4.  Unlabeled data for pretraining (via self-supervised learning)
------
An additional method might be using label smoothing, or focal losses. But I never had good experiences with that.
------
Effective ways to reduce overfitting through additional & altered data:

1.  Get more labeled data
2.  Data augmentation & synthetic data
3. Labeled data from related domain for pretraining (via transfer learning)
4.  Unlabeled data for pretraining (via self-supervised learning)
------
The secret to using matplotlib is building a personal gallery of your most frequently used plots 

https://github.com/rasbt/matplotlib-gallery…
------
Hello, I've been using Matplotlib for 7+ years and I still google how to do everything except plt.plot()
------
Finished section 1 of Chapter 3

Starting Section 2 now! Loving the read!!

“ML…” by ⁦
@rasbt
⁩
------
One Q during today's discussion was how much math we actually need in order to use deep learning.

I am a big fan of covering the basics: e.g., applying the chain rule (with computation graphs ) & always try to reserve some time for it in my lectures.

https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/3-3-model-training-with-stochastic-gradient-descent-part-1-4/…
------
Sure, we may mostly rely on automatic differentiation/autograd in practice. But understanding the chain rule lets us understand backprop, which helps us understand

1.  The need for residual connections
2.  Feature normalization
3.  The design of certain loss functions
etc.
------
On that note, I remember a good (although a bit harsh ) discussion of the design of the triplet loss by 
@alfcnz
 here:
------
Unfortunately that triplet loss is flawed. The most offending negative sample has zero gradient. That power of 2 should be a power of ½.
I feel bad so many people still use it.  twitter.com/BielskiAdam/st…
------
Ready to start the day 
@rasbt
------
Agreed, there will be many open source implementations of ChatGPT. But there won't be many high-quality models.
I think we underestimate how much people hate labeling (or worse: writing) training data by hand.
------
Prediction:

there'll be 10 open reproductions of ChatGPT in 6 months

and if i understand correctly, models are quite smaller than LLMs so they might be significantly easier to deploy at scale (maybe even single-GPU)
------
In an academic collaboration, it was hard to convince my collaborators to even label 50 additional short texts for training a classifier. 
They preferred spending days (if not weeks) fine-tuning hyperparameters.
------
Interested in learning or transitioning to 
@PyTorch
 over the weekend?

The core API can be explained in two steps:
(1) defining the model, and
(2) setting up the training loop.

I have a concise 3-minute video explaining the PyTorch API here: https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/3-5-the-pytorch-api-parts-1-2/…
------
Here we go. I guess that ship has sailed. Researchers started adding ChatGPT as co-author on their papers 

PS: Dear researchers, you forgot the version number!

https://medrxiv.org/content/10.1101/2022.12.19.22283643v2…

(thx 
@3scorciav
 for sharing this!)
------
Re: ChatGPT and authorship.

If a person creates or contributes results for a paper, this person is a coauthor. But this naturally doesn’t extend to models or algorithms.

Imagine AlphaFold were an author on the AlphaFold paper.

So, why would/should it be different for ChatGPT?!
------
Looks like the first open source equivalent of ChatGPT has arrived: https://github.com/lucidrains/PaLM-rlhf-pytorch…

I.e., an implementation of RLHF (Reinforcement Learning with Human Feedback) on top of Google’s 540 billion parameter PaLM architecture
------
Whoa, maybe it's worthwhile investing more time in learning how to use visual debuggers.
------
http://gctpy.com helped me find a bug in this code in under 5 minutes of looking through the repo and got it merged by Phil Wang (aka lucidrains): https://github.com/lucidrains/PaLM-rlhf-pytorch/pull/10…
------
Yesterday, I shared a list of approaches that improve generalization.

Thx to 
@jefrankle
, I discovered his work (https://arxiv.org/abs/2210.13738) studying the observation that pruning improves model generalization, decomposing this phenomenon into 2 factors.
The magic of Twitter!

1/4
------
I know of no evidence that smaller models via pruning/distillation usefully reduce overfitting in any circumstance. It's an ML101 about capacity control, but ML101 intuitions don't apply cleanly to NNs (see double descent). I'd never recommend you do this if you're overfitting. twitter.com/rasbt/status/1…
------
For standard datasets, the authors found that it's due to the improved training regimes when models are pruned. For instance, the replaying of the learning rate schedule in modern pruning techniques is ~ like increasing the number of epochs and using a cyclical schedule.
3/4
------
For datasets with noisy labels the training loss of a pruned model worsens on noisy examples. This suggests that the pruned model does not fit the noisy examples (similar to models with smaller widths). This, I think, is consistent with the original intuition.
4/4
------
And as a companion: Effective ways to reduce overfitting *via model changes*:

1. Regularization: L2 penalty, weight decay, dropout, early stopping
2. Smaller models: lottery ticket hypothesis, knowledge distillation
3. Ensemble models
------
Effective ways to reduce overfitting through additional & altered data:

1.  Get more labeled data
2.  Data augmentation & synthetic data
3. Labeled data from related domain for pretraining (via transfer learning)
4.  Unlabeled data for pretraining (via self-supervised learning)
------
An additional method might be using label smoothing, or focal losses. But I never had good experiences with that.
------
Effective ways to reduce overfitting through additional & altered data:

1.  Get more labeled data
2.  Data augmentation & synthetic data
3. Labeled data from related domain for pretraining (via transfer learning)
4.  Unlabeled data for pretraining (via self-supervised learning)
------
The secret to using matplotlib is building a personal gallery of your most frequently used plots 

https://github.com/rasbt/matplotlib-gallery…
------
Hello, I've been using Matplotlib for 7+ years and I still google how to do everything except plt.plot()
------
Finished section 1 of Chapter 3

Starting Section 2 now! Loving the read!!

“ML…” by ⁦
@rasbt
⁩
------
One Q during today's discussion was how much math we actually need in order to use deep learning.

I am a big fan of covering the basics: e.g., applying the chain rule (with computation graphs ) & always try to reserve some time for it in my lectures.

https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/3-3-model-training-with-stochastic-gradient-descent-part-1-4/…
------
Sure, we may mostly rely on automatic differentiation/autograd in practice. But understanding the chain rule lets us understand backprop, which helps us understand

1.  The need for residual connections
2.  Feature normalization
3.  The design of certain loss functions
etc.
------
On that note, I remember a good (although a bit harsh ) discussion of the design of the triplet loss by 
@alfcnz
 here:
------
Unfortunately that triplet loss is flawed. The most offending negative sample has zero gradient. That power of 2 should be a power of ½.
I feel bad so many people still use it.  twitter.com/BielskiAdam/st…
------
Sure, there are valid concerns about ChatGPT and cheating. 
Personally, I prefer project-oriented assignments (https://arxiv.org/abs/2107.13671), and if students want to use ChatGPT to improve their writing, that's ok!

The more readable the text, the better for me.
------
My rubric may still stand the test of time re ChatGPT-generated content:
It's important to compare your work to related work, give proper attributions, describe contributions, and give original thoughts. If LLMs help automate the tedious parts, that's ok!

https://github.com/rasbt/ecml-teaching-ml-2021/blob/main/rubrics/report-rubric.md…
------
The single most undervalued fact of linear algebra: matrices are graphs, and graphs are matrices.

Encoding matrices as graphs is a cheat code, making complex behavior simple to study.

Let me show you how!
------
Edward Tian 
@edward_the6
 developed GPTZero (…https://etedward-gptzero-main-zqgfwb.streamlit.app) to help detect AI-generated content.

Was just giving it a try, and it seems to work on the example I tried. 

PS: Kudos for providing perplexity scores, which a human has to interpret, rather than binary labels
------
In the absence of a "watermark" made by OpenAI for ChatGPT. 

Edward Tian, a computer science student at Princeton, built an app called GPTZero that can “quickly and efficiently” label whether an essay was written by a person or ChatGPT.

https://gptzero.substack.com

@edward_the6
------
The virtual StateOfTheArt() 2023 conference will take place tomorrow. 
To kick it off, 
@bindureddy
 & I will be chatting about generative & large language models at 9:00 am PST. If you are interested there is a free registration form for the Zoom link here:
------
Convolutional networks strike back, again. The fully convolutional ConvNeXt v2 extends the successful ConvNeXt architecture by adding self-supervised learning capabilities.

What's new?
1/5
------
The use of sparse convolutions is mainly to use high-mask ratios and to boost efficiency during training.
The global response normalization part is a new type of normalization layer that replaces e.g., batch or layer normalization.
4/5
------
For more details, see "ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders" (https://arxiv.org/abs/2301.00808) or shoot me questions. (PS: I covered ConvNeXt v1 in Ahead of AI #4: https://magazine.sebastianraschka.com/p/ahead-of-ai-4-a-big-year-for-ai…)
5/5
------
Here we go. I guess that ship has sailed. Researchers started adding ChatGPT as co-author on their papers 

PS: Dear researchers, you forgot the version number!

https://medrxiv.org/content/10.1101/2022.12.19.22283643v2…

(thx 
@3scorciav
 for sharing this!)
------
Re: ChatGPT and authorship.

If a person creates or contributes results for a paper, this person is a coauthor. But this naturally doesn’t extend to models or algorithms.

Imagine AlphaFold were an author on the AlphaFold paper.

So, why would/should it be different for ChatGPT?!
------
Looks like the first open source equivalent of ChatGPT has arrived: https://github.com/lucidrains/PaLM-rlhf-pytorch…

I.e., an implementation of RLHF (Reinforcement Learning with Human Feedback) on top of Google’s 540 billion parameter PaLM architecture
------
Whoa, maybe it's worthwhile investing more time in learning how to use visual debuggers.
------
http://gctpy.com helped me find a bug in this code in under 5 minutes of looking through the repo and got it merged by Phil Wang (aka lucidrains): https://github.com/lucidrains/PaLM-rlhf-pytorch/pull/10…
------
Yesterday, I shared a list of approaches that improve generalization.

Thx to 
@jefrankle
, I discovered his work (https://arxiv.org/abs/2210.13738) studying the observation that pruning improves model generalization, decomposing this phenomenon into 2 factors.
The magic of Twitter!

1/4
------
I know of no evidence that smaller models via pruning/distillation usefully reduce overfitting in any circumstance. It's an ML101 about capacity control, but ML101 intuitions don't apply cleanly to NNs (see double descent). I'd never recommend you do this if you're overfitting. twitter.com/rasbt/status/1…
------
For standard datasets, the authors found that it's due to the improved training regimes when models are pruned. For instance, the replaying of the learning rate schedule in modern pruning techniques is ~ like increasing the number of epochs and using a cyclical schedule.
3/4
------
For datasets with noisy labels the training loss of a pruned model worsens on noisy examples. This suggests that the pruned model does not fit the noisy examples (similar to models with smaller widths). This, I think, is consistent with the original intuition.
4/4
------
And as a companion: Effective ways to reduce overfitting *via model changes*:

1. Regularization: L2 penalty, weight decay, dropout, early stopping
2. Smaller models: lottery ticket hypothesis, knowledge distillation
3. Ensemble models
------
Effective ways to reduce overfitting through additional & altered data:

1.  Get more labeled data
2.  Data augmentation & synthetic data
3. Labeled data from related domain for pretraining (via transfer learning)
4.  Unlabeled data for pretraining (via self-supervised learning)
------
An additional method might be using label smoothing, or focal losses. But I never had good experiences with that.
------
Effective ways to reduce overfitting through additional & altered data:

1.  Get more labeled data
2.  Data augmentation & synthetic data
3. Labeled data from related domain for pretraining (via transfer learning)
4.  Unlabeled data for pretraining (via self-supervised learning)
------
The secret to using matplotlib is building a personal gallery of your most frequently used plots 

https://github.com/rasbt/matplotlib-gallery…
------
Hello, I've been using Matplotlib for 7+ years and I still google how to do everything except plt.plot()
------
Finished section 1 of Chapter 3

Starting Section 2 now! Loving the read!!

“ML…” by ⁦
@rasbt
⁩
------
One Q during today's discussion was how much math we actually need in order to use deep learning.

I am a big fan of covering the basics: e.g., applying the chain rule (with computation graphs ) & always try to reserve some time for it in my lectures.

https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/3-3-model-training-with-stochastic-gradient-descent-part-1-4/…
------
Sure, we may mostly rely on automatic differentiation/autograd in practice. But understanding the chain rule lets us understand backprop, which helps us understand

1.  The need for residual connections
2.  Feature normalization
3.  The design of certain loss functions
etc.
------
On that note, I remember a good (although a bit harsh ) discussion of the design of the triplet loss by 
@alfcnz
 here:
------
Unfortunately that triplet loss is flawed. The most offending negative sample has zero gradient. That power of 2 should be a power of ½.
I feel bad so many people still use it.  twitter.com/BielskiAdam/st…
------
Sure, there are valid concerns about ChatGPT and cheating. 
Personally, I prefer project-oriented assignments (https://arxiv.org/abs/2107.13671), and if students want to use ChatGPT to improve their writing, that's ok!

The more readable the text, the better for me.
------
My rubric may still stand the test of time re ChatGPT-generated content:
It's important to compare your work to related work, give proper attributions, describe contributions, and give original thoughts. If LLMs help automate the tedious parts, that's ok!

https://github.com/rasbt/ecml-teaching-ml-2021/blob/main/rubrics/report-rubric.md…
------
The single most undervalued fact of linear algebra: matrices are graphs, and graphs are matrices.

Encoding matrices as graphs is a cheat code, making complex behavior simple to study.

Let me show you how!
------
Edward Tian 
@edward_the6
 developed GPTZero (…https://etedward-gptzero-main-zqgfwb.streamlit.app) to help detect AI-generated content.

Was just giving it a try, and it seems to work on the example I tried. 

PS: Kudos for providing perplexity scores, which a human has to interpret, rather than binary labels
------
In the absence of a "watermark" made by OpenAI for ChatGPT. 

Edward Tian, a computer science student at Princeton, built an app called GPTZero that can “quickly and efficiently” label whether an essay was written by a person or ChatGPT.

https://gptzero.substack.com

@edward_the6
------
The virtual StateOfTheArt() 2023 conference will take place tomorrow. 
To kick it off, 
@bindureddy
 & I will be chatting about generative & large language models at 9:00 am PST. If you are interested there is a free registration form for the Zoom link here:
------
Convolutional networks strike back, again. The fully convolutional ConvNeXt v2 extends the successful ConvNeXt architecture by adding self-supervised learning capabilities.

What's new?
1/5
------
The use of sparse convolutions is mainly to use high-mask ratios and to boost efficiency during training.
The global response normalization part is a new type of normalization layer that replaces e.g., batch or layer normalization.
4/5
------
For more details, see "ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders" (https://arxiv.org/abs/2301.00808) or shoot me questions. (PS: I covered ConvNeXt v1 in Ahead of AI #4: https://magazine.sebastianraschka.com/p/ahead-of-ai-4-a-big-year-for-ai…)
5/5
------
Re: ChatGPT and authorship.

If a person creates or contributes results for a paper, this person is a coauthor. But this naturally doesn’t extend to models or algorithms.

Imagine AlphaFold were an author on the AlphaFold paper.

So, why would/should it be different for ChatGPT?!
------
Or in other words, if you use ChatGPT as a tool to interpret & write about some of the results, just reference it in the Methods section where it belongs.
------
Embedding layers are often perceived as a fancy operation that we apply to encode the inputs (each word tokens) for large language models.
But embedding layers = fully-connected layers on one-hot encoded inputs. They just replace expensive matrix multiplications w index look-ups.
------
*The green box should be down by 1 position of course. Didn't mean to use 1-based indexing 
------
Videos on that to be released in Unit 8 of my deep learning fundamentals course 
------
Speak for yourself gdb! I still really enjoy  just writing code, even if I throw it all into the bin, and even after doing so for decades. It's a bit like a relaxing puzzle/game.

Using shitty libraries is not fun though, that's why I like to stick to few good core libraries.
------
Writing code for is not very fun for its own sake. What makes it insanely addictive is the feeling upon shipping — that actual people are doing something useful with what started as a figment of your imagination. Never gets old.
------
Yesterday, I shared a list of approaches that improve generalization.

Thx to 
@jefrankle
, I discovered his work (https://arxiv.org/abs/2210.13738) studying the observation that pruning improves model generalization, decomposing this phenomenon into 2 factors.
The magic of Twitter!

1/4
------
I know of no evidence that smaller models via pruning/distillation usefully reduce overfitting in any circumstance. It's an ML101 about capacity control, but ML101 intuitions don't apply cleanly to NNs (see double descent). I'd never recommend you do this if you're overfitting. twitter.com/rasbt/status/1…
------
For standard datasets, the authors found that it's due to the improved training regimes when models are pruned. For instance, the replaying of the learning rate schedule in modern pruning techniques is ~ like increasing the number of epochs and using a cyclical schedule.
3/4
------
For datasets with noisy labels the training loss of a pruned model worsens on noisy examples. This suggests that the pruned model does not fit the noisy examples (similar to models with smaller widths). This, I think, is consistent with the original intuition.
4/4
------
And as a companion: Effective ways to reduce overfitting *via model changes*:

1. Regularization: L2 penalty, weight decay, dropout, early stopping
2. Smaller models: lottery ticket hypothesis, knowledge distillation
3. Ensemble models
------
Effective ways to reduce overfitting through additional & altered data:

1.  Get more labeled data
2.  Data augmentation & synthetic data
3. Labeled data from related domain for pretraining (via transfer learning)
4.  Unlabeled data for pretraining (via self-supervised learning)
------
An additional method might be using label smoothing, or focal losses. But I never had good experiences with that.
------
Effective ways to reduce overfitting through additional & altered data:

1.  Get more labeled data
2.  Data augmentation & synthetic data
3. Labeled data from related domain for pretraining (via transfer learning)
4.  Unlabeled data for pretraining (via self-supervised learning)
------
The secret to using matplotlib is building a personal gallery of your most frequently used plots 

https://github.com/rasbt/matplotlib-gallery…
------
Hello, I've been using Matplotlib for 7+ years and I still google how to do everything except plt.plot()
------
Finished section 1 of Chapter 3

Starting Section 2 now! Loving the read!!

“ML…” by ⁦
@rasbt
⁩
------
One Q during today's discussion was how much math we actually need in order to use deep learning.

I am a big fan of covering the basics: e.g., applying the chain rule (with computation graphs ) & always try to reserve some time for it in my lectures.

https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/3-3-model-training-with-stochastic-gradient-descent-part-1-4/…
------
Sure, we may mostly rely on automatic differentiation/autograd in practice. But understanding the chain rule lets us understand backprop, which helps us understand

1.  The need for residual connections
2.  Feature normalization
3.  The design of certain loss functions
etc.
------
On that note, I remember a good (although a bit harsh ) discussion of the design of the triplet loss by 
@alfcnz
 here:
------
Unfortunately that triplet loss is flawed. The most offending negative sample has zero gradient. That power of 2 should be a power of ½.
I feel bad so many people still use it.  twitter.com/BielskiAdam/st…
------
Sure, there are valid concerns about ChatGPT and cheating. 
Personally, I prefer project-oriented assignments (https://arxiv.org/abs/2107.13671), and if students want to use ChatGPT to improve their writing, that's ok!

The more readable the text, the better for me.
------
My rubric may still stand the test of time re ChatGPT-generated content:
It's important to compare your work to related work, give proper attributions, describe contributions, and give original thoughts. If LLMs help automate the tedious parts, that's ok!

https://github.com/rasbt/ecml-teaching-ml-2021/blob/main/rubrics/report-rubric.md…
------
The single most undervalued fact of linear algebra: matrices are graphs, and graphs are matrices.

Encoding matrices as graphs is a cheat code, making complex behavior simple to study.

Let me show you how!
------
Edward Tian 
@edward_the6
 developed GPTZero (…https://etedward-gptzero-main-zqgfwb.streamlit.app) to help detect AI-generated content.

Was just giving it a try, and it seems to work on the example I tried. 

PS: Kudos for providing perplexity scores, which a human has to interpret, rather than binary labels
------
In the absence of a "watermark" made by OpenAI for ChatGPT. 

Edward Tian, a computer science student at Princeton, built an app called GPTZero that can “quickly and efficiently” label whether an essay was written by a person or ChatGPT.

https://gptzero.substack.com

@edward_the6
------
The virtual StateOfTheArt() 2023 conference will take place tomorrow. 
To kick it off, 
@bindureddy
 & I will be chatting about generative & large language models at 9:00 am PST. If you are interested there is a free registration form for the Zoom link here:
------
Convolutional networks strike back, again. The fully convolutional ConvNeXt v2 extends the successful ConvNeXt architecture by adding self-supervised learning capabilities.

What's new?
1/5
------
The use of sparse convolutions is mainly to use high-mask ratios and to boost efficiency during training.
The global response normalization part is a new type of normalization layer that replaces e.g., batch or layer normalization.
4/5
------
For more details, see "ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders" (https://arxiv.org/abs/2301.00808) or shoot me questions. (PS: I covered ConvNeXt v1 in Ahead of AI #4: https://magazine.sebastianraschka.com/p/ahead-of-ai-4-a-big-year-for-ai…)
5/5
------
Re: ChatGPT and authorship.

If a person creates or contributes results for a paper, this person is a coauthor. But this naturally doesn’t extend to models or algorithms.

Imagine AlphaFold were an author on the AlphaFold paper.

So, why would/should it be different for ChatGPT?!
------
Or in other words, if you use ChatGPT as a tool to interpret & write about some of the results, just reference it in the Methods section where it belongs.
------
Embedding layers are often perceived as a fancy operation that we apply to encode the inputs (each word tokens) for large language models.
But embedding layers = fully-connected layers on one-hot encoded inputs. They just replace expensive matrix multiplications w index look-ups.
------
*The green box should be down by 1 position of course. Didn't mean to use 1-based indexing 
------
Videos on that to be released in Unit 8 of my deep learning fundamentals course 
------
Speak for yourself gdb! I still really enjoy  just writing code, even if I throw it all into the bin, and even after doing so for decades. It's a bit like a relaxing puzzle/game.

Using shitty libraries is not fun though, that's why I like to stick to few good core libraries.
------
Writing code for is not very fun for its own sake. What makes it insanely addictive is the feeling upon shipping — that actual people are doing something useful with what started as a figment of your imagination. Never gets old.
------
Open source had a good year!
I compiled a list of my favorite machine learning- and AI-related open-source libraries & releases that I discovered, used, or contributed to in 2022:
------
I forgot 
7) Reinforcement Learning with Human Feedback
------
The 6 different ways of using a language transformer / LLM

1 Train from scratch
2 Feature-based: Train new model on embeddings
3 Finetuning I: Freeze all but output layer weights
4 Finetuning II: Update all weights
5 Zero-shot learning
6 Few-shot learning

Anything missing?
1/6
------
And as a companion: Effective ways to reduce overfitting *via model changes*:

1. Regularization: L2 penalty, weight decay, dropout, early stopping
2. Smaller models: lottery ticket hypothesis, knowledge distillation
3. Ensemble models
------
Effective ways to reduce overfitting through additional & altered data:

1.  Get more labeled data
2.  Data augmentation & synthetic data
3. Labeled data from related domain for pretraining (via transfer learning)
4.  Unlabeled data for pretraining (via self-supervised learning)
------
An additional method might be using label smoothing, or focal losses. But I never had good experiences with that.
------
Effective ways to reduce overfitting through additional & altered data:

1.  Get more labeled data
2.  Data augmentation & synthetic data
3. Labeled data from related domain for pretraining (via transfer learning)
4.  Unlabeled data for pretraining (via self-supervised learning)
------
The secret to using matplotlib is building a personal gallery of your most frequently used plots 

https://github.com/rasbt/matplotlib-gallery…
------
Hello, I've been using Matplotlib for 7+ years and I still google how to do everything except plt.plot()
------
Finished section 1 of Chapter 3

Starting Section 2 now! Loving the read!!

“ML…” by ⁦
@rasbt
⁩
------
One Q during today's discussion was how much math we actually need in order to use deep learning.

I am a big fan of covering the basics: e.g., applying the chain rule (with computation graphs ) & always try to reserve some time for it in my lectures.

https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/3-3-model-training-with-stochastic-gradient-descent-part-1-4/…
------
Sure, we may mostly rely on automatic differentiation/autograd in practice. But understanding the chain rule lets us understand backprop, which helps us understand

1.  The need for residual connections
2.  Feature normalization
3.  The design of certain loss functions
etc.
------
On that note, I remember a good (although a bit harsh ) discussion of the design of the triplet loss by 
@alfcnz
 here:
------
Unfortunately that triplet loss is flawed. The most offending negative sample has zero gradient. That power of 2 should be a power of ½.
I feel bad so many people still use it.  twitter.com/BielskiAdam/st…
------
Sure, there are valid concerns about ChatGPT and cheating. 
Personally, I prefer project-oriented assignments (https://arxiv.org/abs/2107.13671), and if students want to use ChatGPT to improve their writing, that's ok!

The more readable the text, the better for me.
------
My rubric may still stand the test of time re ChatGPT-generated content:
It's important to compare your work to related work, give proper attributions, describe contributions, and give original thoughts. If LLMs help automate the tedious parts, that's ok!

https://github.com/rasbt/ecml-teaching-ml-2021/blob/main/rubrics/report-rubric.md…
------
The single most undervalued fact of linear algebra: matrices are graphs, and graphs are matrices.

Encoding matrices as graphs is a cheat code, making complex behavior simple to study.

Let me show you how!
------
Edward Tian 
@edward_the6
 developed GPTZero (…https://etedward-gptzero-main-zqgfwb.streamlit.app) to help detect AI-generated content.

Was just giving it a try, and it seems to work on the example I tried. 

PS: Kudos for providing perplexity scores, which a human has to interpret, rather than binary labels
------
In the absence of a "watermark" made by OpenAI for ChatGPT. 

Edward Tian, a computer science student at Princeton, built an app called GPTZero that can “quickly and efficiently” label whether an essay was written by a person or ChatGPT.

https://gptzero.substack.com

@edward_the6
------
The virtual StateOfTheArt() 2023 conference will take place tomorrow. 
To kick it off, 
@bindureddy
 & I will be chatting about generative & large language models at 9:00 am PST. If you are interested there is a free registration form for the Zoom link here:
------
Convolutional networks strike back, again. The fully convolutional ConvNeXt v2 extends the successful ConvNeXt architecture by adding self-supervised learning capabilities.

What's new?
1/5
------
The use of sparse convolutions is mainly to use high-mask ratios and to boost efficiency during training.
The global response normalization part is a new type of normalization layer that replaces e.g., batch or layer normalization.
4/5
------
For more details, see "ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders" (https://arxiv.org/abs/2301.00808) or shoot me questions. (PS: I covered ConvNeXt v1 in Ahead of AI #4: https://magazine.sebastianraschka.com/p/ahead-of-ai-4-a-big-year-for-ai…)
5/5
------
Re: ChatGPT and authorship.

If a person creates or contributes results for a paper, this person is a coauthor. But this naturally doesn’t extend to models or algorithms.

Imagine AlphaFold were an author on the AlphaFold paper.

So, why would/should it be different for ChatGPT?!
------
Or in other words, if you use ChatGPT as a tool to interpret & write about some of the results, just reference it in the Methods section where it belongs.
------
Embedding layers are often perceived as a fancy operation that we apply to encode the inputs (each word tokens) for large language models.
But embedding layers = fully-connected layers on one-hot encoded inputs. They just replace expensive matrix multiplications w index look-ups.
------
*The green box should be down by 1 position of course. Didn't mean to use 1-based indexing 
------
Videos on that to be released in Unit 8 of my deep learning fundamentals course 
------
Speak for yourself gdb! I still really enjoy  just writing code, even if I throw it all into the bin, and even after doing so for decades. It's a bit like a relaxing puzzle/game.

Using shitty libraries is not fun though, that's why I like to stick to few good core libraries.
------
Writing code for is not very fun for its own sake. What makes it insanely addictive is the feeling upon shipping — that actual people are doing something useful with what started as a figment of your imagination. Never gets old.
------
Open source had a good year!
I compiled a list of my favorite machine learning- and AI-related open-source libraries & releases that I discovered, used, or contributed to in 2022:
------
I forgot 
7) Reinforcement Learning with Human Feedback
------
The 6 different ways of using a language transformer / LLM

1 Train from scratch
2 Feature-based: Train new model on embeddings
3 Finetuning I: Freeze all but output layer weights
4 Finetuning II: Update all weights
5 Zero-shot learning
6 Few-shot learning

Anything missing?
1/6
------
Are you interested in learning how to use PyTorch's autograd module to compute gradients?
I've created a short, 5-minute video to get you started:
https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/3-4-automatic-differentiation-in-pytorch/…
------
I don't think this policy by 
@icmlconf
 will age well.

Its not clear to me what the reservations are -- they can ban "AI as a co-author", but not allowing it as a tool is a bit of an overreach without obviously stated harms.
------
this is kinda gate-keepy, @icmlconf
------
And so it begins ...
Researchers created a peer-review dataset & review comment generation model: https://arxiv.org/abs/2212.04972

"[...] if you thought Human Reviewer 2 was hard to reason with, just wait until reviewer 2 is a language model!" -- via 
@jackclarkSF
------
Researchers want LLMs (large language models) to mimic humans.
But let's also keep in mind that LLMs are highly effective at modeling proteins (as demonstrated by AlphaFold 2 and the ESM metagenomic Atlas) not in spite of but rather *because* they work differently than humans.
------
Would you have a recommendation for a good recent pure Python book, above beginner level, in addition to Fluent Python?
------
Saw lots of people sharing their reading list for 2023! Let me join! 

- A pile of paper first
- Modern Time Series Forecasting with Python (Joseph)
- Modern Deep Learning for Tabular Data (Ye & Wang)
- the new Probabilistic Machine Learning (Murphy), comes out in summer

1/2
------
Started my yearly review . Wow, apparently, I read 29 books in 2022 ...
Some of the more technical ones :

- Designing ML Systems
- The Kaggle Book
- NLP w Transformers
- The Book of Why
- Modeling Mindsets
Let me know if you would like to hear more about them, happy to chat!
------
Sure, we may mostly rely on automatic differentiation/autograd in practice. But understanding the chain rule lets us understand backprop, which helps us understand

1.  The need for residual connections
2.  Feature normalization
3.  The design of certain loss functions
etc.
------
On that note, I remember a good (although a bit harsh ) discussion of the design of the triplet loss by 
@alfcnz
 here:
------
Unfortunately that triplet loss is flawed. The most offending negative sample has zero gradient. That power of 2 should be a power of ½.
I feel bad so many people still use it.  twitter.com/BielskiAdam/st…
------
Sure, there are valid concerns about ChatGPT and cheating. 
Personally, I prefer project-oriented assignments (https://arxiv.org/abs/2107.13671), and if students want to use ChatGPT to improve their writing, that's ok!

The more readable the text, the better for me.
------
My rubric may still stand the test of time re ChatGPT-generated content:
It's important to compare your work to related work, give proper attributions, describe contributions, and give original thoughts. If LLMs help automate the tedious parts, that's ok!

https://github.com/rasbt/ecml-teaching-ml-2021/blob/main/rubrics/report-rubric.md…
------
The single most undervalued fact of linear algebra: matrices are graphs, and graphs are matrices.

Encoding matrices as graphs is a cheat code, making complex behavior simple to study.

Let me show you how!
------
Edward Tian 
@edward_the6
 developed GPTZero (…https://etedward-gptzero-main-zqgfwb.streamlit.app) to help detect AI-generated content.

Was just giving it a try, and it seems to work on the example I tried. 

PS: Kudos for providing perplexity scores, which a human has to interpret, rather than binary labels
------
In the absence of a "watermark" made by OpenAI for ChatGPT. 

Edward Tian, a computer science student at Princeton, built an app called GPTZero that can “quickly and efficiently” label whether an essay was written by a person or ChatGPT.

https://gptzero.substack.com

@edward_the6
------
The virtual StateOfTheArt() 2023 conference will take place tomorrow. 
To kick it off, 
@bindureddy
 & I will be chatting about generative & large language models at 9:00 am PST. If you are interested there is a free registration form for the Zoom link here:
------
Convolutional networks strike back, again. The fully convolutional ConvNeXt v2 extends the successful ConvNeXt architecture by adding self-supervised learning capabilities.

What's new?
1/5
------
The use of sparse convolutions is mainly to use high-mask ratios and to boost efficiency during training.
The global response normalization part is a new type of normalization layer that replaces e.g., batch or layer normalization.
4/5
------
For more details, see "ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders" (https://arxiv.org/abs/2301.00808) or shoot me questions. (PS: I covered ConvNeXt v1 in Ahead of AI #4: https://magazine.sebastianraschka.com/p/ahead-of-ai-4-a-big-year-for-ai…)
5/5
------
Re: ChatGPT and authorship.

If a person creates or contributes results for a paper, this person is a coauthor. But this naturally doesn’t extend to models or algorithms.

Imagine AlphaFold were an author on the AlphaFold paper.

So, why would/should it be different for ChatGPT?!
------
Or in other words, if you use ChatGPT as a tool to interpret & write about some of the results, just reference it in the Methods section where it belongs.
------
Embedding layers are often perceived as a fancy operation that we apply to encode the inputs (each word tokens) for large language models.
But embedding layers = fully-connected layers on one-hot encoded inputs. They just replace expensive matrix multiplications w index look-ups.
------
*The green box should be down by 1 position of course. Didn't mean to use 1-based indexing 
------
Videos on that to be released in Unit 8 of my deep learning fundamentals course 
------
Speak for yourself gdb! I still really enjoy  just writing code, even if I throw it all into the bin, and even after doing so for decades. It's a bit like a relaxing puzzle/game.

Using shitty libraries is not fun though, that's why I like to stick to few good core libraries.
------
Writing code for is not very fun for its own sake. What makes it insanely addictive is the feeling upon shipping — that actual people are doing something useful with what started as a figment of your imagination. Never gets old.
------
Open source had a good year!
I compiled a list of my favorite machine learning- and AI-related open-source libraries & releases that I discovered, used, or contributed to in 2022:
------
I forgot 
7) Reinforcement Learning with Human Feedback
------
The 6 different ways of using a language transformer / LLM

1 Train from scratch
2 Feature-based: Train new model on embeddings
3 Finetuning I: Freeze all but output layer weights
4 Finetuning II: Update all weights
5 Zero-shot learning
6 Few-shot learning

Anything missing?
1/6
------
Are you interested in learning how to use PyTorch's autograd module to compute gradients?
I've created a short, 5-minute video to get you started:
https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/3-4-automatic-differentiation-in-pytorch/…
------
I don't think this policy by 
@icmlconf
 will age well.

Its not clear to me what the reservations are -- they can ban "AI as a co-author", but not allowing it as a tool is a bit of an overreach without obviously stated harms.
------
this is kinda gate-keepy, @icmlconf
------
And so it begins ...
Researchers created a peer-review dataset & review comment generation model: https://arxiv.org/abs/2212.04972

"[...] if you thought Human Reviewer 2 was hard to reason with, just wait until reviewer 2 is a language model!" -- via 
@jackclarkSF
------
Researchers want LLMs (large language models) to mimic humans.
But let's also keep in mind that LLMs are highly effective at modeling proteins (as demonstrated by AlphaFold 2 and the ESM metagenomic Atlas) not in spite of but rather *because* they work differently than humans.
------
Would you have a recommendation for a good recent pure Python book, above beginner level, in addition to Fluent Python?
------
Saw lots of people sharing their reading list for 2023! Let me join! 

- A pile of paper first
- Modern Time Series Forecasting with Python (Joseph)
- Modern Deep Learning for Tabular Data (Ye & Wang)
- the new Probabilistic Machine Learning (Murphy), comes out in summer

1/2
------
Started my yearly review . Wow, apparently, I read 29 books in 2022 ...
Some of the more technical ones :

- Designing ML Systems
- The Kaggle Book
- NLP w Transformers
- The Book of Why
- Modeling Mindsets
Let me know if you would like to hear more about them, happy to chat!
------
- Fluent Python (2nd, Ramalho)
- Creating a Wordle Game in React and TypeScript (Gold)

And some fun ones I got for Xmas:
- Fairy Tale (King)
- The last 2 Witcher books
- The Nineties (Klosterman)

2/2
------
I am delighted to announce that the "real" camera-ready version of my new book, "Probabilistic Machine Learning: Advanced Topics", is now available. It will appear in print this summer, but it is already freely available online at http://probml.github.io/book2.
------
"Ahead of AI #4: A Big Year For AI" is out!

In this issue I'm discussing:
- My top 10 papers I read this year
- AI industry trends
- Open source highlights
- My yearly review routine

 https://magazine.sebastianraschka.com/p/ahead-of-ai-4-a-big-year-for-ai…

Happy New Year!
------
My ML & AI prediction for 2023: 

"open access" becomes the new "open source". Code will remain open source but be less useful due to data requirements & compute costs.

For demos or peer-reviews, devs will have to host models. Easy & cheap hosting will likely be a focus in 2023.
------
3 papers from my personal top-10 2022.*

8. A Generalist Agent
9. Robust Speech Recognition via Large-Scale Weak Supervision
10. Revisiting Pretraining Objectives for Tabular Deep Learning

*The top 7, incl write-ups, in tomorrow's Ahead of AI 
------
Happy New Year! 

What’s your theme for the year?
Personally, I am aiming to automate more mundane tasks again

My first project is automating the 2022 -> 2023 bumps on various websites (using my old, trusty py-args utils https://github.com/rasbt/py-args/tree/master/glob-replace…)
------
Fun fact: According to Google Scholar, researchers published about 22,000! vision transformer papers this year!
------
Edward Tian 
@edward_the6
 developed GPTZero (…https://etedward-gptzero-main-zqgfwb.streamlit.app) to help detect AI-generated content.

Was just giving it a try, and it seems to work on the example I tried. 

PS: Kudos for providing perplexity scores, which a human has to interpret, rather than binary labels
------
In the absence of a "watermark" made by OpenAI for ChatGPT. 

Edward Tian, a computer science student at Princeton, built an app called GPTZero that can “quickly and efficiently” label whether an essay was written by a person or ChatGPT.

https://gptzero.substack.com

@edward_the6
------
The virtual StateOfTheArt() 2023 conference will take place tomorrow. 
To kick it off, 
@bindureddy
 & I will be chatting about generative & large language models at 9:00 am PST. If you are interested there is a free registration form for the Zoom link here:
------
Convolutional networks strike back, again. The fully convolutional ConvNeXt v2 extends the successful ConvNeXt architecture by adding self-supervised learning capabilities.

What's new?
1/5
------
The use of sparse convolutions is mainly to use high-mask ratios and to boost efficiency during training.
The global response normalization part is a new type of normalization layer that replaces e.g., batch or layer normalization.
4/5
------
For more details, see "ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders" (https://arxiv.org/abs/2301.00808) or shoot me questions. (PS: I covered ConvNeXt v1 in Ahead of AI #4: https://magazine.sebastianraschka.com/p/ahead-of-ai-4-a-big-year-for-ai…)
5/5
------
Re: ChatGPT and authorship.

If a person creates or contributes results for a paper, this person is a coauthor. But this naturally doesn’t extend to models or algorithms.

Imagine AlphaFold were an author on the AlphaFold paper.

So, why would/should it be different for ChatGPT?!
------
Or in other words, if you use ChatGPT as a tool to interpret & write about some of the results, just reference it in the Methods section where it belongs.
------
Embedding layers are often perceived as a fancy operation that we apply to encode the inputs (each word tokens) for large language models.
But embedding layers = fully-connected layers on one-hot encoded inputs. They just replace expensive matrix multiplications w index look-ups.
------
*The green box should be down by 1 position of course. Didn't mean to use 1-based indexing 
------
Videos on that to be released in Unit 8 of my deep learning fundamentals course 
------
Speak for yourself gdb! I still really enjoy  just writing code, even if I throw it all into the bin, and even after doing so for decades. It's a bit like a relaxing puzzle/game.

Using shitty libraries is not fun though, that's why I like to stick to few good core libraries.
------
Writing code for is not very fun for its own sake. What makes it insanely addictive is the feeling upon shipping — that actual people are doing something useful with what started as a figment of your imagination. Never gets old.
------
Open source had a good year!
I compiled a list of my favorite machine learning- and AI-related open-source libraries & releases that I discovered, used, or contributed to in 2022:
------
I forgot 
7) Reinforcement Learning with Human Feedback
------
The 6 different ways of using a language transformer / LLM

1 Train from scratch
2 Feature-based: Train new model on embeddings
3 Finetuning I: Freeze all but output layer weights
4 Finetuning II: Update all weights
5 Zero-shot learning
6 Few-shot learning

Anything missing?
1/6
------
Are you interested in learning how to use PyTorch's autograd module to compute gradients?
I've created a short, 5-minute video to get you started:
https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/3-4-automatic-differentiation-in-pytorch/…
------
I don't think this policy by 
@icmlconf
 will age well.

Its not clear to me what the reservations are -- they can ban "AI as a co-author", but not allowing it as a tool is a bit of an overreach without obviously stated harms.
------
this is kinda gate-keepy, @icmlconf
------
And so it begins ...
Researchers created a peer-review dataset & review comment generation model: https://arxiv.org/abs/2212.04972

"[...] if you thought Human Reviewer 2 was hard to reason with, just wait until reviewer 2 is a language model!" -- via 
@jackclarkSF
------
Researchers want LLMs (large language models) to mimic humans.
But let's also keep in mind that LLMs are highly effective at modeling proteins (as demonstrated by AlphaFold 2 and the ESM metagenomic Atlas) not in spite of but rather *because* they work differently than humans.
------
Would you have a recommendation for a good recent pure Python book, above beginner level, in addition to Fluent Python?
------
Saw lots of people sharing their reading list for 2023! Let me join! 

- A pile of paper first
- Modern Time Series Forecasting with Python (Joseph)
- Modern Deep Learning for Tabular Data (Ye & Wang)
- the new Probabilistic Machine Learning (Murphy), comes out in summer

1/2
------
Started my yearly review . Wow, apparently, I read 29 books in 2022 ...
Some of the more technical ones :

- Designing ML Systems
- The Kaggle Book
- NLP w Transformers
- The Book of Why
- Modeling Mindsets
Let me know if you would like to hear more about them, happy to chat!
------
- Fluent Python (2nd, Ramalho)
- Creating a Wordle Game in React and TypeScript (Gold)

And some fun ones I got for Xmas:
- Fairy Tale (King)
- The last 2 Witcher books
- The Nineties (Klosterman)

2/2
------
I am delighted to announce that the "real" camera-ready version of my new book, "Probabilistic Machine Learning: Advanced Topics", is now available. It will appear in print this summer, but it is already freely available online at http://probml.github.io/book2.
------
"Ahead of AI #4: A Big Year For AI" is out!

In this issue I'm discussing:
- My top 10 papers I read this year
- AI industry trends
- Open source highlights
- My yearly review routine

 https://magazine.sebastianraschka.com/p/ahead-of-ai-4-a-big-year-for-ai…

Happy New Year!
------
My ML & AI prediction for 2023: 

"open access" becomes the new "open source". Code will remain open source but be less useful due to data requirements & compute costs.

For demos or peer-reviews, devs will have to host models. Easy & cheap hosting will likely be a focus in 2023.
------
3 papers from my personal top-10 2022.*

8. A Generalist Agent
9. Robust Speech Recognition via Large-Scale Weak Supervision
10. Revisiting Pretraining Objectives for Tabular Deep Learning

*The top 7, incl write-ups, in tomorrow's Ahead of AI 
------
Happy New Year! 

What’s your theme for the year?
Personally, I am aiming to automate more mundane tasks again

My first project is automating the 2022 -> 2023 bumps on various websites (using my old, trusty py-args utils https://github.com/rasbt/py-args/tree/master/glob-replace…)
------
Fun fact: According to Google Scholar, researchers published about 22,000! vision transformer papers this year!
------
Not even a deep learning model can keep up with it all 
------
Sorry, I got too excited there. I didn't mean twenty-two-thousand-"factorial" of course :P
------
Vision transformers have taken the field of computer vision by storm, but what do vision transformers learn?

ViTs have fewer inductive biases than CNNs. However, it turns out that they learn inductive biases (or features) similar to CNNs!

1/2
------
That is, early layers capture edges and textures, and later layers learn more complex representations to capture broader concepts.

More analysis and insights in the paper: "What do vision transformers learn? A visual exploration" https://arxiv.org/abs/2212.06727

2/2
------
Or in other words, if you use ChatGPT as a tool to interpret & write about some of the results, just reference it in the Methods section where it belongs.
------
Embedding layers are often perceived as a fancy operation that we apply to encode the inputs (each word tokens) for large language models.
But embedding layers = fully-connected layers on one-hot encoded inputs. They just replace expensive matrix multiplications w index look-ups.
------
*The green box should be down by 1 position of course. Didn't mean to use 1-based indexing 
------
Videos on that to be released in Unit 8 of my deep learning fundamentals course 
------
Speak for yourself gdb! I still really enjoy  just writing code, even if I throw it all into the bin, and even after doing so for decades. It's a bit like a relaxing puzzle/game.

Using shitty libraries is not fun though, that's why I like to stick to few good core libraries.
------
Writing code for is not very fun for its own sake. What makes it insanely addictive is the feeling upon shipping — that actual people are doing something useful with what started as a figment of your imagination. Never gets old.
------
Open source had a good year!
I compiled a list of my favorite machine learning- and AI-related open-source libraries & releases that I discovered, used, or contributed to in 2022:
------
I forgot 
7) Reinforcement Learning with Human Feedback
------
The 6 different ways of using a language transformer / LLM

1 Train from scratch
2 Feature-based: Train new model on embeddings
3 Finetuning I: Freeze all but output layer weights
4 Finetuning II: Update all weights
5 Zero-shot learning
6 Few-shot learning

Anything missing?
1/6
------
Are you interested in learning how to use PyTorch's autograd module to compute gradients?
I've created a short, 5-minute video to get you started:
https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/3-4-automatic-differentiation-in-pytorch/…
------
I don't think this policy by 
@icmlconf
 will age well.

Its not clear to me what the reservations are -- they can ban "AI as a co-author", but not allowing it as a tool is a bit of an overreach without obviously stated harms.
------
this is kinda gate-keepy, @icmlconf
------
And so it begins ...
Researchers created a peer-review dataset & review comment generation model: https://arxiv.org/abs/2212.04972

"[...] if you thought Human Reviewer 2 was hard to reason with, just wait until reviewer 2 is a language model!" -- via 
@jackclarkSF
------
Researchers want LLMs (large language models) to mimic humans.
But let's also keep in mind that LLMs are highly effective at modeling proteins (as demonstrated by AlphaFold 2 and the ESM metagenomic Atlas) not in spite of but rather *because* they work differently than humans.
------
Would you have a recommendation for a good recent pure Python book, above beginner level, in addition to Fluent Python?
------
Saw lots of people sharing their reading list for 2023! Let me join! 

- A pile of paper first
- Modern Time Series Forecasting with Python (Joseph)
- Modern Deep Learning for Tabular Data (Ye & Wang)
- the new Probabilistic Machine Learning (Murphy), comes out in summer

1/2
------
Started my yearly review . Wow, apparently, I read 29 books in 2022 ...
Some of the more technical ones :

- Designing ML Systems
- The Kaggle Book
- NLP w Transformers
- The Book of Why
- Modeling Mindsets
Let me know if you would like to hear more about them, happy to chat!
------
- Fluent Python (2nd, Ramalho)
- Creating a Wordle Game in React and TypeScript (Gold)

And some fun ones I got for Xmas:
- Fairy Tale (King)
- The last 2 Witcher books
- The Nineties (Klosterman)

2/2
------
I am delighted to announce that the "real" camera-ready version of my new book, "Probabilistic Machine Learning: Advanced Topics", is now available. It will appear in print this summer, but it is already freely available online at http://probml.github.io/book2.
------
"Ahead of AI #4: A Big Year For AI" is out!

In this issue I'm discussing:
- My top 10 papers I read this year
- AI industry trends
- Open source highlights
- My yearly review routine

 https://magazine.sebastianraschka.com/p/ahead-of-ai-4-a-big-year-for-ai…

Happy New Year!
------
My ML & AI prediction for 2023: 

"open access" becomes the new "open source". Code will remain open source but be less useful due to data requirements & compute costs.

For demos or peer-reviews, devs will have to host models. Easy & cheap hosting will likely be a focus in 2023.
------
3 papers from my personal top-10 2022.*

8. A Generalist Agent
9. Robust Speech Recognition via Large-Scale Weak Supervision
10. Revisiting Pretraining Objectives for Tabular Deep Learning

*The top 7, incl write-ups, in tomorrow's Ahead of AI 
------
Happy New Year! 

What’s your theme for the year?
Personally, I am aiming to automate more mundane tasks again

My first project is automating the 2022 -> 2023 bumps on various websites (using my old, trusty py-args utils https://github.com/rasbt/py-args/tree/master/glob-replace…)
------
Fun fact: According to Google Scholar, researchers published about 22,000! vision transformer papers this year!
------
Not even a deep learning model can keep up with it all 
------
Sorry, I got too excited there. I didn't mean twenty-two-thousand-"factorial" of course :P
------
Vision transformers have taken the field of computer vision by storm, but what do vision transformers learn?

ViTs have fewer inductive biases than CNNs. However, it turns out that they learn inductive biases (or features) similar to CNNs!

1/2
------
That is, early layers capture edges and textures, and later layers learn more complex representations to capture broader concepts.

More analysis and insights in the paper: "What do vision transformers learn? A visual exploration" https://arxiv.org/abs/2212.06727

2/2
------
To celebrate the new year, there's a temporary discount on my book "Machine Learning with PyTorch and Scikit-Learn." 

For a limited time, you can purchase the ebook for just $5, which is a 90% discount.
------
Upon popular request, the monthly Ahead Of AI magazine is now also available on Substack: https://magazine.sebastianraschka.com

Enjoy the cleaner layout and higher resolution figures!
------
Wow, between Substack and LinkedIn, Ahead of AI reached 20k subscribers!

20k!! This is both very motivating and flattering!

PS: issue #4 is just around the corner, with my top-5 papers in 2022 and some other fun stuff.
------
Speak for yourself gdb! I still really enjoy  just writing code, even if I throw it all into the bin, and even after doing so for decades. It's a bit like a relaxing puzzle/game.

Using shitty libraries is not fun though, that's why I like to stick to few good core libraries.
------
Writing code for is not very fun for its own sake. What makes it insanely addictive is the feeling upon shipping — that actual people are doing something useful with what started as a figment of your imagination. Never gets old.
------
Open source had a good year!
I compiled a list of my favorite machine learning- and AI-related open-source libraries & releases that I discovered, used, or contributed to in 2022:
------
I forgot 
7) Reinforcement Learning with Human Feedback
------
The 6 different ways of using a language transformer / LLM

1 Train from scratch
2 Feature-based: Train new model on embeddings
3 Finetuning I: Freeze all but output layer weights
4 Finetuning II: Update all weights
5 Zero-shot learning
6 Few-shot learning

Anything missing?
1/6
------
Are you interested in learning how to use PyTorch's autograd module to compute gradients?
I've created a short, 5-minute video to get you started:
https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/3-4-automatic-differentiation-in-pytorch/…
------
I don't think this policy by 
@icmlconf
 will age well.

Its not clear to me what the reservations are -- they can ban "AI as a co-author", but not allowing it as a tool is a bit of an overreach without obviously stated harms.
------
this is kinda gate-keepy, @icmlconf
------
And so it begins ...
Researchers created a peer-review dataset & review comment generation model: https://arxiv.org/abs/2212.04972

"[...] if you thought Human Reviewer 2 was hard to reason with, just wait until reviewer 2 is a language model!" -- via 
@jackclarkSF
------
Researchers want LLMs (large language models) to mimic humans.
But let's also keep in mind that LLMs are highly effective at modeling proteins (as demonstrated by AlphaFold 2 and the ESM metagenomic Atlas) not in spite of but rather *because* they work differently than humans.
------
Would you have a recommendation for a good recent pure Python book, above beginner level, in addition to Fluent Python?
------
Saw lots of people sharing their reading list for 2023! Let me join! 

- A pile of paper first
- Modern Time Series Forecasting with Python (Joseph)
- Modern Deep Learning for Tabular Data (Ye & Wang)
- the new Probabilistic Machine Learning (Murphy), comes out in summer

1/2
------
Started my yearly review . Wow, apparently, I read 29 books in 2022 ...
Some of the more technical ones :

- Designing ML Systems
- The Kaggle Book
- NLP w Transformers
- The Book of Why
- Modeling Mindsets
Let me know if you would like to hear more about them, happy to chat!
------
- Fluent Python (2nd, Ramalho)
- Creating a Wordle Game in React and TypeScript (Gold)

And some fun ones I got for Xmas:
- Fairy Tale (King)
- The last 2 Witcher books
- The Nineties (Klosterman)

2/2
------
I am delighted to announce that the "real" camera-ready version of my new book, "Probabilistic Machine Learning: Advanced Topics", is now available. It will appear in print this summer, but it is already freely available online at http://probml.github.io/book2.
------
"Ahead of AI #4: A Big Year For AI" is out!

In this issue I'm discussing:
- My top 10 papers I read this year
- AI industry trends
- Open source highlights
- My yearly review routine

 https://magazine.sebastianraschka.com/p/ahead-of-ai-4-a-big-year-for-ai…

Happy New Year!
------
My ML & AI prediction for 2023: 

"open access" becomes the new "open source". Code will remain open source but be less useful due to data requirements & compute costs.

For demos or peer-reviews, devs will have to host models. Easy & cheap hosting will likely be a focus in 2023.
------
3 papers from my personal top-10 2022.*

8. A Generalist Agent
9. Robust Speech Recognition via Large-Scale Weak Supervision
10. Revisiting Pretraining Objectives for Tabular Deep Learning

*The top 7, incl write-ups, in tomorrow's Ahead of AI 
------
Happy New Year! 

What’s your theme for the year?
Personally, I am aiming to automate more mundane tasks again

My first project is automating the 2022 -> 2023 bumps on various websites (using my old, trusty py-args utils https://github.com/rasbt/py-args/tree/master/glob-replace…)
------
Fun fact: According to Google Scholar, researchers published about 22,000! vision transformer papers this year!
------
Not even a deep learning model can keep up with it all 
------
Sorry, I got too excited there. I didn't mean twenty-two-thousand-"factorial" of course :P
------
Vision transformers have taken the field of computer vision by storm, but what do vision transformers learn?

ViTs have fewer inductive biases than CNNs. However, it turns out that they learn inductive biases (or features) similar to CNNs!

1/2
------
That is, early layers capture edges and textures, and later layers learn more complex representations to capture broader concepts.

More analysis and insights in the paper: "What do vision transformers learn? A visual exploration" https://arxiv.org/abs/2212.06727

2/2
------
To celebrate the new year, there's a temporary discount on my book "Machine Learning with PyTorch and Scikit-Learn." 

For a limited time, you can purchase the ebook for just $5, which is a 90% discount.
------
Upon popular request, the monthly Ahead Of AI magazine is now also available on Substack: https://magazine.sebastianraschka.com

Enjoy the cleaner layout and higher resolution figures!
------
Wow, between Substack and LinkedIn, Ahead of AI reached 20k subscribers!

20k!! This is both very motivating and flattering!

PS: issue #4 is just around the corner, with my top-5 papers in 2022 and some other fun stuff.
------
Nope, ChatGPT is not ready to replace me yet 
------
Some techniques for optimizing inference speeds (without changing the model architecture):

(1) Parallelization
(2) Vectorization
(3) Loop tiling
(4) Operator fusion
(5) Quantization

Anything missing?

[1/6]
------
Some techniques for optimizing inference speeds (without changing the model architecture):

(1) Parallelization
(2) Vectorization
(3) Loop tiling
(4) Operator fusion
(5) Quantization

Anything missing?

[1/6]
------
Compiling my yearly top-papers list, and
"MaxViT: Multi-axis Vision Transformer"
should probably make the cut.

It's vision transformer with local-global interaction between visual tokens within a single block that didn't get the proper attention it deserved (no pun intended)
1/2
------
The local-global interaction between visual tokens is achieved via multi-axis attention, which can be decomposed into
1.  local attention ("block attention")
2.  global attention ("grid attention")
It scales linearly, not quadratically :)

Paper: https://arxiv.org/abs/2204.01697

2/2
------
If you are looking for a fun event after holidAIs 
------
Here's a great AI conference to kick-start 2023!

@abacusai is hosting :
𝐒𝐭𝐚𝐭𝐞𝐎𝐟𝐓𝐡𝐞𝐀𝐫𝐭() - Free AI Conference 

What's particularly exciting! 
Keynote session by @rasbt on "Generative AI, LLMs and ML in Production"

Reserve FREE spot!
https://eventbrite.com/e/stateoftheart-free-ai-conference-with-top-aiml-influencers-tickets-472810176967…
------
The "Hello World"s of machine learning:

2015: RandomForestClassifier on Iris
2017: MLP on MNIST
2019: AlexNet on Cifar-10
2022: DistilBERT on IMDb movie reviews
------
Scikit-learn 1.2 is out: https://github.com/scikit-learn/scikit-learn/releases/tag/1.2.0…

Was an eventful December & I totally missed the new release of my favorite ML lib

My personal highlights are around the HistGradientBoostingClassifier (if you haven't used it yet, it's a LightGBM impl that works really well)

1/2
------
I don't think this policy by 
@icmlconf
 will age well.

Its not clear to me what the reservations are -- they can ban "AI as a co-author", but not allowing it as a tool is a bit of an overreach without obviously stated harms.
------
this is kinda gate-keepy, @icmlconf
------
And so it begins ...
Researchers created a peer-review dataset & review comment generation model: https://arxiv.org/abs/2212.04972

"[...] if you thought Human Reviewer 2 was hard to reason with, just wait until reviewer 2 is a language model!" -- via 
@jackclarkSF
------
Researchers want LLMs (large language models) to mimic humans.
But let's also keep in mind that LLMs are highly effective at modeling proteins (as demonstrated by AlphaFold 2 and the ESM metagenomic Atlas) not in spite of but rather *because* they work differently than humans.
------
Would you have a recommendation for a good recent pure Python book, above beginner level, in addition to Fluent Python?
------
Saw lots of people sharing their reading list for 2023! Let me join! 

- A pile of paper first
- Modern Time Series Forecasting with Python (Joseph)
- Modern Deep Learning for Tabular Data (Ye & Wang)
- the new Probabilistic Machine Learning (Murphy), comes out in summer

1/2
------
Started my yearly review . Wow, apparently, I read 29 books in 2022 ...
Some of the more technical ones :

- Designing ML Systems
- The Kaggle Book
- NLP w Transformers
- The Book of Why
- Modeling Mindsets
Let me know if you would like to hear more about them, happy to chat!
------
- Fluent Python (2nd, Ramalho)
- Creating a Wordle Game in React and TypeScript (Gold)

And some fun ones I got for Xmas:
- Fairy Tale (King)
- The last 2 Witcher books
- The Nineties (Klosterman)

2/2
------
I am delighted to announce that the "real" camera-ready version of my new book, "Probabilistic Machine Learning: Advanced Topics", is now available. It will appear in print this summer, but it is already freely available online at http://probml.github.io/book2.
------
"Ahead of AI #4: A Big Year For AI" is out!

In this issue I'm discussing:
- My top 10 papers I read this year
- AI industry trends
- Open source highlights
- My yearly review routine

 https://magazine.sebastianraschka.com/p/ahead-of-ai-4-a-big-year-for-ai…

Happy New Year!
------
My ML & AI prediction for 2023: 

"open access" becomes the new "open source". Code will remain open source but be less useful due to data requirements & compute costs.

For demos or peer-reviews, devs will have to host models. Easy & cheap hosting will likely be a focus in 2023.
------
3 papers from my personal top-10 2022.*

8. A Generalist Agent
9. Robust Speech Recognition via Large-Scale Weak Supervision
10. Revisiting Pretraining Objectives for Tabular Deep Learning

*The top 7, incl write-ups, in tomorrow's Ahead of AI 
------
Happy New Year! 

What’s your theme for the year?
Personally, I am aiming to automate more mundane tasks again

My first project is automating the 2022 -> 2023 bumps on various websites (using my old, trusty py-args utils https://github.com/rasbt/py-args/tree/master/glob-replace…)
------
Fun fact: According to Google Scholar, researchers published about 22,000! vision transformer papers this year!
------
Not even a deep learning model can keep up with it all 
------
Sorry, I got too excited there. I didn't mean twenty-two-thousand-"factorial" of course :P
------
Vision transformers have taken the field of computer vision by storm, but what do vision transformers learn?

ViTs have fewer inductive biases than CNNs. However, it turns out that they learn inductive biases (or features) similar to CNNs!

1/2
------
That is, early layers capture edges and textures, and later layers learn more complex representations to capture broader concepts.

More analysis and insights in the paper: "What do vision transformers learn? A visual exploration" https://arxiv.org/abs/2212.06727

2/2
------
To celebrate the new year, there's a temporary discount on my book "Machine Learning with PyTorch and Scikit-Learn." 

For a limited time, you can purchase the ebook for just $5, which is a 90% discount.
------
Upon popular request, the monthly Ahead Of AI magazine is now also available on Substack: https://magazine.sebastianraschka.com

Enjoy the cleaner layout and higher resolution figures!
------
Wow, between Substack and LinkedIn, Ahead of AI reached 20k subscribers!

20k!! This is both very motivating and flattering!

PS: issue #4 is just around the corner, with my top-5 papers in 2022 and some other fun stuff.
------
Nope, ChatGPT is not ready to replace me yet 
------
Some techniques for optimizing inference speeds (without changing the model architecture):

(1) Parallelization
(2) Vectorization
(3) Loop tiling
(4) Operator fusion
(5) Quantization

Anything missing?

[1/6]
------
Some techniques for optimizing inference speeds (without changing the model architecture):

(1) Parallelization
(2) Vectorization
(3) Loop tiling
(4) Operator fusion
(5) Quantization

Anything missing?

[1/6]
------
Compiling my yearly top-papers list, and
"MaxViT: Multi-axis Vision Transformer"
should probably make the cut.

It's vision transformer with local-global interaction between visual tokens within a single block that didn't get the proper attention it deserved (no pun intended)
1/2
------
The local-global interaction between visual tokens is achieved via multi-axis attention, which can be decomposed into
1.  local attention ("block attention")
2.  global attention ("grid attention")
It scales linearly, not quadratically :)

Paper: https://arxiv.org/abs/2204.01697

2/2
------
If you are looking for a fun event after holidAIs 
------
Here's a great AI conference to kick-start 2023!

@abacusai is hosting :
𝐒𝐭𝐚𝐭𝐞𝐎𝐟𝐓𝐡𝐞𝐀𝐫𝐭() - Free AI Conference 

What's particularly exciting! 
Keynote session by @rasbt on "Generative AI, LLMs and ML in Production"

Reserve FREE spot!
https://eventbrite.com/e/stateoftheart-free-ai-conference-with-top-aiml-influencers-tickets-472810176967…
------
The "Hello World"s of machine learning:

2015: RandomForestClassifier on Iris
2017: MLP on MNIST
2019: AlexNet on Cifar-10
2022: DistilBERT on IMDb movie reviews
------
Scikit-learn 1.2 is out: https://github.com/scikit-learn/scikit-learn/releases/tag/1.2.0…

Was an eventful December & I totally missed the new release of my favorite ML lib

My personal highlights are around the HistGradientBoostingClassifier (if you haven't used it yet, it's a LightGBM impl that works really well)

1/2
------
I.e., HistGradientBoostingClassifier now supports

1.  interaction constraints (in trees, features that appear along a particular path are considered as "interacting")
2.  class weights
3.  feature names for categorical features

2/2
------
Started a Mathematics for Machine Learning book many years ago that evolved into an appendix for another book I was working on at the time (around 2018).

I just rediscovered my notes & it might be a fun thing to o pick up over the break.

https://sebastianraschka.com/resources/math-for-ml/…

Happy Holidays!
------
PPS: some “L04: Linear algebra and calculus for deep learning” videos I recorded:
------
PS: as little bonus, 4 short videos on linear algebra basics in a PyTorch context:
------
- Fluent Python (2nd, Ramalho)
- Creating a Wordle Game in React and TypeScript (Gold)

And some fun ones I got for Xmas:
- Fairy Tale (King)
- The last 2 Witcher books
- The Nineties (Klosterman)

2/2
------
I am delighted to announce that the "real" camera-ready version of my new book, "Probabilistic Machine Learning: Advanced Topics", is now available. It will appear in print this summer, but it is already freely available online at http://probml.github.io/book2.
------
"Ahead of AI #4: A Big Year For AI" is out!

In this issue I'm discussing:
- My top 10 papers I read this year
- AI industry trends
- Open source highlights
- My yearly review routine

 https://magazine.sebastianraschka.com/p/ahead-of-ai-4-a-big-year-for-ai…

Happy New Year!
------
My ML & AI prediction for 2023: 

"open access" becomes the new "open source". Code will remain open source but be less useful due to data requirements & compute costs.

For demos or peer-reviews, devs will have to host models. Easy & cheap hosting will likely be a focus in 2023.
------
3 papers from my personal top-10 2022.*

8. A Generalist Agent
9. Robust Speech Recognition via Large-Scale Weak Supervision
10. Revisiting Pretraining Objectives for Tabular Deep Learning

*The top 7, incl write-ups, in tomorrow's Ahead of AI 
------
Happy New Year! 

What’s your theme for the year?
Personally, I am aiming to automate more mundane tasks again

My first project is automating the 2022 -> 2023 bumps on various websites (using my old, trusty py-args utils https://github.com/rasbt/py-args/tree/master/glob-replace…)
------
Fun fact: According to Google Scholar, researchers published about 22,000! vision transformer papers this year!
------
Not even a deep learning model can keep up with it all 
------
Sorry, I got too excited there. I didn't mean twenty-two-thousand-"factorial" of course :P
------
Vision transformers have taken the field of computer vision by storm, but what do vision transformers learn?

ViTs have fewer inductive biases than CNNs. However, it turns out that they learn inductive biases (or features) similar to CNNs!

1/2
------
That is, early layers capture edges and textures, and later layers learn more complex representations to capture broader concepts.

More analysis and insights in the paper: "What do vision transformers learn? A visual exploration" https://arxiv.org/abs/2212.06727

2/2
------
To celebrate the new year, there's a temporary discount on my book "Machine Learning with PyTorch and Scikit-Learn." 

For a limited time, you can purchase the ebook for just $5, which is a 90% discount.
------
Upon popular request, the monthly Ahead Of AI magazine is now also available on Substack: https://magazine.sebastianraschka.com

Enjoy the cleaner layout and higher resolution figures!
------
Wow, between Substack and LinkedIn, Ahead of AI reached 20k subscribers!

20k!! This is both very motivating and flattering!

PS: issue #4 is just around the corner, with my top-5 papers in 2022 and some other fun stuff.
------
Nope, ChatGPT is not ready to replace me yet 
------
Some techniques for optimizing inference speeds (without changing the model architecture):

(1) Parallelization
(2) Vectorization
(3) Loop tiling
(4) Operator fusion
(5) Quantization

Anything missing?

[1/6]
------
Some techniques for optimizing inference speeds (without changing the model architecture):

(1) Parallelization
(2) Vectorization
(3) Loop tiling
(4) Operator fusion
(5) Quantization

Anything missing?

[1/6]
------
Compiling my yearly top-papers list, and
"MaxViT: Multi-axis Vision Transformer"
should probably make the cut.

It's vision transformer with local-global interaction between visual tokens within a single block that didn't get the proper attention it deserved (no pun intended)
1/2
------
The local-global interaction between visual tokens is achieved via multi-axis attention, which can be decomposed into
1.  local attention ("block attention")
2.  global attention ("grid attention")
It scales linearly, not quadratically :)

Paper: https://arxiv.org/abs/2204.01697

2/2
------
If you are looking for a fun event after holidAIs 
------
Here's a great AI conference to kick-start 2023!

@abacusai is hosting :
𝐒𝐭𝐚𝐭𝐞𝐎𝐟𝐓𝐡𝐞𝐀𝐫𝐭() - Free AI Conference 

What's particularly exciting! 
Keynote session by @rasbt on "Generative AI, LLMs and ML in Production"

Reserve FREE spot!
https://eventbrite.com/e/stateoftheart-free-ai-conference-with-top-aiml-influencers-tickets-472810176967…
------
The "Hello World"s of machine learning:

2015: RandomForestClassifier on Iris
2017: MLP on MNIST
2019: AlexNet on Cifar-10
2022: DistilBERT on IMDb movie reviews
------
Scikit-learn 1.2 is out: https://github.com/scikit-learn/scikit-learn/releases/tag/1.2.0…

Was an eventful December & I totally missed the new release of my favorite ML lib

My personal highlights are around the HistGradientBoostingClassifier (if you haven't used it yet, it's a LightGBM impl that works really well)

1/2
------
I.e., HistGradientBoostingClassifier now supports

1.  interaction constraints (in trees, features that appear along a particular path are considered as "interacting")
2.  class weights
3.  feature names for categorical features

2/2
------
Started a Mathematics for Machine Learning book many years ago that evolved into an appendix for another book I was working on at the time (around 2018).

I just rediscovered my notes & it might be a fun thing to o pick up over the break.

https://sebastianraschka.com/resources/math-for-ml/…

Happy Holidays!
------
PPS: some “L04: Linear algebra and calculus for deep learning” videos I recorded:
------
PS: as little bonus, 4 short videos on linear algebra basics in a PyTorch context:
------
If you can’t decide whether ChatGPT should replace traditional web search, just have them side by side.
------
Today, youChat goes live.

Open, broadly capable, conversational AI for search with knowledge of recent events and citations of sources.

Search and chat of the future: https://you.com/search?q=what+was+the+recent+breakthrough+in+fusion+research%3F…
------
We are running out of a vital resource: words!

There are “only” 5 to 10 trillion high-quality words (papers, books, code) on the internet.  Our AI models will have used all of that for training by 2026. Low-quality data (tweets, fanfic) will last to 2040. https://arxiv.org/pdf/2211.04325.pdf…
------
High school students are going to be the most experienced prompt engineers; they will be in high demand for employment at tech companies.
------
In 10 years, the world will be very different from today.

Once upon a time, it was okay to learn sth in school & then apply it for the rest of your life. 

Today, to keep up and thrive with the rapid pace of change, it's pivotal to keep reinventing yourself and to keep learning.
------
Call for Proposals for #SciPy2023 is open! Join us in #ATX on July 10-14, 2023 for the 22nd annual SciPyConf  

Have a talk, tutorial, or poster you want to share? Get your submission in before February 22, 2023 More conference info coming soon 
------
Fun fact: According to Google Scholar, researchers published about 22,000! vision transformer papers this year!
------
Not even a deep learning model can keep up with it all 
------
Sorry, I got too excited there. I didn't mean twenty-two-thousand-"factorial" of course :P
------
Vision transformers have taken the field of computer vision by storm, but what do vision transformers learn?

ViTs have fewer inductive biases than CNNs. However, it turns out that they learn inductive biases (or features) similar to CNNs!

1/2
------
That is, early layers capture edges and textures, and later layers learn more complex representations to capture broader concepts.

More analysis and insights in the paper: "What do vision transformers learn? A visual exploration" https://arxiv.org/abs/2212.06727

2/2
------
To celebrate the new year, there's a temporary discount on my book "Machine Learning with PyTorch and Scikit-Learn." 

For a limited time, you can purchase the ebook for just $5, which is a 90% discount.
------
Upon popular request, the monthly Ahead Of AI magazine is now also available on Substack: https://magazine.sebastianraschka.com

Enjoy the cleaner layout and higher resolution figures!
------
Wow, between Substack and LinkedIn, Ahead of AI reached 20k subscribers!

20k!! This is both very motivating and flattering!

PS: issue #4 is just around the corner, with my top-5 papers in 2022 and some other fun stuff.
------
Nope, ChatGPT is not ready to replace me yet 
------
Some techniques for optimizing inference speeds (without changing the model architecture):

(1) Parallelization
(2) Vectorization
(3) Loop tiling
(4) Operator fusion
(5) Quantization

Anything missing?

[1/6]
------
Some techniques for optimizing inference speeds (without changing the model architecture):

(1) Parallelization
(2) Vectorization
(3) Loop tiling
(4) Operator fusion
(5) Quantization

Anything missing?

[1/6]
------
Compiling my yearly top-papers list, and
"MaxViT: Multi-axis Vision Transformer"
should probably make the cut.

It's vision transformer with local-global interaction between visual tokens within a single block that didn't get the proper attention it deserved (no pun intended)
1/2
------
The local-global interaction between visual tokens is achieved via multi-axis attention, which can be decomposed into
1.  local attention ("block attention")
2.  global attention ("grid attention")
It scales linearly, not quadratically :)

Paper: https://arxiv.org/abs/2204.01697

2/2
------
If you are looking for a fun event after holidAIs 
------
Here's a great AI conference to kick-start 2023!

@abacusai is hosting :
𝐒𝐭𝐚𝐭𝐞𝐎𝐟𝐓𝐡𝐞𝐀𝐫𝐭() - Free AI Conference 

What's particularly exciting! 
Keynote session by @rasbt on "Generative AI, LLMs and ML in Production"

Reserve FREE spot!
https://eventbrite.com/e/stateoftheart-free-ai-conference-with-top-aiml-influencers-tickets-472810176967…
------
The "Hello World"s of machine learning:

2015: RandomForestClassifier on Iris
2017: MLP on MNIST
2019: AlexNet on Cifar-10
2022: DistilBERT on IMDb movie reviews
------
Scikit-learn 1.2 is out: https://github.com/scikit-learn/scikit-learn/releases/tag/1.2.0…

Was an eventful December & I totally missed the new release of my favorite ML lib

My personal highlights are around the HistGradientBoostingClassifier (if you haven't used it yet, it's a LightGBM impl that works really well)

1/2
------
I.e., HistGradientBoostingClassifier now supports

1.  interaction constraints (in trees, features that appear along a particular path are considered as "interacting")
2.  class weights
3.  feature names for categorical features

2/2
------
Started a Mathematics for Machine Learning book many years ago that evolved into an appendix for another book I was working on at the time (around 2018).

I just rediscovered my notes & it might be a fun thing to o pick up over the break.

https://sebastianraschka.com/resources/math-for-ml/…

Happy Holidays!
------
PPS: some “L04: Linear algebra and calculus for deep learning” videos I recorded:
------
PS: as little bonus, 4 short videos on linear algebra basics in a PyTorch context:
------
If you can’t decide whether ChatGPT should replace traditional web search, just have them side by side.
------
Today, youChat goes live.

Open, broadly capable, conversational AI for search with knowledge of recent events and citations of sources.

Search and chat of the future: https://you.com/search?q=what+was+the+recent+breakthrough+in+fusion+research%3F…
------
We are running out of a vital resource: words!

There are “only” 5 to 10 trillion high-quality words (papers, books, code) on the internet.  Our AI models will have used all of that for training by 2026. Low-quality data (tweets, fanfic) will last to 2040. https://arxiv.org/pdf/2211.04325.pdf…
------
High school students are going to be the most experienced prompt engineers; they will be in high demand for employment at tech companies.
------
In 10 years, the world will be very different from today.

Once upon a time, it was okay to learn sth in school & then apply it for the rest of your life. 

Today, to keep up and thrive with the rapid pace of change, it's pivotal to keep reinventing yourself and to keep learning.
------
Call for Proposals for #SciPy2023 is open! Join us in #ATX on July 10-14, 2023 for the 22nd annual SciPyConf  

Have a talk, tutorial, or poster you want to share? Get your submission in before February 22, 2023 More conference info coming soon 
------
I wasn’t suggesting the ChatGPT *should* replace Google Search. 

But some people were saying that ChatGPT wasn’t even a discussion topic for Search.

Well here you have it now  

https://cnet.com/google-amp/news/chatgpt-caused-code-red-at-google-report-says/…
------
Re: Replacing Google Search with ChatGPT

If Google Search used Chat-GPT, Google would probably be bankrupt in a month or so given that there are 8.5 billion search queries a day.

Using LLMs in/for Google Search makes sense though. They already started doing that years ago.
------
I want to learn to become better at saying No and stay more focused so that I can learn more things more efficiently 
------
I know you all can't wait to fine-tune your LLMs over the holiday break! We just added a template that gets you started. 
E.g., copy and paste the code from the website here to fine-tune a 3-billion param BLOOM model on a multi-node GPU cluster: https://lightning.ai
------
How do language models of different sizes learn during the course of pre-training? 

We study the training trajectories with training checkpoints of language model from 125M to 175B for a better understanding!

Check out our new paper : http://arxiv.org/abs/2212.09803
(1/N)
------
To celebrate the new year, there's a temporary discount on my book "Machine Learning with PyTorch and Scikit-Learn." 

For a limited time, you can purchase the ebook for just $5, which is a 90% discount.
------
Upon popular request, the monthly Ahead Of AI magazine is now also available on Substack: https://magazine.sebastianraschka.com

Enjoy the cleaner layout and higher resolution figures!
------
Wow, between Substack and LinkedIn, Ahead of AI reached 20k subscribers!

20k!! This is both very motivating and flattering!

PS: issue #4 is just around the corner, with my top-5 papers in 2022 and some other fun stuff.
------
Nope, ChatGPT is not ready to replace me yet 
------
Some techniques for optimizing inference speeds (without changing the model architecture):

(1) Parallelization
(2) Vectorization
(3) Loop tiling
(4) Operator fusion
(5) Quantization

Anything missing?

[1/6]
------
Some techniques for optimizing inference speeds (without changing the model architecture):

(1) Parallelization
(2) Vectorization
(3) Loop tiling
(4) Operator fusion
(5) Quantization

Anything missing?

[1/6]
------
Compiling my yearly top-papers list, and
"MaxViT: Multi-axis Vision Transformer"
should probably make the cut.

It's vision transformer with local-global interaction between visual tokens within a single block that didn't get the proper attention it deserved (no pun intended)
1/2
------
The local-global interaction between visual tokens is achieved via multi-axis attention, which can be decomposed into
1.  local attention ("block attention")
2.  global attention ("grid attention")
It scales linearly, not quadratically :)

Paper: https://arxiv.org/abs/2204.01697

2/2
------
If you are looking for a fun event after holidAIs 
------
Here's a great AI conference to kick-start 2023!

@abacusai is hosting :
𝐒𝐭𝐚𝐭𝐞𝐎𝐟𝐓𝐡𝐞𝐀𝐫𝐭() - Free AI Conference 

What's particularly exciting! 
Keynote session by @rasbt on "Generative AI, LLMs and ML in Production"

Reserve FREE spot!
https://eventbrite.com/e/stateoftheart-free-ai-conference-with-top-aiml-influencers-tickets-472810176967…
------
The "Hello World"s of machine learning:

2015: RandomForestClassifier on Iris
2017: MLP on MNIST
2019: AlexNet on Cifar-10
2022: DistilBERT on IMDb movie reviews
------
Scikit-learn 1.2 is out: https://github.com/scikit-learn/scikit-learn/releases/tag/1.2.0…

Was an eventful December & I totally missed the new release of my favorite ML lib

My personal highlights are around the HistGradientBoostingClassifier (if you haven't used it yet, it's a LightGBM impl that works really well)

1/2
------
I.e., HistGradientBoostingClassifier now supports

1.  interaction constraints (in trees, features that appear along a particular path are considered as "interacting")
2.  class weights
3.  feature names for categorical features

2/2
------
Started a Mathematics for Machine Learning book many years ago that evolved into an appendix for another book I was working on at the time (around 2018).

I just rediscovered my notes & it might be a fun thing to o pick up over the break.

https://sebastianraschka.com/resources/math-for-ml/…

Happy Holidays!
------
PPS: some “L04: Linear algebra and calculus for deep learning” videos I recorded:
------
PS: as little bonus, 4 short videos on linear algebra basics in a PyTorch context:
------
If you can’t decide whether ChatGPT should replace traditional web search, just have them side by side.
------
Today, youChat goes live.

Open, broadly capable, conversational AI for search with knowledge of recent events and citations of sources.

Search and chat of the future: https://you.com/search?q=what+was+the+recent+breakthrough+in+fusion+research%3F…
------
We are running out of a vital resource: words!

There are “only” 5 to 10 trillion high-quality words (papers, books, code) on the internet.  Our AI models will have used all of that for training by 2026. Low-quality data (tweets, fanfic) will last to 2040. https://arxiv.org/pdf/2211.04325.pdf…
------
High school students are going to be the most experienced prompt engineers; they will be in high demand for employment at tech companies.
------
In 10 years, the world will be very different from today.

Once upon a time, it was okay to learn sth in school & then apply it for the rest of your life. 

Today, to keep up and thrive with the rapid pace of change, it's pivotal to keep reinventing yourself and to keep learning.
------
Call for Proposals for #SciPy2023 is open! Join us in #ATX on July 10-14, 2023 for the 22nd annual SciPyConf  

Have a talk, tutorial, or poster you want to share? Get your submission in before February 22, 2023 More conference info coming soon 
------
I wasn’t suggesting the ChatGPT *should* replace Google Search. 

But some people were saying that ChatGPT wasn’t even a discussion topic for Search.

Well here you have it now  

https://cnet.com/google-amp/news/chatgpt-caused-code-red-at-google-report-says/…
------
Re: Replacing Google Search with ChatGPT

If Google Search used Chat-GPT, Google would probably be bankrupt in a month or so given that there are 8.5 billion search queries a day.

Using LLMs in/for Google Search makes sense though. They already started doing that years ago.
------
I want to learn to become better at saying No and stay more focused so that I can learn more things more efficiently 
------
I know you all can't wait to fine-tune your LLMs over the holiday break! We just added a template that gets you started. 
E.g., copy and paste the code from the website here to fine-tune a 3-billion param BLOOM model on a multi-node GPU cluster: https://lightning.ai
------
How do language models of different sizes learn during the course of pre-training? 

We study the training trajectories with training checkpoints of language model from 125M to 175B for a better understanding!

Check out our new paper : http://arxiv.org/abs/2212.09803
(1/N)
------
For my deep learning projects I use both 
- notebooks (for analysis) and 
- .py scripts (for submitting/running code)

I have a template for my typical deep learning workflow in the README here: https://github.com/rasbt/machine-learning-notes/tree/main/templates/pl_classifier…
------
Same, having too many browser tabs open makes me anxious. 
That’s why I am using JupyterLab over Jupyter Notebook 
------
i don’t understand tab hoarding. i don’t think i ever have more than 15 tabs open. when i finish the day i just close them all. no regrets
------
Thanks for all the positive feedback on my new, free deep learning course!

Happy to share that Unit 3 is now live!

Math can be fun. But let's be honest, who doesn't like the convenience of automatic differentiation 

 Unit 3: https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/…

Enjoy the holidays!
------
The focus of this one is on

- computation graphs
- full-batch vs stochastic gradient descent
- automatic differentiation
- the PyTorch API

Covering all the foundations before we'll dive deeper into multilayer neural networks in Unit 4!
------
Btw. if you have any questions as you are taking the course don't hesitate to reach out, I am always excited to chat more about deep learning 
------
Compiling my yearly top-papers list, and
"MaxViT: Multi-axis Vision Transformer"
should probably make the cut.

It's vision transformer with local-global interaction between visual tokens within a single block that didn't get the proper attention it deserved (no pun intended)
1/2
------
The local-global interaction between visual tokens is achieved via multi-axis attention, which can be decomposed into
1.  local attention ("block attention")
2.  global attention ("grid attention")
It scales linearly, not quadratically :)

Paper: https://arxiv.org/abs/2204.01697

2/2
------
If you are looking for a fun event after holidAIs 
------
Here's a great AI conference to kick-start 2023!

@abacusai is hosting :
𝐒𝐭𝐚𝐭𝐞𝐎𝐟𝐓𝐡𝐞𝐀𝐫𝐭() - Free AI Conference 

What's particularly exciting! 
Keynote session by @rasbt on "Generative AI, LLMs and ML in Production"

Reserve FREE spot!
https://eventbrite.com/e/stateoftheart-free-ai-conference-with-top-aiml-influencers-tickets-472810176967…
------
The "Hello World"s of machine learning:

2015: RandomForestClassifier on Iris
2017: MLP on MNIST
2019: AlexNet on Cifar-10
2022: DistilBERT on IMDb movie reviews
------
Scikit-learn 1.2 is out: https://github.com/scikit-learn/scikit-learn/releases/tag/1.2.0…

Was an eventful December & I totally missed the new release of my favorite ML lib

My personal highlights are around the HistGradientBoostingClassifier (if you haven't used it yet, it's a LightGBM impl that works really well)

1/2
------
I.e., HistGradientBoostingClassifier now supports

1.  interaction constraints (in trees, features that appear along a particular path are considered as "interacting")
2.  class weights
3.  feature names for categorical features

2/2
------
Started a Mathematics for Machine Learning book many years ago that evolved into an appendix for another book I was working on at the time (around 2018).

I just rediscovered my notes & it might be a fun thing to o pick up over the break.

https://sebastianraschka.com/resources/math-for-ml/…

Happy Holidays!
------
PPS: some “L04: Linear algebra and calculus for deep learning” videos I recorded:
------
PS: as little bonus, 4 short videos on linear algebra basics in a PyTorch context:
------
If you can’t decide whether ChatGPT should replace traditional web search, just have them side by side.
------
Today, youChat goes live.

Open, broadly capable, conversational AI for search with knowledge of recent events and citations of sources.

Search and chat of the future: https://you.com/search?q=what+was+the+recent+breakthrough+in+fusion+research%3F…
------
We are running out of a vital resource: words!

There are “only” 5 to 10 trillion high-quality words (papers, books, code) on the internet.  Our AI models will have used all of that for training by 2026. Low-quality data (tweets, fanfic) will last to 2040. https://arxiv.org/pdf/2211.04325.pdf…
------
High school students are going to be the most experienced prompt engineers; they will be in high demand for employment at tech companies.
------
In 10 years, the world will be very different from today.

Once upon a time, it was okay to learn sth in school & then apply it for the rest of your life. 

Today, to keep up and thrive with the rapid pace of change, it's pivotal to keep reinventing yourself and to keep learning.
------
Call for Proposals for #SciPy2023 is open! Join us in #ATX on July 10-14, 2023 for the 22nd annual SciPyConf  

Have a talk, tutorial, or poster you want to share? Get your submission in before February 22, 2023 More conference info coming soon 
------
I wasn’t suggesting the ChatGPT *should* replace Google Search. 

But some people were saying that ChatGPT wasn’t even a discussion topic for Search.

Well here you have it now  

https://cnet.com/google-amp/news/chatgpt-caused-code-red-at-google-report-says/…
------
Re: Replacing Google Search with ChatGPT

If Google Search used Chat-GPT, Google would probably be bankrupt in a month or so given that there are 8.5 billion search queries a day.

Using LLMs in/for Google Search makes sense though. They already started doing that years ago.
------
I want to learn to become better at saying No and stay more focused so that I can learn more things more efficiently 
------
I know you all can't wait to fine-tune your LLMs over the holiday break! We just added a template that gets you started. 
E.g., copy and paste the code from the website here to fine-tune a 3-billion param BLOOM model on a multi-node GPU cluster: https://lightning.ai
------
How do language models of different sizes learn during the course of pre-training? 

We study the training trajectories with training checkpoints of language model from 125M to 175B for a better understanding!

Check out our new paper : http://arxiv.org/abs/2212.09803
(1/N)
------
For my deep learning projects I use both 
- notebooks (for analysis) and 
- .py scripts (for submitting/running code)

I have a template for my typical deep learning workflow in the README here: https://github.com/rasbt/machine-learning-notes/tree/main/templates/pl_classifier…
------
Same, having too many browser tabs open makes me anxious. 
That’s why I am using JupyterLab over Jupyter Notebook 
------
i don’t understand tab hoarding. i don’t think i ever have more than 15 tabs open. when i finish the day i just close them all. no regrets
------
Thanks for all the positive feedback on my new, free deep learning course!

Happy to share that Unit 3 is now live!

Math can be fun. But let's be honest, who doesn't like the convenience of automatic differentiation 

 Unit 3: https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/…

Enjoy the holidays!
------
The focus of this one is on

- computation graphs
- full-batch vs stochastic gradient descent
- automatic differentiation
- the PyTorch API

Covering all the foundations before we'll dive deeper into multilayer neural networks in Unit 4!
------
Btw. if you have any questions as you are taking the course don't hesitate to reach out, I am always excited to chat more about deep learning 
------
Scikit-learn 1.2 is out: https://github.com/scikit-learn/scikit-learn/releases/tag/1.2.0…

Was an eventful December & I totally missed the new release of my favorite ML lib

My personal highlights are around the HistGradientBoostingClassifier (if you haven't used it yet, it's a LightGBM impl that works really well)

1/2
------
I.e., HistGradientBoostingClassifier now supports

1.  interaction constraints (in trees, features that appear along a particular path are considered as "interacting")
2.  class weights
3.  feature names for categorical features

2/2
------
Started a Mathematics for Machine Learning book many years ago that evolved into an appendix for another book I was working on at the time (around 2018).

I just rediscovered my notes & it might be a fun thing to o pick up over the break.

https://sebastianraschka.com/resources/math-for-ml/…

Happy Holidays!
------
PPS: some “L04: Linear algebra and calculus for deep learning” videos I recorded:
------
PS: as little bonus, 4 short videos on linear algebra basics in a PyTorch context:
------
If you can’t decide whether ChatGPT should replace traditional web search, just have them side by side.
------
Today, youChat goes live.

Open, broadly capable, conversational AI for search with knowledge of recent events and citations of sources.

Search and chat of the future: https://you.com/search?q=what+was+the+recent+breakthrough+in+fusion+research%3F…
------
We are running out of a vital resource: words!

There are “only” 5 to 10 trillion high-quality words (papers, books, code) on the internet.  Our AI models will have used all of that for training by 2026. Low-quality data (tweets, fanfic) will last to 2040. https://arxiv.org/pdf/2211.04325.pdf…
------
High school students are going to be the most experienced prompt engineers; they will be in high demand for employment at tech companies.
------
In 10 years, the world will be very different from today.

Once upon a time, it was okay to learn sth in school & then apply it for the rest of your life. 

Today, to keep up and thrive with the rapid pace of change, it's pivotal to keep reinventing yourself and to keep learning.
------
Call for Proposals for #SciPy2023 is open! Join us in #ATX on July 10-14, 2023 for the 22nd annual SciPyConf  

Have a talk, tutorial, or poster you want to share? Get your submission in before February 22, 2023 More conference info coming soon 
------
I wasn’t suggesting the ChatGPT *should* replace Google Search. 

But some people were saying that ChatGPT wasn’t even a discussion topic for Search.

Well here you have it now  

https://cnet.com/google-amp/news/chatgpt-caused-code-red-at-google-report-says/…
------
Re: Replacing Google Search with ChatGPT

If Google Search used Chat-GPT, Google would probably be bankrupt in a month or so given that there are 8.5 billion search queries a day.

Using LLMs in/for Google Search makes sense though. They already started doing that years ago.
------
I want to learn to become better at saying No and stay more focused so that I can learn more things more efficiently 
------
I know you all can't wait to fine-tune your LLMs over the holiday break! We just added a template that gets you started. 
E.g., copy and paste the code from the website here to fine-tune a 3-billion param BLOOM model on a multi-node GPU cluster: https://lightning.ai
------
How do language models of different sizes learn during the course of pre-training? 

We study the training trajectories with training checkpoints of language model from 125M to 175B for a better understanding!

Check out our new paper : http://arxiv.org/abs/2212.09803
(1/N)
------
For my deep learning projects I use both 
- notebooks (for analysis) and 
- .py scripts (for submitting/running code)

I have a template for my typical deep learning workflow in the README here: https://github.com/rasbt/machine-learning-notes/tree/main/templates/pl_classifier…
------
Same, having too many browser tabs open makes me anxious. 
That’s why I am using JupyterLab over Jupyter Notebook 
------
i don’t understand tab hoarding. i don’t think i ever have more than 15 tabs open. when i finish the day i just close them all. no regrets
------
Thanks for all the positive feedback on my new, free deep learning course!

Happy to share that Unit 3 is now live!

Math can be fun. But let's be honest, who doesn't like the convenience of automatic differentiation 

 Unit 3: https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/…

Enjoy the holidays!
------
The focus of this one is on

- computation graphs
- full-batch vs stochastic gradient descent
- automatic differentiation
- the PyTorch API

Covering all the foundations before we'll dive deeper into multilayer neural networks in Unit 4!
------
Btw. if you have any questions as you are taking the course don't hesitate to reach out, I am always excited to chat more about deep learning 
------
If you can’t decide whether ChatGPT should replace traditional web search, just have them side by side.
------
Today, youChat goes live.

Open, broadly capable, conversational AI for search with knowledge of recent events and citations of sources.

Search and chat of the future: https://you.com/search?q=what+was+the+recent+breakthrough+in+fusion+research%3F…
------
We are running out of a vital resource: words!

There are “only” 5 to 10 trillion high-quality words (papers, books, code) on the internet.  Our AI models will have used all of that for training by 2026. Low-quality data (tweets, fanfic) will last to 2040. https://arxiv.org/pdf/2211.04325.pdf…
------
High school students are going to be the most experienced prompt engineers; they will be in high demand for employment at tech companies.
------
In 10 years, the world will be very different from today.

Once upon a time, it was okay to learn sth in school & then apply it for the rest of your life. 

Today, to keep up and thrive with the rapid pace of change, it's pivotal to keep reinventing yourself and to keep learning.
------
Call for Proposals for #SciPy2023 is open! Join us in #ATX on July 10-14, 2023 for the 22nd annual SciPyConf  

Have a talk, tutorial, or poster you want to share? Get your submission in before February 22, 2023 More conference info coming soon 
------
I wasn’t suggesting the ChatGPT *should* replace Google Search. 

But some people were saying that ChatGPT wasn’t even a discussion topic for Search.

Well here you have it now  

https://cnet.com/google-amp/news/chatgpt-caused-code-red-at-google-report-says/…
------
Re: Replacing Google Search with ChatGPT

If Google Search used Chat-GPT, Google would probably be bankrupt in a month or so given that there are 8.5 billion search queries a day.

Using LLMs in/for Google Search makes sense though. They already started doing that years ago.
------
I want to learn to become better at saying No and stay more focused so that I can learn more things more efficiently 
------
I know you all can't wait to fine-tune your LLMs over the holiday break! We just added a template that gets you started. 
E.g., copy and paste the code from the website here to fine-tune a 3-billion param BLOOM model on a multi-node GPU cluster: https://lightning.ai
------
How do language models of different sizes learn during the course of pre-training? 

We study the training trajectories with training checkpoints of language model from 125M to 175B for a better understanding!

Check out our new paper : http://arxiv.org/abs/2212.09803
(1/N)
------
For my deep learning projects I use both 
- notebooks (for analysis) and 
- .py scripts (for submitting/running code)

I have a template for my typical deep learning workflow in the README here: https://github.com/rasbt/machine-learning-notes/tree/main/templates/pl_classifier…
------
Same, having too many browser tabs open makes me anxious. 
That’s why I am using JupyterLab over Jupyter Notebook 
------
i don’t understand tab hoarding. i don’t think i ever have more than 15 tabs open. when i finish the day i just close them all. no regrets
------
Thanks for all the positive feedback on my new, free deep learning course!

Happy to share that Unit 3 is now live!

Math can be fun. But let's be honest, who doesn't like the convenience of automatic differentiation 

 Unit 3: https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/…

Enjoy the holidays!
------
The focus of this one is on

- computation graphs
- full-batch vs stochastic gradient descent
- automatic differentiation
- the PyTorch API

Covering all the foundations before we'll dive deeper into multilayer neural networks in Unit 4!
------
Btw. if you have any questions as you are taking the course don't hesitate to reach out, I am always excited to chat more about deep learning 
------
If you can’t decide whether ChatGPT should replace traditional web search, just have them side by side.
------
Today, youChat goes live.

Open, broadly capable, conversational AI for search with knowledge of recent events and citations of sources.

Search and chat of the future: https://you.com/search?q=what+was+the+recent+breakthrough+in+fusion+research%3F…
------
We are running out of a vital resource: words!

There are “only” 5 to 10 trillion high-quality words (papers, books, code) on the internet.  Our AI models will have used all of that for training by 2026. Low-quality data (tweets, fanfic) will last to 2040. https://arxiv.org/pdf/2211.04325.pdf…
------
High school students are going to be the most experienced prompt engineers; they will be in high demand for employment at tech companies.
------
In 10 years, the world will be very different from today.

Once upon a time, it was okay to learn sth in school & then apply it for the rest of your life. 

Today, to keep up and thrive with the rapid pace of change, it's pivotal to keep reinventing yourself and to keep learning.
------
Call for Proposals for #SciPy2023 is open! Join us in #ATX on July 10-14, 2023 for the 22nd annual SciPyConf  

Have a talk, tutorial, or poster you want to share? Get your submission in before February 22, 2023 More conference info coming soon 
------
I wasn’t suggesting the ChatGPT *should* replace Google Search. 

But some people were saying that ChatGPT wasn’t even a discussion topic for Search.

Well here you have it now  

https://cnet.com/google-amp/news/chatgpt-caused-code-red-at-google-report-says/…
------
Re: Replacing Google Search with ChatGPT

If Google Search used Chat-GPT, Google would probably be bankrupt in a month or so given that there are 8.5 billion search queries a day.

Using LLMs in/for Google Search makes sense though. They already started doing that years ago.
------
I want to learn to become better at saying No and stay more focused so that I can learn more things more efficiently 
------
I know you all can't wait to fine-tune your LLMs over the holiday break! We just added a template that gets you started. 
E.g., copy and paste the code from the website here to fine-tune a 3-billion param BLOOM model on a multi-node GPU cluster: https://lightning.ai
------
How do language models of different sizes learn during the course of pre-training? 

We study the training trajectories with training checkpoints of language model from 125M to 175B for a better understanding!

Check out our new paper : http://arxiv.org/abs/2212.09803
(1/N)
------
For my deep learning projects I use both 
- notebooks (for analysis) and 
- .py scripts (for submitting/running code)

I have a template for my typical deep learning workflow in the README here: https://github.com/rasbt/machine-learning-notes/tree/main/templates/pl_classifier…
------
Same, having too many browser tabs open makes me anxious. 
That’s why I am using JupyterLab over Jupyter Notebook 
------
i don’t understand tab hoarding. i don’t think i ever have more than 15 tabs open. when i finish the day i just close them all. no regrets
------
Thanks for all the positive feedback on my new, free deep learning course!

Happy to share that Unit 3 is now live!

Math can be fun. But let's be honest, who doesn't like the convenience of automatic differentiation 

 Unit 3: https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/…

Enjoy the holidays!
------
The focus of this one is on

- computation graphs
- full-batch vs stochastic gradient descent
- automatic differentiation
- the PyTorch API

Covering all the foundations before we'll dive deeper into multilayer neural networks in Unit 4!
------
Btw. if you have any questions as you are taking the course don't hesitate to reach out, I am always excited to chat more about deep learning 
------
If you can’t decide whether ChatGPT should replace traditional web search, just have them side by side.
------
Today, youChat goes live.

Open, broadly capable, conversational AI for search with knowledge of recent events and citations of sources.

Search and chat of the future: https://you.com/search?q=what+was+the+recent+breakthrough+in+fusion+research%3F…
------
We are running out of a vital resource: words!

There are “only” 5 to 10 trillion high-quality words (papers, books, code) on the internet.  Our AI models will have used all of that for training by 2026. Low-quality data (tweets, fanfic) will last to 2040. https://arxiv.org/pdf/2211.04325.pdf…
------
High school students are going to be the most experienced prompt engineers; they will be in high demand for employment at tech companies.
------
In 10 years, the world will be very different from today.

Once upon a time, it was okay to learn sth in school & then apply it for the rest of your life. 

Today, to keep up and thrive with the rapid pace of change, it's pivotal to keep reinventing yourself and to keep learning.
------
Call for Proposals for #SciPy2023 is open! Join us in #ATX on July 10-14, 2023 for the 22nd annual SciPyConf  

Have a talk, tutorial, or poster you want to share? Get your submission in before February 22, 2023 More conference info coming soon 
------
I wasn’t suggesting the ChatGPT *should* replace Google Search. 

But some people were saying that ChatGPT wasn’t even a discussion topic for Search.

Well here you have it now  

https://cnet.com/google-amp/news/chatgpt-caused-code-red-at-google-report-says/…
------
Re: Replacing Google Search with ChatGPT

If Google Search used Chat-GPT, Google would probably be bankrupt in a month or so given that there are 8.5 billion search queries a day.

Using LLMs in/for Google Search makes sense though. They already started doing that years ago.
------
I want to learn to become better at saying No and stay more focused so that I can learn more things more efficiently 
------
I know you all can't wait to fine-tune your LLMs over the holiday break! We just added a template that gets you started. 
E.g., copy and paste the code from the website here to fine-tune a 3-billion param BLOOM model on a multi-node GPU cluster: https://lightning.ai
------
How do language models of different sizes learn during the course of pre-training? 

We study the training trajectories with training checkpoints of language model from 125M to 175B for a better understanding!

Check out our new paper : http://arxiv.org/abs/2212.09803
(1/N)
------
For my deep learning projects I use both 
- notebooks (for analysis) and 
- .py scripts (for submitting/running code)

I have a template for my typical deep learning workflow in the README here: https://github.com/rasbt/machine-learning-notes/tree/main/templates/pl_classifier…
------
Same, having too many browser tabs open makes me anxious. 
That’s why I am using JupyterLab over Jupyter Notebook 
------
i don’t understand tab hoarding. i don’t think i ever have more than 15 tabs open. when i finish the day i just close them all. no regrets
------
Thanks for all the positive feedback on my new, free deep learning course!

Happy to share that Unit 3 is now live!

Math can be fun. But let's be honest, who doesn't like the convenience of automatic differentiation 

 Unit 3: https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/…

Enjoy the holidays!
------
The focus of this one is on

- computation graphs
- full-batch vs stochastic gradient descent
- automatic differentiation
- the PyTorch API

Covering all the foundations before we'll dive deeper into multilayer neural networks in Unit 4!
------
Btw. if you have any questions as you are taking the course don't hesitate to reach out, I am always excited to chat more about deep learning 
------
If you can’t decide whether ChatGPT should replace traditional web search, just have them side by side.
------
Today, youChat goes live.

Open, broadly capable, conversational AI for search with knowledge of recent events and citations of sources.

Search and chat of the future: https://you.com/search?q=what+was+the+recent+breakthrough+in+fusion+research%3F…
------
We are running out of a vital resource: words!

There are “only” 5 to 10 trillion high-quality words (papers, books, code) on the internet.  Our AI models will have used all of that for training by 2026. Low-quality data (tweets, fanfic) will last to 2040. https://arxiv.org/pdf/2211.04325.pdf…
------
High school students are going to be the most experienced prompt engineers; they will be in high demand for employment at tech companies.
------
In 10 years, the world will be very different from today.

Once upon a time, it was okay to learn sth in school & then apply it for the rest of your life. 

Today, to keep up and thrive with the rapid pace of change, it's pivotal to keep reinventing yourself and to keep learning.
------
Call for Proposals for #SciPy2023 is open! Join us in #ATX on July 10-14, 2023 for the 22nd annual SciPyConf  

Have a talk, tutorial, or poster you want to share? Get your submission in before February 22, 2023 More conference info coming soon 
------
I wasn’t suggesting the ChatGPT *should* replace Google Search. 

But some people were saying that ChatGPT wasn’t even a discussion topic for Search.

Well here you have it now  

https://cnet.com/google-amp/news/chatgpt-caused-code-red-at-google-report-says/…
------
Re: Replacing Google Search with ChatGPT

If Google Search used Chat-GPT, Google would probably be bankrupt in a month or so given that there are 8.5 billion search queries a day.

Using LLMs in/for Google Search makes sense though. They already started doing that years ago.
------
I want to learn to become better at saying No and stay more focused so that I can learn more things more efficiently 
------
I know you all can't wait to fine-tune your LLMs over the holiday break! We just added a template that gets you started. 
E.g., copy and paste the code from the website here to fine-tune a 3-billion param BLOOM model on a multi-node GPU cluster: https://lightning.ai
------
How do language models of different sizes learn during the course of pre-training? 

We study the training trajectories with training checkpoints of language model from 125M to 175B for a better understanding!

Check out our new paper : http://arxiv.org/abs/2212.09803
(1/N)
------
For my deep learning projects I use both 
- notebooks (for analysis) and 
- .py scripts (for submitting/running code)

I have a template for my typical deep learning workflow in the README here: https://github.com/rasbt/machine-learning-notes/tree/main/templates/pl_classifier…
------
Same, having too many browser tabs open makes me anxious. 
That’s why I am using JupyterLab over Jupyter Notebook 
------
i don’t understand tab hoarding. i don’t think i ever have more than 15 tabs open. when i finish the day i just close them all. no regrets
------
Thanks for all the positive feedback on my new, free deep learning course!

Happy to share that Unit 3 is now live!

Math can be fun. But let's be honest, who doesn't like the convenience of automatic differentiation 

 Unit 3: https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/…

Enjoy the holidays!
------
The focus of this one is on

- computation graphs
- full-batch vs stochastic gradient descent
- automatic differentiation
- the PyTorch API

Covering all the foundations before we'll dive deeper into multilayer neural networks in Unit 4!
------
Btw. if you have any questions as you are taking the course don't hesitate to reach out, I am always excited to chat more about deep learning 
------
If you can’t decide whether ChatGPT should replace traditional web search, just have them side by side.
------
Today, youChat goes live.

Open, broadly capable, conversational AI for search with knowledge of recent events and citations of sources.

Search and chat of the future: https://you.com/search?q=what+was+the+recent+breakthrough+in+fusion+research%3F…
------
We are running out of a vital resource: words!

There are “only” 5 to 10 trillion high-quality words (papers, books, code) on the internet.  Our AI models will have used all of that for training by 2026. Low-quality data (tweets, fanfic) will last to 2040. https://arxiv.org/pdf/2211.04325.pdf…
------
High school students are going to be the most experienced prompt engineers; they will be in high demand for employment at tech companies.
------
In 10 years, the world will be very different from today.

Once upon a time, it was okay to learn sth in school & then apply it for the rest of your life. 

Today, to keep up and thrive with the rapid pace of change, it's pivotal to keep reinventing yourself and to keep learning.
------
Call for Proposals for #SciPy2023 is open! Join us in #ATX on July 10-14, 2023 for the 22nd annual SciPyConf  

Have a talk, tutorial, or poster you want to share? Get your submission in before February 22, 2023 More conference info coming soon 
------
I wasn’t suggesting the ChatGPT *should* replace Google Search. 

But some people were saying that ChatGPT wasn’t even a discussion topic for Search.

Well here you have it now  

https://cnet.com/google-amp/news/chatgpt-caused-code-red-at-google-report-says/…
------
Re: Replacing Google Search with ChatGPT

If Google Search used Chat-GPT, Google would probably be bankrupt in a month or so given that there are 8.5 billion search queries a day.

Using LLMs in/for Google Search makes sense though. They already started doing that years ago.
------
I want to learn to become better at saying No and stay more focused so that I can learn more things more efficiently 
------
I know you all can't wait to fine-tune your LLMs over the holiday break! We just added a template that gets you started. 
E.g., copy and paste the code from the website here to fine-tune a 3-billion param BLOOM model on a multi-node GPU cluster: https://lightning.ai
------
How do language models of different sizes learn during the course of pre-training? 

We study the training trajectories with training checkpoints of language model from 125M to 175B for a better understanding!

Check out our new paper : http://arxiv.org/abs/2212.09803
(1/N)
------
For my deep learning projects I use both 
- notebooks (for analysis) and 
- .py scripts (for submitting/running code)

I have a template for my typical deep learning workflow in the README here: https://github.com/rasbt/machine-learning-notes/tree/main/templates/pl_classifier…
------
Same, having too many browser tabs open makes me anxious. 
That’s why I am using JupyterLab over Jupyter Notebook 
------
i don’t understand tab hoarding. i don’t think i ever have more than 15 tabs open. when i finish the day i just close them all. no regrets
------
Thanks for all the positive feedback on my new, free deep learning course!

Happy to share that Unit 3 is now live!

Math can be fun. But let's be honest, who doesn't like the convenience of automatic differentiation 

 Unit 3: https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/…

Enjoy the holidays!
------
The focus of this one is on

- computation graphs
- full-batch vs stochastic gradient descent
- automatic differentiation
- the PyTorch API

Covering all the foundations before we'll dive deeper into multilayer neural networks in Unit 4!
------
Btw. if you have any questions as you are taking the course don't hesitate to reach out, I am always excited to chat more about deep learning 
------
If you can’t decide whether ChatGPT should replace traditional web search, just have them side by side.
------
Today, youChat goes live.

Open, broadly capable, conversational AI for search with knowledge of recent events and citations of sources.

Search and chat of the future: https://you.com/search?q=what+was+the+recent+breakthrough+in+fusion+research%3F…
------
We are running out of a vital resource: words!

There are “only” 5 to 10 trillion high-quality words (papers, books, code) on the internet.  Our AI models will have used all of that for training by 2026. Low-quality data (tweets, fanfic) will last to 2040. https://arxiv.org/pdf/2211.04325.pdf…
------
High school students are going to be the most experienced prompt engineers; they will be in high demand for employment at tech companies.
------
In 10 years, the world will be very different from today.

Once upon a time, it was okay to learn sth in school & then apply it for the rest of your life. 

Today, to keep up and thrive with the rapid pace of change, it's pivotal to keep reinventing yourself and to keep learning.
------
Call for Proposals for #SciPy2023 is open! Join us in #ATX on July 10-14, 2023 for the 22nd annual SciPyConf  

Have a talk, tutorial, or poster you want to share? Get your submission in before February 22, 2023 More conference info coming soon 
------
I wasn’t suggesting the ChatGPT *should* replace Google Search. 

But some people were saying that ChatGPT wasn’t even a discussion topic for Search.

Well here you have it now  

https://cnet.com/google-amp/news/chatgpt-caused-code-red-at-google-report-says/…
------
Re: Replacing Google Search with ChatGPT

If Google Search used Chat-GPT, Google would probably be bankrupt in a month or so given that there are 8.5 billion search queries a day.

Using LLMs in/for Google Search makes sense though. They already started doing that years ago.
------
I want to learn to become better at saying No and stay more focused so that I can learn more things more efficiently 
------
I know you all can't wait to fine-tune your LLMs over the holiday break! We just added a template that gets you started. 
E.g., copy and paste the code from the website here to fine-tune a 3-billion param BLOOM model on a multi-node GPU cluster: https://lightning.ai
------
How do language models of different sizes learn during the course of pre-training? 

We study the training trajectories with training checkpoints of language model from 125M to 175B for a better understanding!

Check out our new paper : http://arxiv.org/abs/2212.09803
(1/N)
------
For my deep learning projects I use both 
- notebooks (for analysis) and 
- .py scripts (for submitting/running code)

I have a template for my typical deep learning workflow in the README here: https://github.com/rasbt/machine-learning-notes/tree/main/templates/pl_classifier…
------
Same, having too many browser tabs open makes me anxious. 
That’s why I am using JupyterLab over Jupyter Notebook 
------
i don’t understand tab hoarding. i don’t think i ever have more than 15 tabs open. when i finish the day i just close them all. no regrets
------
Thanks for all the positive feedback on my new, free deep learning course!

Happy to share that Unit 3 is now live!

Math can be fun. But let's be honest, who doesn't like the convenience of automatic differentiation 

 Unit 3: https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/…

Enjoy the holidays!
------
The focus of this one is on

- computation graphs
- full-batch vs stochastic gradient descent
- automatic differentiation
- the PyTorch API

Covering all the foundations before we'll dive deeper into multilayer neural networks in Unit 4!
------
Btw. if you have any questions as you are taking the course don't hesitate to reach out, I am always excited to chat more about deep learning 
------
If you can’t decide whether ChatGPT should replace traditional web search, just have them side by side.
------
Today, youChat goes live.

Open, broadly capable, conversational AI for search with knowledge of recent events and citations of sources.

Search and chat of the future: https://you.com/search?q=what+was+the+recent+breakthrough+in+fusion+research%3F…
------
We are running out of a vital resource: words!

There are “only” 5 to 10 trillion high-quality words (papers, books, code) on the internet.  Our AI models will have used all of that for training by 2026. Low-quality data (tweets, fanfic) will last to 2040. https://arxiv.org/pdf/2211.04325.pdf…
------
High school students are going to be the most experienced prompt engineers; they will be in high demand for employment at tech companies.
------
In 10 years, the world will be very different from today.

Once upon a time, it was okay to learn sth in school & then apply it for the rest of your life. 

Today, to keep up and thrive with the rapid pace of change, it's pivotal to keep reinventing yourself and to keep learning.
------
Call for Proposals for #SciPy2023 is open! Join us in #ATX on July 10-14, 2023 for the 22nd annual SciPyConf  

Have a talk, tutorial, or poster you want to share? Get your submission in before February 22, 2023 More conference info coming soon 
------
I wasn’t suggesting the ChatGPT *should* replace Google Search. 

But some people were saying that ChatGPT wasn’t even a discussion topic for Search.

Well here you have it now  

https://cnet.com/google-amp/news/chatgpt-caused-code-red-at-google-report-says/…
------
Re: Replacing Google Search with ChatGPT

If Google Search used Chat-GPT, Google would probably be bankrupt in a month or so given that there are 8.5 billion search queries a day.

Using LLMs in/for Google Search makes sense though. They already started doing that years ago.
------
I want to learn to become better at saying No and stay more focused so that I can learn more things more efficiently 
------
I know you all can't wait to fine-tune your LLMs over the holiday break! We just added a template that gets you started. 
E.g., copy and paste the code from the website here to fine-tune a 3-billion param BLOOM model on a multi-node GPU cluster: https://lightning.ai
------
How do language models of different sizes learn during the course of pre-training? 

We study the training trajectories with training checkpoints of language model from 125M to 175B for a better understanding!

Check out our new paper : http://arxiv.org/abs/2212.09803
(1/N)
------
For my deep learning projects I use both 
- notebooks (for analysis) and 
- .py scripts (for submitting/running code)

I have a template for my typical deep learning workflow in the README here: https://github.com/rasbt/machine-learning-notes/tree/main/templates/pl_classifier…
------
Same, having too many browser tabs open makes me anxious. 
That’s why I am using JupyterLab over Jupyter Notebook 
------
i don’t understand tab hoarding. i don’t think i ever have more than 15 tabs open. when i finish the day i just close them all. no regrets
------
Thanks for all the positive feedback on my new, free deep learning course!

Happy to share that Unit 3 is now live!

Math can be fun. But let's be honest, who doesn't like the convenience of automatic differentiation 

 Unit 3: https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/…

Enjoy the holidays!
------
The focus of this one is on

- computation graphs
- full-batch vs stochastic gradient descent
- automatic differentiation
- the PyTorch API

Covering all the foundations before we'll dive deeper into multilayer neural networks in Unit 4!
------
Btw. if you have any questions as you are taking the course don't hesitate to reach out, I am always excited to chat more about deep learning 
------
If you can’t decide whether ChatGPT should replace traditional web search, just have them side by side.
------
Today, youChat goes live.

Open, broadly capable, conversational AI for search with knowledge of recent events and citations of sources.

Search and chat of the future: https://you.com/search?q=what+was+the+recent+breakthrough+in+fusion+research%3F…
------
We are running out of a vital resource: words!

There are “only” 5 to 10 trillion high-quality words (papers, books, code) on the internet.  Our AI models will have used all of that for training by 2026. Low-quality data (tweets, fanfic) will last to 2040. https://arxiv.org/pdf/2211.04325.pdf…
------
High school students are going to be the most experienced prompt engineers; they will be in high demand for employment at tech companies.
------
In 10 years, the world will be very different from today.

Once upon a time, it was okay to learn sth in school & then apply it for the rest of your life. 

Today, to keep up and thrive with the rapid pace of change, it's pivotal to keep reinventing yourself and to keep learning.
------
Call for Proposals for #SciPy2023 is open! Join us in #ATX on July 10-14, 2023 for the 22nd annual SciPyConf  

Have a talk, tutorial, or poster you want to share? Get your submission in before February 22, 2023 More conference info coming soon 
------
I wasn’t suggesting the ChatGPT *should* replace Google Search. 

But some people were saying that ChatGPT wasn’t even a discussion topic for Search.

Well here you have it now  

https://cnet.com/google-amp/news/chatgpt-caused-code-red-at-google-report-says/…
------
Re: Replacing Google Search with ChatGPT

If Google Search used Chat-GPT, Google would probably be bankrupt in a month or so given that there are 8.5 billion search queries a day.

Using LLMs in/for Google Search makes sense though. They already started doing that years ago.
------
I want to learn to become better at saying No and stay more focused so that I can learn more things more efficiently 
------
I know you all can't wait to fine-tune your LLMs over the holiday break! We just added a template that gets you started. 
E.g., copy and paste the code from the website here to fine-tune a 3-billion param BLOOM model on a multi-node GPU cluster: https://lightning.ai
------
How do language models of different sizes learn during the course of pre-training? 

We study the training trajectories with training checkpoints of language model from 125M to 175B for a better understanding!

Check out our new paper : http://arxiv.org/abs/2212.09803
(1/N)
------
For my deep learning projects I use both 
- notebooks (for analysis) and 
- .py scripts (for submitting/running code)

I have a template for my typical deep learning workflow in the README here: https://github.com/rasbt/machine-learning-notes/tree/main/templates/pl_classifier…
------
Same, having too many browser tabs open makes me anxious. 
That’s why I am using JupyterLab over Jupyter Notebook 
------
i don’t understand tab hoarding. i don’t think i ever have more than 15 tabs open. when i finish the day i just close them all. no regrets
------
Thanks for all the positive feedback on my new, free deep learning course!

Happy to share that Unit 3 is now live!

Math can be fun. But let's be honest, who doesn't like the convenience of automatic differentiation 

 Unit 3: https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/…

Enjoy the holidays!
------
The focus of this one is on

- computation graphs
- full-batch vs stochastic gradient descent
- automatic differentiation
- the PyTorch API

Covering all the foundations before we'll dive deeper into multilayer neural networks in Unit 4!
------
Btw. if you have any questions as you are taking the course don't hesitate to reach out, I am always excited to chat more about deep learning 
------
If you can’t decide whether ChatGPT should replace traditional web search, just have them side by side.
------
Today, youChat goes live.

Open, broadly capable, conversational AI for search with knowledge of recent events and citations of sources.

Search and chat of the future: https://you.com/search?q=what+was+the+recent+breakthrough+in+fusion+research%3F…
------
We are running out of a vital resource: words!

There are “only” 5 to 10 trillion high-quality words (papers, books, code) on the internet.  Our AI models will have used all of that for training by 2026. Low-quality data (tweets, fanfic) will last to 2040. https://arxiv.org/pdf/2211.04325.pdf…
------
High school students are going to be the most experienced prompt engineers; they will be in high demand for employment at tech companies.
------
In 10 years, the world will be very different from today.

Once upon a time, it was okay to learn sth in school & then apply it for the rest of your life. 

Today, to keep up and thrive with the rapid pace of change, it's pivotal to keep reinventing yourself and to keep learning.
------
Call for Proposals for #SciPy2023 is open! Join us in #ATX on July 10-14, 2023 for the 22nd annual SciPyConf  

Have a talk, tutorial, or poster you want to share? Get your submission in before February 22, 2023 More conference info coming soon 
------
I wasn’t suggesting the ChatGPT *should* replace Google Search. 

But some people were saying that ChatGPT wasn’t even a discussion topic for Search.

Well here you have it now  

https://cnet.com/google-amp/news/chatgpt-caused-code-red-at-google-report-says/…
------
Re: Replacing Google Search with ChatGPT

If Google Search used Chat-GPT, Google would probably be bankrupt in a month or so given that there are 8.5 billion search queries a day.

Using LLMs in/for Google Search makes sense though. They already started doing that years ago.
------
I want to learn to become better at saying No and stay more focused so that I can learn more things more efficiently 
------
I know you all can't wait to fine-tune your LLMs over the holiday break! We just added a template that gets you started. 
E.g., copy and paste the code from the website here to fine-tune a 3-billion param BLOOM model on a multi-node GPU cluster: https://lightning.ai
------
How do language models of different sizes learn during the course of pre-training? 

We study the training trajectories with training checkpoints of language model from 125M to 175B for a better understanding!

Check out our new paper : http://arxiv.org/abs/2212.09803
(1/N)
------
For my deep learning projects I use both 
- notebooks (for analysis) and 
- .py scripts (for submitting/running code)

I have a template for my typical deep learning workflow in the README here: https://github.com/rasbt/machine-learning-notes/tree/main/templates/pl_classifier…
------
Same, having too many browser tabs open makes me anxious. 
That’s why I am using JupyterLab over Jupyter Notebook 
------
i don’t understand tab hoarding. i don’t think i ever have more than 15 tabs open. when i finish the day i just close them all. no regrets
------
Thanks for all the positive feedback on my new, free deep learning course!

Happy to share that Unit 3 is now live!

Math can be fun. But let's be honest, who doesn't like the convenience of automatic differentiation 

 Unit 3: https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/…

Enjoy the holidays!
------
The focus of this one is on

- computation graphs
- full-batch vs stochastic gradient descent
- automatic differentiation
- the PyTorch API

Covering all the foundations before we'll dive deeper into multilayer neural networks in Unit 4!
------
Btw. if you have any questions as you are taking the course don't hesitate to reach out, I am always excited to chat more about deep learning 
------
If you can’t decide whether ChatGPT should replace traditional web search, just have them side by side.
------
Today, youChat goes live.

Open, broadly capable, conversational AI for search with knowledge of recent events and citations of sources.

Search and chat of the future: https://you.com/search?q=what+was+the+recent+breakthrough+in+fusion+research%3F…
------
We are running out of a vital resource: words!

There are “only” 5 to 10 trillion high-quality words (papers, books, code) on the internet.  Our AI models will have used all of that for training by 2026. Low-quality data (tweets, fanfic) will last to 2040. https://arxiv.org/pdf/2211.04325.pdf…
------
High school students are going to be the most experienced prompt engineers; they will be in high demand for employment at tech companies.
------
In 10 years, the world will be very different from today.

Once upon a time, it was okay to learn sth in school & then apply it for the rest of your life. 

Today, to keep up and thrive with the rapid pace of change, it's pivotal to keep reinventing yourself and to keep learning.
------
Call for Proposals for #SciPy2023 is open! Join us in #ATX on July 10-14, 2023 for the 22nd annual SciPyConf  

Have a talk, tutorial, or poster you want to share? Get your submission in before February 22, 2023 More conference info coming soon 
------
I wasn’t suggesting the ChatGPT *should* replace Google Search. 

But some people were saying that ChatGPT wasn’t even a discussion topic for Search.

Well here you have it now  

https://cnet.com/google-amp/news/chatgpt-caused-code-red-at-google-report-says/…
------
Re: Replacing Google Search with ChatGPT

If Google Search used Chat-GPT, Google would probably be bankrupt in a month or so given that there are 8.5 billion search queries a day.

Using LLMs in/for Google Search makes sense though. They already started doing that years ago.
------
I want to learn to become better at saying No and stay more focused so that I can learn more things more efficiently 
------
I know you all can't wait to fine-tune your LLMs over the holiday break! We just added a template that gets you started. 
E.g., copy and paste the code from the website here to fine-tune a 3-billion param BLOOM model on a multi-node GPU cluster: https://lightning.ai
------
How do language models of different sizes learn during the course of pre-training? 

We study the training trajectories with training checkpoints of language model from 125M to 175B for a better understanding!

Check out our new paper : http://arxiv.org/abs/2212.09803
(1/N)
------
For my deep learning projects I use both 
- notebooks (for analysis) and 
- .py scripts (for submitting/running code)

I have a template for my typical deep learning workflow in the README here: https://github.com/rasbt/machine-learning-notes/tree/main/templates/pl_classifier…
------
Same, having too many browser tabs open makes me anxious. 
That’s why I am using JupyterLab over Jupyter Notebook 
------
i don’t understand tab hoarding. i don’t think i ever have more than 15 tabs open. when i finish the day i just close them all. no regrets
------
Thanks for all the positive feedback on my new, free deep learning course!

Happy to share that Unit 3 is now live!

Math can be fun. But let's be honest, who doesn't like the convenience of automatic differentiation 

 Unit 3: https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/…

Enjoy the holidays!
------
The focus of this one is on

- computation graphs
- full-batch vs stochastic gradient descent
- automatic differentiation
- the PyTorch API

Covering all the foundations before we'll dive deeper into multilayer neural networks in Unit 4!
------
Btw. if you have any questions as you are taking the course don't hesitate to reach out, I am always excited to chat more about deep learning 
------
If you can’t decide whether ChatGPT should replace traditional web search, just have them side by side.
------
Today, youChat goes live.

Open, broadly capable, conversational AI for search with knowledge of recent events and citations of sources.

Search and chat of the future: https://you.com/search?q=what+was+the+recent+breakthrough+in+fusion+research%3F…
------
We are running out of a vital resource: words!

There are “only” 5 to 10 trillion high-quality words (papers, books, code) on the internet.  Our AI models will have used all of that for training by 2026. Low-quality data (tweets, fanfic) will last to 2040. https://arxiv.org/pdf/2211.04325.pdf…
------
High school students are going to be the most experienced prompt engineers; they will be in high demand for employment at tech companies.
------
In 10 years, the world will be very different from today.

Once upon a time, it was okay to learn sth in school & then apply it for the rest of your life. 

Today, to keep up and thrive with the rapid pace of change, it's pivotal to keep reinventing yourself and to keep learning.
------
Call for Proposals for #SciPy2023 is open! Join us in #ATX on July 10-14, 2023 for the 22nd annual SciPyConf  

Have a talk, tutorial, or poster you want to share? Get your submission in before February 22, 2023 More conference info coming soon 
------
I wasn’t suggesting the ChatGPT *should* replace Google Search. 

But some people were saying that ChatGPT wasn’t even a discussion topic for Search.

Well here you have it now  

https://cnet.com/google-amp/news/chatgpt-caused-code-red-at-google-report-says/…
------
Re: Replacing Google Search with ChatGPT

If Google Search used Chat-GPT, Google would probably be bankrupt in a month or so given that there are 8.5 billion search queries a day.

Using LLMs in/for Google Search makes sense though. They already started doing that years ago.
------
I want to learn to become better at saying No and stay more focused so that I can learn more things more efficiently 
------
I know you all can't wait to fine-tune your LLMs over the holiday break! We just added a template that gets you started. 
E.g., copy and paste the code from the website here to fine-tune a 3-billion param BLOOM model on a multi-node GPU cluster: https://lightning.ai
------
How do language models of different sizes learn during the course of pre-training? 

We study the training trajectories with training checkpoints of language model from 125M to 175B for a better understanding!

Check out our new paper : http://arxiv.org/abs/2212.09803
(1/N)
------
For my deep learning projects I use both 
- notebooks (for analysis) and 
- .py scripts (for submitting/running code)

I have a template for my typical deep learning workflow in the README here: https://github.com/rasbt/machine-learning-notes/tree/main/templates/pl_classifier…
------
Same, having too many browser tabs open makes me anxious. 
That’s why I am using JupyterLab over Jupyter Notebook 
------
i don’t understand tab hoarding. i don’t think i ever have more than 15 tabs open. when i finish the day i just close them all. no regrets
------
Thanks for all the positive feedback on my new, free deep learning course!

Happy to share that Unit 3 is now live!

Math can be fun. But let's be honest, who doesn't like the convenience of automatic differentiation 

 Unit 3: https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/…

Enjoy the holidays!
------
The focus of this one is on

- computation graphs
- full-batch vs stochastic gradient descent
- automatic differentiation
- the PyTorch API

Covering all the foundations before we'll dive deeper into multilayer neural networks in Unit 4!
------
Btw. if you have any questions as you are taking the course don't hesitate to reach out, I am always excited to chat more about deep learning 
------
If you can’t decide whether ChatGPT should replace traditional web search, just have them side by side.
------
Today, youChat goes live.

Open, broadly capable, conversational AI for search with knowledge of recent events and citations of sources.

Search and chat of the future: https://you.com/search?q=what+was+the+recent+breakthrough+in+fusion+research%3F…
------
We are running out of a vital resource: words!

There are “only” 5 to 10 trillion high-quality words (papers, books, code) on the internet.  Our AI models will have used all of that for training by 2026. Low-quality data (tweets, fanfic) will last to 2040. https://arxiv.org/pdf/2211.04325.pdf…
------
High school students are going to be the most experienced prompt engineers; they will be in high demand for employment at tech companies.
------
In 10 years, the world will be very different from today.

Once upon a time, it was okay to learn sth in school & then apply it for the rest of your life. 

Today, to keep up and thrive with the rapid pace of change, it's pivotal to keep reinventing yourself and to keep learning.
------
Call for Proposals for #SciPy2023 is open! Join us in #ATX on July 10-14, 2023 for the 22nd annual SciPyConf  

Have a talk, tutorial, or poster you want to share? Get your submission in before February 22, 2023 More conference info coming soon 
------
I wasn’t suggesting the ChatGPT *should* replace Google Search. 

But some people were saying that ChatGPT wasn’t even a discussion topic for Search.

Well here you have it now  

https://cnet.com/google-amp/news/chatgpt-caused-code-red-at-google-report-says/…
------
Re: Replacing Google Search with ChatGPT

If Google Search used Chat-GPT, Google would probably be bankrupt in a month or so given that there are 8.5 billion search queries a day.

Using LLMs in/for Google Search makes sense though. They already started doing that years ago.
------
I want to learn to become better at saying No and stay more focused so that I can learn more things more efficiently 
------
I know you all can't wait to fine-tune your LLMs over the holiday break! We just added a template that gets you started. 
E.g., copy and paste the code from the website here to fine-tune a 3-billion param BLOOM model on a multi-node GPU cluster: https://lightning.ai
------
How do language models of different sizes learn during the course of pre-training? 

We study the training trajectories with training checkpoints of language model from 125M to 175B for a better understanding!

Check out our new paper : http://arxiv.org/abs/2212.09803
(1/N)
------
For my deep learning projects I use both 
- notebooks (for analysis) and 
- .py scripts (for submitting/running code)

I have a template for my typical deep learning workflow in the README here: https://github.com/rasbt/machine-learning-notes/tree/main/templates/pl_classifier…
------
Same, having too many browser tabs open makes me anxious. 
That’s why I am using JupyterLab over Jupyter Notebook 
------
i don’t understand tab hoarding. i don’t think i ever have more than 15 tabs open. when i finish the day i just close them all. no regrets
------
Thanks for all the positive feedback on my new, free deep learning course!

Happy to share that Unit 3 is now live!

Math can be fun. But let's be honest, who doesn't like the convenience of automatic differentiation 

 Unit 3: https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/…

Enjoy the holidays!
------
The focus of this one is on

- computation graphs
- full-batch vs stochastic gradient descent
- automatic differentiation
- the PyTorch API

Covering all the foundations before we'll dive deeper into multilayer neural networks in Unit 4!
------
Btw. if you have any questions as you are taking the course don't hesitate to reach out, I am always excited to chat more about deep learning 
------
If you can’t decide whether ChatGPT should replace traditional web search, just have them side by side.
------
Today, youChat goes live.

Open, broadly capable, conversational AI for search with knowledge of recent events and citations of sources.

Search and chat of the future: https://you.com/search?q=what+was+the+recent+breakthrough+in+fusion+research%3F…
------
We are running out of a vital resource: words!

There are “only” 5 to 10 trillion high-quality words (papers, books, code) on the internet.  Our AI models will have used all of that for training by 2026. Low-quality data (tweets, fanfic) will last to 2040. https://arxiv.org/pdf/2211.04325.pdf…
------
High school students are going to be the most experienced prompt engineers; they will be in high demand for employment at tech companies.
------
In 10 years, the world will be very different from today.

Once upon a time, it was okay to learn sth in school & then apply it for the rest of your life. 

Today, to keep up and thrive with the rapid pace of change, it's pivotal to keep reinventing yourself and to keep learning.
------
Call for Proposals for #SciPy2023 is open! Join us in #ATX on July 10-14, 2023 for the 22nd annual SciPyConf  

Have a talk, tutorial, or poster you want to share? Get your submission in before February 22, 2023 More conference info coming soon 
------
I wasn’t suggesting the ChatGPT *should* replace Google Search. 

But some people were saying that ChatGPT wasn’t even a discussion topic for Search.

Well here you have it now  

https://cnet.com/google-amp/news/chatgpt-caused-code-red-at-google-report-says/…
------
Re: Replacing Google Search with ChatGPT

If Google Search used Chat-GPT, Google would probably be bankrupt in a month or so given that there are 8.5 billion search queries a day.

Using LLMs in/for Google Search makes sense though. They already started doing that years ago.
------
I want to learn to become better at saying No and stay more focused so that I can learn more things more efficiently 
------
I know you all can't wait to fine-tune your LLMs over the holiday break! We just added a template that gets you started. 
E.g., copy and paste the code from the website here to fine-tune a 3-billion param BLOOM model on a multi-node GPU cluster: https://lightning.ai
------
How do language models of different sizes learn during the course of pre-training? 

We study the training trajectories with training checkpoints of language model from 125M to 175B for a better understanding!

Check out our new paper : http://arxiv.org/abs/2212.09803
(1/N)
------
For my deep learning projects I use both 
- notebooks (for analysis) and 
- .py scripts (for submitting/running code)

I have a template for my typical deep learning workflow in the README here: https://github.com/rasbt/machine-learning-notes/tree/main/templates/pl_classifier…
------
Same, having too many browser tabs open makes me anxious. 
That’s why I am using JupyterLab over Jupyter Notebook 
------
i don’t understand tab hoarding. i don’t think i ever have more than 15 tabs open. when i finish the day i just close them all. no regrets
------
Thanks for all the positive feedback on my new, free deep learning course!

Happy to share that Unit 3 is now live!

Math can be fun. But let's be honest, who doesn't like the convenience of automatic differentiation 

 Unit 3: https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/…

Enjoy the holidays!
------
The focus of this one is on

- computation graphs
- full-batch vs stochastic gradient descent
- automatic differentiation
- the PyTorch API

Covering all the foundations before we'll dive deeper into multilayer neural networks in Unit 4!
------
Btw. if you have any questions as you are taking the course don't hesitate to reach out, I am always excited to chat more about deep learning 
------
If you can’t decide whether ChatGPT should replace traditional web search, just have them side by side.
------
Today, youChat goes live.

Open, broadly capable, conversational AI for search with knowledge of recent events and citations of sources.

Search and chat of the future: https://you.com/search?q=what+was+the+recent+breakthrough+in+fusion+research%3F…
------
We are running out of a vital resource: words!

There are “only” 5 to 10 trillion high-quality words (papers, books, code) on the internet.  Our AI models will have used all of that for training by 2026. Low-quality data (tweets, fanfic) will last to 2040. https://arxiv.org/pdf/2211.04325.pdf…
------
High school students are going to be the most experienced prompt engineers; they will be in high demand for employment at tech companies.
------
In 10 years, the world will be very different from today.

Once upon a time, it was okay to learn sth in school & then apply it for the rest of your life. 

Today, to keep up and thrive with the rapid pace of change, it's pivotal to keep reinventing yourself and to keep learning.
------
Call for Proposals for #SciPy2023 is open! Join us in #ATX on July 10-14, 2023 for the 22nd annual SciPyConf  

Have a talk, tutorial, or poster you want to share? Get your submission in before February 22, 2023 More conference info coming soon 
------
I wasn’t suggesting the ChatGPT *should* replace Google Search. 

But some people were saying that ChatGPT wasn’t even a discussion topic for Search.

Well here you have it now  

https://cnet.com/google-amp/news/chatgpt-caused-code-red-at-google-report-says/…
------
Re: Replacing Google Search with ChatGPT

If Google Search used Chat-GPT, Google would probably be bankrupt in a month or so given that there are 8.5 billion search queries a day.

Using LLMs in/for Google Search makes sense though. They already started doing that years ago.
------
I want to learn to become better at saying No and stay more focused so that I can learn more things more efficiently 
------
I know you all can't wait to fine-tune your LLMs over the holiday break! We just added a template that gets you started. 
E.g., copy and paste the code from the website here to fine-tune a 3-billion param BLOOM model on a multi-node GPU cluster: https://lightning.ai
------
How do language models of different sizes learn during the course of pre-training? 

We study the training trajectories with training checkpoints of language model from 125M to 175B for a better understanding!

Check out our new paper : http://arxiv.org/abs/2212.09803
(1/N)
------
For my deep learning projects I use both 
- notebooks (for analysis) and 
- .py scripts (for submitting/running code)

I have a template for my typical deep learning workflow in the README here: https://github.com/rasbt/machine-learning-notes/tree/main/templates/pl_classifier…
------
Same, having too many browser tabs open makes me anxious. 
That’s why I am using JupyterLab over Jupyter Notebook 
------
i don’t understand tab hoarding. i don’t think i ever have more than 15 tabs open. when i finish the day i just close them all. no regrets
------
Thanks for all the positive feedback on my new, free deep learning course!

Happy to share that Unit 3 is now live!

Math can be fun. But let's be honest, who doesn't like the convenience of automatic differentiation 

 Unit 3: https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/…

Enjoy the holidays!
------
The focus of this one is on

- computation graphs
- full-batch vs stochastic gradient descent
- automatic differentiation
- the PyTorch API

Covering all the foundations before we'll dive deeper into multilayer neural networks in Unit 4!
------
Btw. if you have any questions as you are taking the course don't hesitate to reach out, I am always excited to chat more about deep learning 
------
If you can’t decide whether ChatGPT should replace traditional web search, just have them side by side.
------
Today, youChat goes live.

Open, broadly capable, conversational AI for search with knowledge of recent events and citations of sources.

Search and chat of the future: https://you.com/search?q=what+was+the+recent+breakthrough+in+fusion+research%3F…
------
We are running out of a vital resource: words!

There are “only” 5 to 10 trillion high-quality words (papers, books, code) on the internet.  Our AI models will have used all of that for training by 2026. Low-quality data (tweets, fanfic) will last to 2040. https://arxiv.org/pdf/2211.04325.pdf…
------
High school students are going to be the most experienced prompt engineers; they will be in high demand for employment at tech companies.
------
In 10 years, the world will be very different from today.

Once upon a time, it was okay to learn sth in school & then apply it for the rest of your life. 

Today, to keep up and thrive with the rapid pace of change, it's pivotal to keep reinventing yourself and to keep learning.
------
Call for Proposals for #SciPy2023 is open! Join us in #ATX on July 10-14, 2023 for the 22nd annual SciPyConf  

Have a talk, tutorial, or poster you want to share? Get your submission in before February 22, 2023 More conference info coming soon 
------
I wasn’t suggesting the ChatGPT *should* replace Google Search. 

But some people were saying that ChatGPT wasn’t even a discussion topic for Search.

Well here you have it now  

https://cnet.com/google-amp/news/chatgpt-caused-code-red-at-google-report-says/…
------
Re: Replacing Google Search with ChatGPT

If Google Search used Chat-GPT, Google would probably be bankrupt in a month or so given that there are 8.5 billion search queries a day.

Using LLMs in/for Google Search makes sense though. They already started doing that years ago.
------
I want to learn to become better at saying No and stay more focused so that I can learn more things more efficiently 
------
I know you all can't wait to fine-tune your LLMs over the holiday break! We just added a template that gets you started. 
E.g., copy and paste the code from the website here to fine-tune a 3-billion param BLOOM model on a multi-node GPU cluster: https://lightning.ai
------
How do language models of different sizes learn during the course of pre-training? 

We study the training trajectories with training checkpoints of language model from 125M to 175B for a better understanding!

Check out our new paper : http://arxiv.org/abs/2212.09803
(1/N)
------
For my deep learning projects I use both 
- notebooks (for analysis) and 
- .py scripts (for submitting/running code)

I have a template for my typical deep learning workflow in the README here: https://github.com/rasbt/machine-learning-notes/tree/main/templates/pl_classifier…
------
Same, having too many browser tabs open makes me anxious. 
That’s why I am using JupyterLab over Jupyter Notebook 
------
i don’t understand tab hoarding. i don’t think i ever have more than 15 tabs open. when i finish the day i just close them all. no regrets
------
Thanks for all the positive feedback on my new, free deep learning course!

Happy to share that Unit 3 is now live!

Math can be fun. But let's be honest, who doesn't like the convenience of automatic differentiation 

 Unit 3: https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/…

Enjoy the holidays!
------
The focus of this one is on

- computation graphs
- full-batch vs stochastic gradient descent
- automatic differentiation
- the PyTorch API

Covering all the foundations before we'll dive deeper into multilayer neural networks in Unit 4!
------
Btw. if you have any questions as you are taking the course don't hesitate to reach out, I am always excited to chat more about deep learning 
------
If you can’t decide whether ChatGPT should replace traditional web search, just have them side by side.
------
Today, youChat goes live.

Open, broadly capable, conversational AI for search with knowledge of recent events and citations of sources.

Search and chat of the future: https://you.com/search?q=what+was+the+recent+breakthrough+in+fusion+research%3F…
------
We are running out of a vital resource: words!

There are “only” 5 to 10 trillion high-quality words (papers, books, code) on the internet.  Our AI models will have used all of that for training by 2026. Low-quality data (tweets, fanfic) will last to 2040. https://arxiv.org/pdf/2211.04325.pdf…
------
High school students are going to be the most experienced prompt engineers; they will be in high demand for employment at tech companies.
------
In 10 years, the world will be very different from today.

Once upon a time, it was okay to learn sth in school & then apply it for the rest of your life. 

Today, to keep up and thrive with the rapid pace of change, it's pivotal to keep reinventing yourself and to keep learning.
------
Call for Proposals for #SciPy2023 is open! Join us in #ATX on July 10-14, 2023 for the 22nd annual SciPyConf  

Have a talk, tutorial, or poster you want to share? Get your submission in before February 22, 2023 More conference info coming soon 
------
I wasn’t suggesting the ChatGPT *should* replace Google Search. 

But some people were saying that ChatGPT wasn’t even a discussion topic for Search.

Well here you have it now  

https://cnet.com/google-amp/news/chatgpt-caused-code-red-at-google-report-says/…
------
Re: Replacing Google Search with ChatGPT

If Google Search used Chat-GPT, Google would probably be bankrupt in a month or so given that there are 8.5 billion search queries a day.

Using LLMs in/for Google Search makes sense though. They already started doing that years ago.
------
I want to learn to become better at saying No and stay more focused so that I can learn more things more efficiently 
------
I know you all can't wait to fine-tune your LLMs over the holiday break! We just added a template that gets you started. 
E.g., copy and paste the code from the website here to fine-tune a 3-billion param BLOOM model on a multi-node GPU cluster: https://lightning.ai
------
How do language models of different sizes learn during the course of pre-training? 

We study the training trajectories with training checkpoints of language model from 125M to 175B for a better understanding!

Check out our new paper : http://arxiv.org/abs/2212.09803
(1/N)
------
For my deep learning projects I use both 
- notebooks (for analysis) and 
- .py scripts (for submitting/running code)

I have a template for my typical deep learning workflow in the README here: https://github.com/rasbt/machine-learning-notes/tree/main/templates/pl_classifier…
------
Same, having too many browser tabs open makes me anxious. 
That’s why I am using JupyterLab over Jupyter Notebook 
------
i don’t understand tab hoarding. i don’t think i ever have more than 15 tabs open. when i finish the day i just close them all. no regrets
------
Thanks for all the positive feedback on my new, free deep learning course!

Happy to share that Unit 3 is now live!

Math can be fun. But let's be honest, who doesn't like the convenience of automatic differentiation 

 Unit 3: https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/…

Enjoy the holidays!
------
The focus of this one is on

- computation graphs
- full-batch vs stochastic gradient descent
- automatic differentiation
- the PyTorch API

Covering all the foundations before we'll dive deeper into multilayer neural networks in Unit 4!
------
Btw. if you have any questions as you are taking the course don't hesitate to reach out, I am always excited to chat more about deep learning 
------
If you can’t decide whether ChatGPT should replace traditional web search, just have them side by side.
------
Today, youChat goes live.

Open, broadly capable, conversational AI for search with knowledge of recent events and citations of sources.

Search and chat of the future: https://you.com/search?q=what+was+the+recent+breakthrough+in+fusion+research%3F…
------
We are running out of a vital resource: words!

There are “only” 5 to 10 trillion high-quality words (papers, books, code) on the internet.  Our AI models will have used all of that for training by 2026. Low-quality data (tweets, fanfic) will last to 2040. https://arxiv.org/pdf/2211.04325.pdf…
------
High school students are going to be the most experienced prompt engineers; they will be in high demand for employment at tech companies.
------
In 10 years, the world will be very different from today.

Once upon a time, it was okay to learn sth in school & then apply it for the rest of your life. 

Today, to keep up and thrive with the rapid pace of change, it's pivotal to keep reinventing yourself and to keep learning.
------
Call for Proposals for #SciPy2023 is open! Join us in #ATX on July 10-14, 2023 for the 22nd annual SciPyConf  

Have a talk, tutorial, or poster you want to share? Get your submission in before February 22, 2023 More conference info coming soon 
------
I wasn’t suggesting the ChatGPT *should* replace Google Search. 

But some people were saying that ChatGPT wasn’t even a discussion topic for Search.

Well here you have it now  

https://cnet.com/google-amp/news/chatgpt-caused-code-red-at-google-report-says/…
------
Re: Replacing Google Search with ChatGPT

If Google Search used Chat-GPT, Google would probably be bankrupt in a month or so given that there are 8.5 billion search queries a day.

Using LLMs in/for Google Search makes sense though. They already started doing that years ago.
------
I want to learn to become better at saying No and stay more focused so that I can learn more things more efficiently 
------
I know you all can't wait to fine-tune your LLMs over the holiday break! We just added a template that gets you started. 
E.g., copy and paste the code from the website here to fine-tune a 3-billion param BLOOM model on a multi-node GPU cluster: https://lightning.ai
------
How do language models of different sizes learn during the course of pre-training? 

We study the training trajectories with training checkpoints of language model from 125M to 175B for a better understanding!

Check out our new paper : http://arxiv.org/abs/2212.09803
(1/N)
------
For my deep learning projects I use both 
- notebooks (for analysis) and 
- .py scripts (for submitting/running code)

I have a template for my typical deep learning workflow in the README here: https://github.com/rasbt/machine-learning-notes/tree/main/templates/pl_classifier…
------
Same, having too many browser tabs open makes me anxious. 
That’s why I am using JupyterLab over Jupyter Notebook 
------
i don’t understand tab hoarding. i don’t think i ever have more than 15 tabs open. when i finish the day i just close them all. no regrets
------
Thanks for all the positive feedback on my new, free deep learning course!

Happy to share that Unit 3 is now live!

Math can be fun. But let's be honest, who doesn't like the convenience of automatic differentiation 

 Unit 3: https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/…

Enjoy the holidays!
------
The focus of this one is on

- computation graphs
- full-batch vs stochastic gradient descent
- automatic differentiation
- the PyTorch API

Covering all the foundations before we'll dive deeper into multilayer neural networks in Unit 4!
------
Btw. if you have any questions as you are taking the course don't hesitate to reach out, I am always excited to chat more about deep learning 
------
If you can’t decide whether ChatGPT should replace traditional web search, just have them side by side.
------
Today, youChat goes live.

Open, broadly capable, conversational AI for search with knowledge of recent events and citations of sources.

Search and chat of the future: https://you.com/search?q=what+was+the+recent+breakthrough+in+fusion+research%3F…
------
We are running out of a vital resource: words!

There are “only” 5 to 10 trillion high-quality words (papers, books, code) on the internet.  Our AI models will have used all of that for training by 2026. Low-quality data (tweets, fanfic) will last to 2040. https://arxiv.org/pdf/2211.04325.pdf…
------
High school students are going to be the most experienced prompt engineers; they will be in high demand for employment at tech companies.
------
In 10 years, the world will be very different from today.

Once upon a time, it was okay to learn sth in school & then apply it for the rest of your life. 

Today, to keep up and thrive with the rapid pace of change, it's pivotal to keep reinventing yourself and to keep learning.
------
Call for Proposals for #SciPy2023 is open! Join us in #ATX on July 10-14, 2023 for the 22nd annual SciPyConf  

Have a talk, tutorial, or poster you want to share? Get your submission in before February 22, 2023 More conference info coming soon 
------
I wasn’t suggesting the ChatGPT *should* replace Google Search. 

But some people were saying that ChatGPT wasn’t even a discussion topic for Search.

Well here you have it now  

https://cnet.com/google-amp/news/chatgpt-caused-code-red-at-google-report-says/…
------
Re: Replacing Google Search with ChatGPT

If Google Search used Chat-GPT, Google would probably be bankrupt in a month or so given that there are 8.5 billion search queries a day.

Using LLMs in/for Google Search makes sense though. They already started doing that years ago.
------
I want to learn to become better at saying No and stay more focused so that I can learn more things more efficiently 
------
I know you all can't wait to fine-tune your LLMs over the holiday break! We just added a template that gets you started. 
E.g., copy and paste the code from the website here to fine-tune a 3-billion param BLOOM model on a multi-node GPU cluster: https://lightning.ai
------
How do language models of different sizes learn during the course of pre-training? 

We study the training trajectories with training checkpoints of language model from 125M to 175B for a better understanding!

Check out our new paper : http://arxiv.org/abs/2212.09803
(1/N)
------
For my deep learning projects I use both 
- notebooks (for analysis) and 
- .py scripts (for submitting/running code)

I have a template for my typical deep learning workflow in the README here: https://github.com/rasbt/machine-learning-notes/tree/main/templates/pl_classifier…
------
Same, having too many browser tabs open makes me anxious. 
That’s why I am using JupyterLab over Jupyter Notebook 
------
i don’t understand tab hoarding. i don’t think i ever have more than 15 tabs open. when i finish the day i just close them all. no regrets
------
Thanks for all the positive feedback on my new, free deep learning course!

Happy to share that Unit 3 is now live!

Math can be fun. But let's be honest, who doesn't like the convenience of automatic differentiation 

 Unit 3: https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/…

Enjoy the holidays!
------
The focus of this one is on

- computation graphs
- full-batch vs stochastic gradient descent
- automatic differentiation
- the PyTorch API

Covering all the foundations before we'll dive deeper into multilayer neural networks in Unit 4!
------
Btw. if you have any questions as you are taking the course don't hesitate to reach out, I am always excited to chat more about deep learning 
------
If you can’t decide whether ChatGPT should replace traditional web search, just have them side by side.
------
Today, youChat goes live.

Open, broadly capable, conversational AI for search with knowledge of recent events and citations of sources.

Search and chat of the future: https://you.com/search?q=what+was+the+recent+breakthrough+in+fusion+research%3F…
------
We are running out of a vital resource: words!

There are “only” 5 to 10 trillion high-quality words (papers, books, code) on the internet.  Our AI models will have used all of that for training by 2026. Low-quality data (tweets, fanfic) will last to 2040. https://arxiv.org/pdf/2211.04325.pdf…
------
High school students are going to be the most experienced prompt engineers; they will be in high demand for employment at tech companies.
------
In 10 years, the world will be very different from today.

Once upon a time, it was okay to learn sth in school & then apply it for the rest of your life. 

Today, to keep up and thrive with the rapid pace of change, it's pivotal to keep reinventing yourself and to keep learning.
------
Call for Proposals for #SciPy2023 is open! Join us in #ATX on July 10-14, 2023 for the 22nd annual SciPyConf  

Have a talk, tutorial, or poster you want to share? Get your submission in before February 22, 2023 More conference info coming soon 
------
I wasn’t suggesting the ChatGPT *should* replace Google Search. 

But some people were saying that ChatGPT wasn’t even a discussion topic for Search.

Well here you have it now  

https://cnet.com/google-amp/news/chatgpt-caused-code-red-at-google-report-says/…
------
Re: Replacing Google Search with ChatGPT

If Google Search used Chat-GPT, Google would probably be bankrupt in a month or so given that there are 8.5 billion search queries a day.

Using LLMs in/for Google Search makes sense though. They already started doing that years ago.
------
I want to learn to become better at saying No and stay more focused so that I can learn more things more efficiently 
------
I know you all can't wait to fine-tune your LLMs over the holiday break! We just added a template that gets you started. 
E.g., copy and paste the code from the website here to fine-tune a 3-billion param BLOOM model on a multi-node GPU cluster: https://lightning.ai
------
How do language models of different sizes learn during the course of pre-training? 

We study the training trajectories with training checkpoints of language model from 125M to 175B for a better understanding!

Check out our new paper : http://arxiv.org/abs/2212.09803
(1/N)
------
For my deep learning projects I use both 
- notebooks (for analysis) and 
- .py scripts (for submitting/running code)

I have a template for my typical deep learning workflow in the README here: https://github.com/rasbt/machine-learning-notes/tree/main/templates/pl_classifier…
------
Same, having too many browser tabs open makes me anxious. 
That’s why I am using JupyterLab over Jupyter Notebook 
------
i don’t understand tab hoarding. i don’t think i ever have more than 15 tabs open. when i finish the day i just close them all. no regrets
------
Thanks for all the positive feedback on my new, free deep learning course!

Happy to share that Unit 3 is now live!

Math can be fun. But let's be honest, who doesn't like the convenience of automatic differentiation 

 Unit 3: https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/…

Enjoy the holidays!
------
The focus of this one is on

- computation graphs
- full-batch vs stochastic gradient descent
- automatic differentiation
- the PyTorch API

Covering all the foundations before we'll dive deeper into multilayer neural networks in Unit 4!
------
Btw. if you have any questions as you are taking the course don't hesitate to reach out, I am always excited to chat more about deep learning 
------
If you can’t decide whether ChatGPT should replace traditional web search, just have them side by side.
------
Today, youChat goes live.

Open, broadly capable, conversational AI for search with knowledge of recent events and citations of sources.

Search and chat of the future: https://you.com/search?q=what+was+the+recent+breakthrough+in+fusion+research%3F…
------
We are running out of a vital resource: words!

There are “only” 5 to 10 trillion high-quality words (papers, books, code) on the internet.  Our AI models will have used all of that for training by 2026. Low-quality data (tweets, fanfic) will last to 2040. https://arxiv.org/pdf/2211.04325.pdf…
------
High school students are going to be the most experienced prompt engineers; they will be in high demand for employment at tech companies.
------
In 10 years, the world will be very different from today.

Once upon a time, it was okay to learn sth in school & then apply it for the rest of your life. 

Today, to keep up and thrive with the rapid pace of change, it's pivotal to keep reinventing yourself and to keep learning.
------
Call for Proposals for #SciPy2023 is open! Join us in #ATX on July 10-14, 2023 for the 22nd annual SciPyConf  

Have a talk, tutorial, or poster you want to share? Get your submission in before February 22, 2023 More conference info coming soon 
------
I wasn’t suggesting the ChatGPT *should* replace Google Search. 

But some people were saying that ChatGPT wasn’t even a discussion topic for Search.

Well here you have it now  

https://cnet.com/google-amp/news/chatgpt-caused-code-red-at-google-report-says/…
------
Re: Replacing Google Search with ChatGPT

If Google Search used Chat-GPT, Google would probably be bankrupt in a month or so given that there are 8.5 billion search queries a day.

Using LLMs in/for Google Search makes sense though. They already started doing that years ago.
------
I want to learn to become better at saying No and stay more focused so that I can learn more things more efficiently 
------
I know you all can't wait to fine-tune your LLMs over the holiday break! We just added a template that gets you started. 
E.g., copy and paste the code from the website here to fine-tune a 3-billion param BLOOM model on a multi-node GPU cluster: https://lightning.ai
------
How do language models of different sizes learn during the course of pre-training? 

We study the training trajectories with training checkpoints of language model from 125M to 175B for a better understanding!

Check out our new paper : http://arxiv.org/abs/2212.09803
(1/N)
------
For my deep learning projects I use both 
- notebooks (for analysis) and 
- .py scripts (for submitting/running code)

I have a template for my typical deep learning workflow in the README here: https://github.com/rasbt/machine-learning-notes/tree/main/templates/pl_classifier…
------
Same, having too many browser tabs open makes me anxious. 
That’s why I am using JupyterLab over Jupyter Notebook 
------
i don’t understand tab hoarding. i don’t think i ever have more than 15 tabs open. when i finish the day i just close them all. no regrets
------
Thanks for all the positive feedback on my new, free deep learning course!

Happy to share that Unit 3 is now live!

Math can be fun. But let's be honest, who doesn't like the convenience of automatic differentiation 

 Unit 3: https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/…

Enjoy the holidays!
------
The focus of this one is on

- computation graphs
- full-batch vs stochastic gradient descent
- automatic differentiation
- the PyTorch API

Covering all the foundations before we'll dive deeper into multilayer neural networks in Unit 4!
------
Btw. if you have any questions as you are taking the course don't hesitate to reach out, I am always excited to chat more about deep learning 
------
If you can’t decide whether ChatGPT should replace traditional web search, just have them side by side.
------
Today, youChat goes live.

Open, broadly capable, conversational AI for search with knowledge of recent events and citations of sources.

Search and chat of the future: https://you.com/search?q=what+was+the+recent+breakthrough+in+fusion+research%3F…
------
We are running out of a vital resource: words!

There are “only” 5 to 10 trillion high-quality words (papers, books, code) on the internet.  Our AI models will have used all of that for training by 2026. Low-quality data (tweets, fanfic) will last to 2040. https://arxiv.org/pdf/2211.04325.pdf…
------
High school students are going to be the most experienced prompt engineers; they will be in high demand for employment at tech companies.
------
In 10 years, the world will be very different from today.

Once upon a time, it was okay to learn sth in school & then apply it for the rest of your life. 

Today, to keep up and thrive with the rapid pace of change, it's pivotal to keep reinventing yourself and to keep learning.
------
Call for Proposals for #SciPy2023 is open! Join us in #ATX on July 10-14, 2023 for the 22nd annual SciPyConf  

Have a talk, tutorial, or poster you want to share? Get your submission in before February 22, 2023 More conference info coming soon 
------
I wasn’t suggesting the ChatGPT *should* replace Google Search. 

But some people were saying that ChatGPT wasn’t even a discussion topic for Search.

Well here you have it now  

https://cnet.com/google-amp/news/chatgpt-caused-code-red-at-google-report-says/…
------
Re: Replacing Google Search with ChatGPT

If Google Search used Chat-GPT, Google would probably be bankrupt in a month or so given that there are 8.5 billion search queries a day.

Using LLMs in/for Google Search makes sense though. They already started doing that years ago.
------
I want to learn to become better at saying No and stay more focused so that I can learn more things more efficiently 
------
I know you all can't wait to fine-tune your LLMs over the holiday break! We just added a template that gets you started. 
E.g., copy and paste the code from the website here to fine-tune a 3-billion param BLOOM model on a multi-node GPU cluster: https://lightning.ai
------
How do language models of different sizes learn during the course of pre-training? 

We study the training trajectories with training checkpoints of language model from 125M to 175B for a better understanding!

Check out our new paper : http://arxiv.org/abs/2212.09803
(1/N)
------
For my deep learning projects I use both 
- notebooks (for analysis) and 
- .py scripts (for submitting/running code)

I have a template for my typical deep learning workflow in the README here: https://github.com/rasbt/machine-learning-notes/tree/main/templates/pl_classifier…
------
Same, having too many browser tabs open makes me anxious. 
That’s why I am using JupyterLab over Jupyter Notebook 
------
i don’t understand tab hoarding. i don’t think i ever have more than 15 tabs open. when i finish the day i just close them all. no regrets
------
Thanks for all the positive feedback on my new, free deep learning course!

Happy to share that Unit 3 is now live!

Math can be fun. But let's be honest, who doesn't like the convenience of automatic differentiation 

 Unit 3: https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/…

Enjoy the holidays!
------
The focus of this one is on

- computation graphs
- full-batch vs stochastic gradient descent
- automatic differentiation
- the PyTorch API

Covering all the foundations before we'll dive deeper into multilayer neural networks in Unit 4!
------
Btw. if you have any questions as you are taking the course don't hesitate to reach out, I am always excited to chat more about deep learning 
------
If you can’t decide whether ChatGPT should replace traditional web search, just have them side by side.
------
Today, youChat goes live.

Open, broadly capable, conversational AI for search with knowledge of recent events and citations of sources.

Search and chat of the future: https://you.com/search?q=what+was+the+recent+breakthrough+in+fusion+research%3F…
------
We are running out of a vital resource: words!

There are “only” 5 to 10 trillion high-quality words (papers, books, code) on the internet.  Our AI models will have used all of that for training by 2026. Low-quality data (tweets, fanfic) will last to 2040. https://arxiv.org/pdf/2211.04325.pdf…
------
High school students are going to be the most experienced prompt engineers; they will be in high demand for employment at tech companies.
------
In 10 years, the world will be very different from today.

Once upon a time, it was okay to learn sth in school & then apply it for the rest of your life. 

Today, to keep up and thrive with the rapid pace of change, it's pivotal to keep reinventing yourself and to keep learning.
------
Call for Proposals for #SciPy2023 is open! Join us in #ATX on July 10-14, 2023 for the 22nd annual SciPyConf  

Have a talk, tutorial, or poster you want to share? Get your submission in before February 22, 2023 More conference info coming soon 
------
I wasn’t suggesting the ChatGPT *should* replace Google Search. 

But some people were saying that ChatGPT wasn’t even a discussion topic for Search.

Well here you have it now  

https://cnet.com/google-amp/news/chatgpt-caused-code-red-at-google-report-says/…
------
Re: Replacing Google Search with ChatGPT

If Google Search used Chat-GPT, Google would probably be bankrupt in a month or so given that there are 8.5 billion search queries a day.

Using LLMs in/for Google Search makes sense though. They already started doing that years ago.
------
I want to learn to become better at saying No and stay more focused so that I can learn more things more efficiently 
------
I know you all can't wait to fine-tune your LLMs over the holiday break! We just added a template that gets you started. 
E.g., copy and paste the code from the website here to fine-tune a 3-billion param BLOOM model on a multi-node GPU cluster: https://lightning.ai
------
How do language models of different sizes learn during the course of pre-training? 

We study the training trajectories with training checkpoints of language model from 125M to 175B for a better understanding!

Check out our new paper : http://arxiv.org/abs/2212.09803
(1/N)
------
For my deep learning projects I use both 
- notebooks (for analysis) and 
- .py scripts (for submitting/running code)

I have a template for my typical deep learning workflow in the README here: https://github.com/rasbt/machine-learning-notes/tree/main/templates/pl_classifier…
------
Same, having too many browser tabs open makes me anxious. 
That’s why I am using JupyterLab over Jupyter Notebook 
------
i don’t understand tab hoarding. i don’t think i ever have more than 15 tabs open. when i finish the day i just close them all. no regrets
------
Thanks for all the positive feedback on my new, free deep learning course!

Happy to share that Unit 3 is now live!

Math can be fun. But let's be honest, who doesn't like the convenience of automatic differentiation 

 Unit 3: https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/…

Enjoy the holidays!
------
The focus of this one is on

- computation graphs
- full-batch vs stochastic gradient descent
- automatic differentiation
- the PyTorch API

Covering all the foundations before we'll dive deeper into multilayer neural networks in Unit 4!
------
Btw. if you have any questions as you are taking the course don't hesitate to reach out, I am always excited to chat more about deep learning 
------
If you can’t decide whether ChatGPT should replace traditional web search, just have them side by side.
------
Today, youChat goes live.

Open, broadly capable, conversational AI for search with knowledge of recent events and citations of sources.

Search and chat of the future: https://you.com/search?q=what+was+the+recent+breakthrough+in+fusion+research%3F…
------
We are running out of a vital resource: words!

There are “only” 5 to 10 trillion high-quality words (papers, books, code) on the internet.  Our AI models will have used all of that for training by 2026. Low-quality data (tweets, fanfic) will last to 2040. https://arxiv.org/pdf/2211.04325.pdf…
------
High school students are going to be the most experienced prompt engineers; they will be in high demand for employment at tech companies.
------
In 10 years, the world will be very different from today.

Once upon a time, it was okay to learn sth in school & then apply it for the rest of your life. 

Today, to keep up and thrive with the rapid pace of change, it's pivotal to keep reinventing yourself and to keep learning.
------
Call for Proposals for #SciPy2023 is open! Join us in #ATX on July 10-14, 2023 for the 22nd annual SciPyConf  

Have a talk, tutorial, or poster you want to share? Get your submission in before February 22, 2023 More conference info coming soon 
------
I wasn’t suggesting the ChatGPT *should* replace Google Search. 

But some people were saying that ChatGPT wasn’t even a discussion topic for Search.

Well here you have it now  

https://cnet.com/google-amp/news/chatgpt-caused-code-red-at-google-report-says/…
------
Re: Replacing Google Search with ChatGPT

If Google Search used Chat-GPT, Google would probably be bankrupt in a month or so given that there are 8.5 billion search queries a day.

Using LLMs in/for Google Search makes sense though. They already started doing that years ago.
------
I want to learn to become better at saying No and stay more focused so that I can learn more things more efficiently 
------
I know you all can't wait to fine-tune your LLMs over the holiday break! We just added a template that gets you started. 
E.g., copy and paste the code from the website here to fine-tune a 3-billion param BLOOM model on a multi-node GPU cluster: https://lightning.ai
------
How do language models of different sizes learn during the course of pre-training? 

We study the training trajectories with training checkpoints of language model from 125M to 175B for a better understanding!

Check out our new paper : http://arxiv.org/abs/2212.09803
(1/N)
------
For my deep learning projects I use both 
- notebooks (for analysis) and 
- .py scripts (for submitting/running code)

I have a template for my typical deep learning workflow in the README here: https://github.com/rasbt/machine-learning-notes/tree/main/templates/pl_classifier…
------
Same, having too many browser tabs open makes me anxious. 
That’s why I am using JupyterLab over Jupyter Notebook 
------
i don’t understand tab hoarding. i don’t think i ever have more than 15 tabs open. when i finish the day i just close them all. no regrets
------
Thanks for all the positive feedback on my new, free deep learning course!

Happy to share that Unit 3 is now live!

Math can be fun. But let's be honest, who doesn't like the convenience of automatic differentiation 

 Unit 3: https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/…

Enjoy the holidays!
------
The focus of this one is on

- computation graphs
- full-batch vs stochastic gradient descent
- automatic differentiation
- the PyTorch API

Covering all the foundations before we'll dive deeper into multilayer neural networks in Unit 4!
------
Btw. if you have any questions as you are taking the course don't hesitate to reach out, I am always excited to chat more about deep learning 
------
If you can’t decide whether ChatGPT should replace traditional web search, just have them side by side.
------
Today, youChat goes live.

Open, broadly capable, conversational AI for search with knowledge of recent events and citations of sources.

Search and chat of the future: https://you.com/search?q=what+was+the+recent+breakthrough+in+fusion+research%3F…
------
We are running out of a vital resource: words!

There are “only” 5 to 10 trillion high-quality words (papers, books, code) on the internet.  Our AI models will have used all of that for training by 2026. Low-quality data (tweets, fanfic) will last to 2040. https://arxiv.org/pdf/2211.04325.pdf…
------
High school students are going to be the most experienced prompt engineers; they will be in high demand for employment at tech companies.
------
In 10 years, the world will be very different from today.

Once upon a time, it was okay to learn sth in school & then apply it for the rest of your life. 

Today, to keep up and thrive with the rapid pace of change, it's pivotal to keep reinventing yourself and to keep learning.
------
Call for Proposals for #SciPy2023 is open! Join us in #ATX on July 10-14, 2023 for the 22nd annual SciPyConf  

Have a talk, tutorial, or poster you want to share? Get your submission in before February 22, 2023 More conference info coming soon 
------
I wasn’t suggesting the ChatGPT *should* replace Google Search. 

But some people were saying that ChatGPT wasn’t even a discussion topic for Search.

Well here you have it now  

https://cnet.com/google-amp/news/chatgpt-caused-code-red-at-google-report-says/…
------
Re: Replacing Google Search with ChatGPT

If Google Search used Chat-GPT, Google would probably be bankrupt in a month or so given that there are 8.5 billion search queries a day.

Using LLMs in/for Google Search makes sense though. They already started doing that years ago.
------
I want to learn to become better at saying No and stay more focused so that I can learn more things more efficiently 
------
I know you all can't wait to fine-tune your LLMs over the holiday break! We just added a template that gets you started. 
E.g., copy and paste the code from the website here to fine-tune a 3-billion param BLOOM model on a multi-node GPU cluster: https://lightning.ai
------
How do language models of different sizes learn during the course of pre-training? 

We study the training trajectories with training checkpoints of language model from 125M to 175B for a better understanding!

Check out our new paper : http://arxiv.org/abs/2212.09803
(1/N)
------
For my deep learning projects I use both 
- notebooks (for analysis) and 
- .py scripts (for submitting/running code)

I have a template for my typical deep learning workflow in the README here: https://github.com/rasbt/machine-learning-notes/tree/main/templates/pl_classifier…
------
Same, having too many browser tabs open makes me anxious. 
That’s why I am using JupyterLab over Jupyter Notebook 
------
i don’t understand tab hoarding. i don’t think i ever have more than 15 tabs open. when i finish the day i just close them all. no regrets
------
Thanks for all the positive feedback on my new, free deep learning course!

Happy to share that Unit 3 is now live!

Math can be fun. But let's be honest, who doesn't like the convenience of automatic differentiation 

 Unit 3: https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/…

Enjoy the holidays!
------
The focus of this one is on

- computation graphs
- full-batch vs stochastic gradient descent
- automatic differentiation
- the PyTorch API

Covering all the foundations before we'll dive deeper into multilayer neural networks in Unit 4!
------
Btw. if you have any questions as you are taking the course don't hesitate to reach out, I am always excited to chat more about deep learning 
------
If you can’t decide whether ChatGPT should replace traditional web search, just have them side by side.
------
Today, youChat goes live.

Open, broadly capable, conversational AI for search with knowledge of recent events and citations of sources.

Search and chat of the future: https://you.com/search?q=what+was+the+recent+breakthrough+in+fusion+research%3F…
------
We are running out of a vital resource: words!

There are “only” 5 to 10 trillion high-quality words (papers, books, code) on the internet.  Our AI models will have used all of that for training by 2026. Low-quality data (tweets, fanfic) will last to 2040. https://arxiv.org/pdf/2211.04325.pdf…
------
High school students are going to be the most experienced prompt engineers; they will be in high demand for employment at tech companies.
------
In 10 years, the world will be very different from today.

Once upon a time, it was okay to learn sth in school & then apply it for the rest of your life. 

Today, to keep up and thrive with the rapid pace of change, it's pivotal to keep reinventing yourself and to keep learning.
------
Call for Proposals for #SciPy2023 is open! Join us in #ATX on July 10-14, 2023 for the 22nd annual SciPyConf  

Have a talk, tutorial, or poster you want to share? Get your submission in before February 22, 2023 More conference info coming soon 
------
I wasn’t suggesting the ChatGPT *should* replace Google Search. 

But some people were saying that ChatGPT wasn’t even a discussion topic for Search.

Well here you have it now  

https://cnet.com/google-amp/news/chatgpt-caused-code-red-at-google-report-says/…
------
Re: Replacing Google Search with ChatGPT

If Google Search used Chat-GPT, Google would probably be bankrupt in a month or so given that there are 8.5 billion search queries a day.

Using LLMs in/for Google Search makes sense though. They already started doing that years ago.
------
I want to learn to become better at saying No and stay more focused so that I can learn more things more efficiently 
------
I know you all can't wait to fine-tune your LLMs over the holiday break! We just added a template that gets you started. 
E.g., copy and paste the code from the website here to fine-tune a 3-billion param BLOOM model on a multi-node GPU cluster: https://lightning.ai
------
How do language models of different sizes learn during the course of pre-training? 

We study the training trajectories with training checkpoints of language model from 125M to 175B for a better understanding!

Check out our new paper : http://arxiv.org/abs/2212.09803
(1/N)
------
For my deep learning projects I use both 
- notebooks (for analysis) and 
- .py scripts (for submitting/running code)

I have a template for my typical deep learning workflow in the README here: https://github.com/rasbt/machine-learning-notes/tree/main/templates/pl_classifier…
------
Same, having too many browser tabs open makes me anxious. 
That’s why I am using JupyterLab over Jupyter Notebook 
------
i don’t understand tab hoarding. i don’t think i ever have more than 15 tabs open. when i finish the day i just close them all. no regrets
------
Thanks for all the positive feedback on my new, free deep learning course!

Happy to share that Unit 3 is now live!

Math can be fun. But let's be honest, who doesn't like the convenience of automatic differentiation 

 Unit 3: https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/…

Enjoy the holidays!
------
The focus of this one is on

- computation graphs
- full-batch vs stochastic gradient descent
- automatic differentiation
- the PyTorch API

Covering all the foundations before we'll dive deeper into multilayer neural networks in Unit 4!
------
Btw. if you have any questions as you are taking the course don't hesitate to reach out, I am always excited to chat more about deep learning 
------
If you can’t decide whether ChatGPT should replace traditional web search, just have them side by side.
------
Today, youChat goes live.

Open, broadly capable, conversational AI for search with knowledge of recent events and citations of sources.

Search and chat of the future: https://you.com/search?q=what+was+the+recent+breakthrough+in+fusion+research%3F…
------
We are running out of a vital resource: words!

There are “only” 5 to 10 trillion high-quality words (papers, books, code) on the internet.  Our AI models will have used all of that for training by 2026. Low-quality data (tweets, fanfic) will last to 2040. https://arxiv.org/pdf/2211.04325.pdf…
------
High school students are going to be the most experienced prompt engineers; they will be in high demand for employment at tech companies.
------
In 10 years, the world will be very different from today.

Once upon a time, it was okay to learn sth in school & then apply it for the rest of your life. 

Today, to keep up and thrive with the rapid pace of change, it's pivotal to keep reinventing yourself and to keep learning.
------
Call for Proposals for #SciPy2023 is open! Join us in #ATX on July 10-14, 2023 for the 22nd annual SciPyConf  

Have a talk, tutorial, or poster you want to share? Get your submission in before February 22, 2023 More conference info coming soon 
------
I wasn’t suggesting the ChatGPT *should* replace Google Search. 

But some people were saying that ChatGPT wasn’t even a discussion topic for Search.

Well here you have it now  

https://cnet.com/google-amp/news/chatgpt-caused-code-red-at-google-report-says/…
------
Re: Replacing Google Search with ChatGPT

If Google Search used Chat-GPT, Google would probably be bankrupt in a month or so given that there are 8.5 billion search queries a day.

Using LLMs in/for Google Search makes sense though. They already started doing that years ago.
------
I want to learn to become better at saying No and stay more focused so that I can learn more things more efficiently 
------
I know you all can't wait to fine-tune your LLMs over the holiday break! We just added a template that gets you started. 
E.g., copy and paste the code from the website here to fine-tune a 3-billion param BLOOM model on a multi-node GPU cluster: https://lightning.ai
------
How do language models of different sizes learn during the course of pre-training? 

We study the training trajectories with training checkpoints of language model from 125M to 175B for a better understanding!

Check out our new paper : http://arxiv.org/abs/2212.09803
(1/N)
------
For my deep learning projects I use both 
- notebooks (for analysis) and 
- .py scripts (for submitting/running code)

I have a template for my typical deep learning workflow in the README here: https://github.com/rasbt/machine-learning-notes/tree/main/templates/pl_classifier…
------
Same, having too many browser tabs open makes me anxious. 
That’s why I am using JupyterLab over Jupyter Notebook 
------
i don’t understand tab hoarding. i don’t think i ever have more than 15 tabs open. when i finish the day i just close them all. no regrets
------
Thanks for all the positive feedback on my new, free deep learning course!

Happy to share that Unit 3 is now live!

Math can be fun. But let's be honest, who doesn't like the convenience of automatic differentiation 

 Unit 3: https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/…

Enjoy the holidays!
------
The focus of this one is on

- computation graphs
- full-batch vs stochastic gradient descent
- automatic differentiation
- the PyTorch API

Covering all the foundations before we'll dive deeper into multilayer neural networks in Unit 4!
------
Btw. if you have any questions as you are taking the course don't hesitate to reach out, I am always excited to chat more about deep learning 
------
If you can’t decide whether ChatGPT should replace traditional web search, just have them side by side.
------
Today, youChat goes live.

Open, broadly capable, conversational AI for search with knowledge of recent events and citations of sources.

Search and chat of the future: https://you.com/search?q=what+was+the+recent+breakthrough+in+fusion+research%3F…
------
We are running out of a vital resource: words!

There are “only” 5 to 10 trillion high-quality words (papers, books, code) on the internet.  Our AI models will have used all of that for training by 2026. Low-quality data (tweets, fanfic) will last to 2040. https://arxiv.org/pdf/2211.04325.pdf…
------
High school students are going to be the most experienced prompt engineers; they will be in high demand for employment at tech companies.
------
In 10 years, the world will be very different from today.

Once upon a time, it was okay to learn sth in school & then apply it for the rest of your life. 

Today, to keep up and thrive with the rapid pace of change, it's pivotal to keep reinventing yourself and to keep learning.
------
Call for Proposals for #SciPy2023 is open! Join us in #ATX on July 10-14, 2023 for the 22nd annual SciPyConf  

Have a talk, tutorial, or poster you want to share? Get your submission in before February 22, 2023 More conference info coming soon 
------
I wasn’t suggesting the ChatGPT *should* replace Google Search. 

But some people were saying that ChatGPT wasn’t even a discussion topic for Search.

Well here you have it now  

https://cnet.com/google-amp/news/chatgpt-caused-code-red-at-google-report-says/…
------
Re: Replacing Google Search with ChatGPT

If Google Search used Chat-GPT, Google would probably be bankrupt in a month or so given that there are 8.5 billion search queries a day.

Using LLMs in/for Google Search makes sense though. They already started doing that years ago.
------
I want to learn to become better at saying No and stay more focused so that I can learn more things more efficiently 
------
I know you all can't wait to fine-tune your LLMs over the holiday break! We just added a template that gets you started. 
E.g., copy and paste the code from the website here to fine-tune a 3-billion param BLOOM model on a multi-node GPU cluster: https://lightning.ai
------
How do language models of different sizes learn during the course of pre-training? 

We study the training trajectories with training checkpoints of language model from 125M to 175B for a better understanding!

Check out our new paper : http://arxiv.org/abs/2212.09803
(1/N)
------
For my deep learning projects I use both 
- notebooks (for analysis) and 
- .py scripts (for submitting/running code)

I have a template for my typical deep learning workflow in the README here: https://github.com/rasbt/machine-learning-notes/tree/main/templates/pl_classifier…
------
Same, having too many browser tabs open makes me anxious. 
That’s why I am using JupyterLab over Jupyter Notebook 
------
i don’t understand tab hoarding. i don’t think i ever have more than 15 tabs open. when i finish the day i just close them all. no regrets
------
Thanks for all the positive feedback on my new, free deep learning course!

Happy to share that Unit 3 is now live!

Math can be fun. But let's be honest, who doesn't like the convenience of automatic differentiation 

 Unit 3: https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/…

Enjoy the holidays!
------
The focus of this one is on

- computation graphs
- full-batch vs stochastic gradient descent
- automatic differentiation
- the PyTorch API

Covering all the foundations before we'll dive deeper into multilayer neural networks in Unit 4!
------
Btw. if you have any questions as you are taking the course don't hesitate to reach out, I am always excited to chat more about deep learning 
------
If you can’t decide whether ChatGPT should replace traditional web search, just have them side by side.
------
Today, youChat goes live.

Open, broadly capable, conversational AI for search with knowledge of recent events and citations of sources.

Search and chat of the future: https://you.com/search?q=what+was+the+recent+breakthrough+in+fusion+research%3F…
------
We are running out of a vital resource: words!

There are “only” 5 to 10 trillion high-quality words (papers, books, code) on the internet.  Our AI models will have used all of that for training by 2026. Low-quality data (tweets, fanfic) will last to 2040. https://arxiv.org/pdf/2211.04325.pdf…
------
High school students are going to be the most experienced prompt engineers; they will be in high demand for employment at tech companies.
------
In 10 years, the world will be very different from today.

Once upon a time, it was okay to learn sth in school & then apply it for the rest of your life. 

Today, to keep up and thrive with the rapid pace of change, it's pivotal to keep reinventing yourself and to keep learning.
------
Call for Proposals for #SciPy2023 is open! Join us in #ATX on July 10-14, 2023 for the 22nd annual SciPyConf  

Have a talk, tutorial, or poster you want to share? Get your submission in before February 22, 2023 More conference info coming soon 
------
I wasn’t suggesting the ChatGPT *should* replace Google Search. 

But some people were saying that ChatGPT wasn’t even a discussion topic for Search.

Well here you have it now  

https://cnet.com/google-amp/news/chatgpt-caused-code-red-at-google-report-says/…
------
Re: Replacing Google Search with ChatGPT

If Google Search used Chat-GPT, Google would probably be bankrupt in a month or so given that there are 8.5 billion search queries a day.

Using LLMs in/for Google Search makes sense though. They already started doing that years ago.
------
I want to learn to become better at saying No and stay more focused so that I can learn more things more efficiently 
------
I know you all can't wait to fine-tune your LLMs over the holiday break! We just added a template that gets you started. 
E.g., copy and paste the code from the website here to fine-tune a 3-billion param BLOOM model on a multi-node GPU cluster: https://lightning.ai
------
How do language models of different sizes learn during the course of pre-training? 

We study the training trajectories with training checkpoints of language model from 125M to 175B for a better understanding!

Check out our new paper : http://arxiv.org/abs/2212.09803
(1/N)
------
For my deep learning projects I use both 
- notebooks (for analysis) and 
- .py scripts (for submitting/running code)

I have a template for my typical deep learning workflow in the README here: https://github.com/rasbt/machine-learning-notes/tree/main/templates/pl_classifier…
------
Same, having too many browser tabs open makes me anxious. 
That’s why I am using JupyterLab over Jupyter Notebook 
------
i don’t understand tab hoarding. i don’t think i ever have more than 15 tabs open. when i finish the day i just close them all. no regrets
------
Thanks for all the positive feedback on my new, free deep learning course!

Happy to share that Unit 3 is now live!

Math can be fun. But let's be honest, who doesn't like the convenience of automatic differentiation 

 Unit 3: https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/…

Enjoy the holidays!
------
The focus of this one is on

- computation graphs
- full-batch vs stochastic gradient descent
- automatic differentiation
- the PyTorch API

Covering all the foundations before we'll dive deeper into multilayer neural networks in Unit 4!
------
Btw. if you have any questions as you are taking the course don't hesitate to reach out, I am always excited to chat more about deep learning 
------
If you can’t decide whether ChatGPT should replace traditional web search, just have them side by side.
------
Today, youChat goes live.

Open, broadly capable, conversational AI for search with knowledge of recent events and citations of sources.

Search and chat of the future: https://you.com/search?q=what+was+the+recent+breakthrough+in+fusion+research%3F…
------
We are running out of a vital resource: words!

There are “only” 5 to 10 trillion high-quality words (papers, books, code) on the internet.  Our AI models will have used all of that for training by 2026. Low-quality data (tweets, fanfic) will last to 2040. https://arxiv.org/pdf/2211.04325.pdf…
------
High school students are going to be the most experienced prompt engineers; they will be in high demand for employment at tech companies.
------
In 10 years, the world will be very different from today.

Once upon a time, it was okay to learn sth in school & then apply it for the rest of your life. 

Today, to keep up and thrive with the rapid pace of change, it's pivotal to keep reinventing yourself and to keep learning.
------
Call for Proposals for #SciPy2023 is open! Join us in #ATX on July 10-14, 2023 for the 22nd annual SciPyConf  

Have a talk, tutorial, or poster you want to share? Get your submission in before February 22, 2023 More conference info coming soon 
------
I wasn’t suggesting the ChatGPT *should* replace Google Search. 

But some people were saying that ChatGPT wasn’t even a discussion topic for Search.

Well here you have it now  

https://cnet.com/google-amp/news/chatgpt-caused-code-red-at-google-report-says/…
------
Re: Replacing Google Search with ChatGPT

If Google Search used Chat-GPT, Google would probably be bankrupt in a month or so given that there are 8.5 billion search queries a day.

Using LLMs in/for Google Search makes sense though. They already started doing that years ago.
------
I want to learn to become better at saying No and stay more focused so that I can learn more things more efficiently 
------
I know you all can't wait to fine-tune your LLMs over the holiday break! We just added a template that gets you started. 
E.g., copy and paste the code from the website here to fine-tune a 3-billion param BLOOM model on a multi-node GPU cluster: https://lightning.ai
------
How do language models of different sizes learn during the course of pre-training? 

We study the training trajectories with training checkpoints of language model from 125M to 175B for a better understanding!

Check out our new paper : http://arxiv.org/abs/2212.09803
(1/N)
------
For my deep learning projects I use both 
- notebooks (for analysis) and 
- .py scripts (for submitting/running code)

I have a template for my typical deep learning workflow in the README here: https://github.com/rasbt/machine-learning-notes/tree/main/templates/pl_classifier…
------
Same, having too many browser tabs open makes me anxious. 
That’s why I am using JupyterLab over Jupyter Notebook 
------
i don’t understand tab hoarding. i don’t think i ever have more than 15 tabs open. when i finish the day i just close them all. no regrets
------
Thanks for all the positive feedback on my new, free deep learning course!

Happy to share that Unit 3 is now live!

Math can be fun. But let's be honest, who doesn't like the convenience of automatic differentiation 

 Unit 3: https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/…

Enjoy the holidays!
------
The focus of this one is on

- computation graphs
- full-batch vs stochastic gradient descent
- automatic differentiation
- the PyTorch API

Covering all the foundations before we'll dive deeper into multilayer neural networks in Unit 4!
------
Btw. if you have any questions as you are taking the course don't hesitate to reach out, I am always excited to chat more about deep learning 
------
If you can’t decide whether ChatGPT should replace traditional web search, just have them side by side.
------
Today, youChat goes live.

Open, broadly capable, conversational AI for search with knowledge of recent events and citations of sources.

Search and chat of the future: https://you.com/search?q=what+was+the+recent+breakthrough+in+fusion+research%3F…
------
We are running out of a vital resource: words!

There are “only” 5 to 10 trillion high-quality words (papers, books, code) on the internet.  Our AI models will have used all of that for training by 2026. Low-quality data (tweets, fanfic) will last to 2040. https://arxiv.org/pdf/2211.04325.pdf…
------
High school students are going to be the most experienced prompt engineers; they will be in high demand for employment at tech companies.
------
In 10 years, the world will be very different from today.

Once upon a time, it was okay to learn sth in school & then apply it for the rest of your life. 

Today, to keep up and thrive with the rapid pace of change, it's pivotal to keep reinventing yourself and to keep learning.
------
Call for Proposals for #SciPy2023 is open! Join us in #ATX on July 10-14, 2023 for the 22nd annual SciPyConf  

Have a talk, tutorial, or poster you want to share? Get your submission in before February 22, 2023 More conference info coming soon 
------
I wasn’t suggesting the ChatGPT *should* replace Google Search. 

But some people were saying that ChatGPT wasn’t even a discussion topic for Search.

Well here you have it now  

https://cnet.com/google-amp/news/chatgpt-caused-code-red-at-google-report-says/…
------
Re: Replacing Google Search with ChatGPT

If Google Search used Chat-GPT, Google would probably be bankrupt in a month or so given that there are 8.5 billion search queries a day.

Using LLMs in/for Google Search makes sense though. They already started doing that years ago.
------
I want to learn to become better at saying No and stay more focused so that I can learn more things more efficiently 
------
I know you all can't wait to fine-tune your LLMs over the holiday break! We just added a template that gets you started. 
E.g., copy and paste the code from the website here to fine-tune a 3-billion param BLOOM model on a multi-node GPU cluster: https://lightning.ai
------
How do language models of different sizes learn during the course of pre-training? 

We study the training trajectories with training checkpoints of language model from 125M to 175B for a better understanding!

Check out our new paper : http://arxiv.org/abs/2212.09803
(1/N)
------
For my deep learning projects I use both 
- notebooks (for analysis) and 
- .py scripts (for submitting/running code)

I have a template for my typical deep learning workflow in the README here: https://github.com/rasbt/machine-learning-notes/tree/main/templates/pl_classifier…
------
Same, having too many browser tabs open makes me anxious. 
That’s why I am using JupyterLab over Jupyter Notebook 
------
i don’t understand tab hoarding. i don’t think i ever have more than 15 tabs open. when i finish the day i just close them all. no regrets
------
Thanks for all the positive feedback on my new, free deep learning course!

Happy to share that Unit 3 is now live!

Math can be fun. But let's be honest, who doesn't like the convenience of automatic differentiation 

 Unit 3: https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/…

Enjoy the holidays!
------
The focus of this one is on

- computation graphs
- full-batch vs stochastic gradient descent
- automatic differentiation
- the PyTorch API

Covering all the foundations before we'll dive deeper into multilayer neural networks in Unit 4!
------
Btw. if you have any questions as you are taking the course don't hesitate to reach out, I am always excited to chat more about deep learning 
------
If you can’t decide whether ChatGPT should replace traditional web search, just have them side by side.
------
Today, youChat goes live.

Open, broadly capable, conversational AI for search with knowledge of recent events and citations of sources.

Search and chat of the future: https://you.com/search?q=what+was+the+recent+breakthrough+in+fusion+research%3F…
------
We are running out of a vital resource: words!

There are “only” 5 to 10 trillion high-quality words (papers, books, code) on the internet.  Our AI models will have used all of that for training by 2026. Low-quality data (tweets, fanfic) will last to 2040. https://arxiv.org/pdf/2211.04325.pdf…
------
High school students are going to be the most experienced prompt engineers; they will be in high demand for employment at tech companies.
------
In 10 years, the world will be very different from today.

Once upon a time, it was okay to learn sth in school & then apply it for the rest of your life. 

Today, to keep up and thrive with the rapid pace of change, it's pivotal to keep reinventing yourself and to keep learning.
------
Call for Proposals for #SciPy2023 is open! Join us in #ATX on July 10-14, 2023 for the 22nd annual SciPyConf  

Have a talk, tutorial, or poster you want to share? Get your submission in before February 22, 2023 More conference info coming soon 
------
I wasn’t suggesting the ChatGPT *should* replace Google Search. 

But some people were saying that ChatGPT wasn’t even a discussion topic for Search.

Well here you have it now  

https://cnet.com/google-amp/news/chatgpt-caused-code-red-at-google-report-says/…
------
Re: Replacing Google Search with ChatGPT

If Google Search used Chat-GPT, Google would probably be bankrupt in a month or so given that there are 8.5 billion search queries a day.

Using LLMs in/for Google Search makes sense though. They already started doing that years ago.
------
I want to learn to become better at saying No and stay more focused so that I can learn more things more efficiently 
------
I know you all can't wait to fine-tune your LLMs over the holiday break! We just added a template that gets you started. 
E.g., copy and paste the code from the website here to fine-tune a 3-billion param BLOOM model on a multi-node GPU cluster: https://lightning.ai
------
How do language models of different sizes learn during the course of pre-training? 

We study the training trajectories with training checkpoints of language model from 125M to 175B for a better understanding!

Check out our new paper : http://arxiv.org/abs/2212.09803
(1/N)
------
For my deep learning projects I use both 
- notebooks (for analysis) and 
- .py scripts (for submitting/running code)

I have a template for my typical deep learning workflow in the README here: https://github.com/rasbt/machine-learning-notes/tree/main/templates/pl_classifier…
------
Same, having too many browser tabs open makes me anxious. 
That’s why I am using JupyterLab over Jupyter Notebook 
------
i don’t understand tab hoarding. i don’t think i ever have more than 15 tabs open. when i finish the day i just close them all. no regrets
------
Thanks for all the positive feedback on my new, free deep learning course!

Happy to share that Unit 3 is now live!

Math can be fun. But let's be honest, who doesn't like the convenience of automatic differentiation 

 Unit 3: https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/…

Enjoy the holidays!
------
The focus of this one is on

- computation graphs
- full-batch vs stochastic gradient descent
- automatic differentiation
- the PyTorch API

Covering all the foundations before we'll dive deeper into multilayer neural networks in Unit 4!
------
Btw. if you have any questions as you are taking the course don't hesitate to reach out, I am always excited to chat more about deep learning 
------
If you can’t decide whether ChatGPT should replace traditional web search, just have them side by side.
------
Today, youChat goes live.

Open, broadly capable, conversational AI for search with knowledge of recent events and citations of sources.

Search and chat of the future: https://you.com/search?q=what+was+the+recent+breakthrough+in+fusion+research%3F…
------
We are running out of a vital resource: words!

There are “only” 5 to 10 trillion high-quality words (papers, books, code) on the internet.  Our AI models will have used all of that for training by 2026. Low-quality data (tweets, fanfic) will last to 2040. https://arxiv.org/pdf/2211.04325.pdf…
------
High school students are going to be the most experienced prompt engineers; they will be in high demand for employment at tech companies.
------
In 10 years, the world will be very different from today.

Once upon a time, it was okay to learn sth in school & then apply it for the rest of your life. 

Today, to keep up and thrive with the rapid pace of change, it's pivotal to keep reinventing yourself and to keep learning.
------
Call for Proposals for #SciPy2023 is open! Join us in #ATX on July 10-14, 2023 for the 22nd annual SciPyConf  

Have a talk, tutorial, or poster you want to share? Get your submission in before February 22, 2023 More conference info coming soon 
------
I wasn’t suggesting the ChatGPT *should* replace Google Search. 

But some people were saying that ChatGPT wasn’t even a discussion topic for Search.

Well here you have it now  

https://cnet.com/google-amp/news/chatgpt-caused-code-red-at-google-report-says/…
------
Re: Replacing Google Search with ChatGPT

If Google Search used Chat-GPT, Google would probably be bankrupt in a month or so given that there are 8.5 billion search queries a day.

Using LLMs in/for Google Search makes sense though. They already started doing that years ago.
------
I want to learn to become better at saying No and stay more focused so that I can learn more things more efficiently 
------
I know you all can't wait to fine-tune your LLMs over the holiday break! We just added a template that gets you started. 
E.g., copy and paste the code from the website here to fine-tune a 3-billion param BLOOM model on a multi-node GPU cluster: https://lightning.ai
------
How do language models of different sizes learn during the course of pre-training? 

We study the training trajectories with training checkpoints of language model from 125M to 175B for a better understanding!

Check out our new paper : http://arxiv.org/abs/2212.09803
(1/N)
------
For my deep learning projects I use both 
- notebooks (for analysis) and 
- .py scripts (for submitting/running code)

I have a template for my typical deep learning workflow in the README here: https://github.com/rasbt/machine-learning-notes/tree/main/templates/pl_classifier…
------
Same, having too many browser tabs open makes me anxious. 
That’s why I am using JupyterLab over Jupyter Notebook 
------
i don’t understand tab hoarding. i don’t think i ever have more than 15 tabs open. when i finish the day i just close them all. no regrets
------
Thanks for all the positive feedback on my new, free deep learning course!

Happy to share that Unit 3 is now live!

Math can be fun. But let's be honest, who doesn't like the convenience of automatic differentiation 

 Unit 3: https://lightning.ai/pages/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/…

Enjoy the holidays!
------
The focus of this one is on

- computation graphs
- full-batch vs stochastic gradient descent
- automatic differentiation
- the PyTorch API

Covering all the foundations before we'll dive deeper into multilayer neural networks in Unit 4!
------
Btw. if you have any questions as you are taking the course don't hesitate to reach out, I am always excited to chat more about deep learning 